# Replication, reproducibility, and transparency  {#replication}

::: {.learning-goals}
üçé Learning goals: 

* Understand the "crisis" narrative in psychology and the empirical evidence supporting it
* Define the distinction between reproducibility and replicability
* Consider types of replication 
* Reason about the relation of replication to theory building
:::

In the previous chapter, we gave a sober and considered introduction to the topic of experiments, their connection with causal inference, and their role in building psychological theory. In this chapter we're going to change gears a little bit and tell the story of the period from 2011 -- 2021 and how it has given rise to a number of "crisis" narratives in psychology. 


In order to set the terms of discussion, we need to describe some of the outcomes we are interested in. Figure \@ref(fig:replication-terms) gives us a basic starting point for our definitions.^[These terms have been a bit of a problem for the field, at least at first, but it seems like people have been agreeing on them recently.] For some claim in a paper, if we can take the same data that were analyzed in that paper, do the same analysis, and get the same result, we call that result **reproducible** (sometimes, **computationally reproducible**). If we can collect new data in the same experiment, do the same analysis, and get the same result, we call that a **replication** and say that the experiment is **replicable**. If we can do a different analysis with the original dataset, we call this a **robustness check** and so if a claim passes it is **robust**.^[These are less common in experimental psychology, but are very common in fields that work with large, complex observational datasets like sociology.] And if the same finding obtains in a different population, perhaps with a different analysis, the result is likely to be more **generalizable** beyond the initial conditions in which it was observed.^[You might have observed that a lot of work is being done here by the word "same." How do we operationalize same-ness for experimental procedures, statistical analyses, or samples? These are difficult questions that we'll address in part below, but there's no single answer and so these terms are always going to helpful guides rather than exact labels.]



```{r replication-terms, fig.cap="A terminological framework for meta-science discussions. Based on [https://figshare.com/articles/Publishing_a_reproducible_paper/5440621]().", fig.margin=TRUE}
knitr::include_graphics("images/replication/terms.png")
```

We're also going to abandon the sober tone of the introduction and try to get you a bit worked up. From an empirical perspective, things have been far from ideal in the psychology literature. Many classic findings may be wrong, or at least overstated. Their statistical tests are probably not trustworthy. The actual numbers are even wrong in many papers! And even when experimental findings are "real" they may not reflect deep psychological generalizations. And even if they do, they likely don't reflect generalizations that are true about people in general, only some very specific groups of people. Your hair should be on fire, at least a little bit. If you by the end of this chapter, you don't feel a little bit of despair about the published psychological literature, then we haven't done our job.

On the other hand, you might be thinking, how do you know that all this bad stuff is true? Claims about a literature or field as a whole go beyond the kind of standard paradigmatic science that we were talking about in the previous chapter -- instead they are part of a new field called **meta-science**. Meta-science research is research *about research*, for example investigating the rate of replication in a literature, or trying to figure out how widespread some negative practice is within a subfield. Meta-science allows us to go beyond one-off anecdotes about particular results or rumors about bad practices. Critically, we can also evaluate the strength of claims about the field using the same tools we use to evaluate standard research -- that is, we can critique research designs and inferences systematically, rather than just accepting (or failing to accept) assertions.

In this 


::: {.case-study}
üî¨ Case study: The Open Science Collaboration

Around 2011, we were teaching our Experimental Methods course for the first time, based on a course model that we had worked on with Rebeccca Saxe [@frank2012]. The idea was to have a replication-based course that introduced students to the nuts and bolts of research.^[One of the current author team was a student in the course that year!] A guy named Brian Nosek was on sabbatical nearby, and over coffee we learned that he was starting up an ambitious project to replicate a large sample of studies from top psychology journals in 2008.

Students in Experimental Methods that year chose replication projects from the sample that Nosek had told us about. Four of these projects were done at a high standard and were nominated by the course TAs for inclusion in the broader project on the basis of strong implementations of the paradigm and good sample sizes. A few years later, when the final group of 100 replication studies was completed, we got a look at the results, shown in Figure \@ref(fig:replication-osc-2015). 

```{r replication-osc-2015, fig.cap="Results from the Open Science Collaboration (2015). Each point represents one of the studies in the sample, with the horizontal position giving the original effect size and the vertical position giving the replication effect size. Dot size shows estimated statistical power. The dotted line represents a perfect replication."}
knitr::include_graphics("images/replication/osc-2015.png")
```
The resulting meta-science paper made a substantial impression on both psychologists and the broader community, defining both a field of psychology meta-science studies and providing a template for many-author  collaborative projects [@osc2015]. But the most striking thing was the result: disappointingly, by several different criteria, only around a third of studies showed the same finding as the original. The others yielded smaller effects that were no significant in the replication sample. This study provided the first large-scale evidence that  methodological issues in the psychological literature might lead to a bad outcome in the literature -- namely a literature with many findings that didn't replicate.

The study's results -- and their interpretation -- were controversial, however, and much ink was spilled on what these data showed. In particular, critics pointed to different degrees of fidelity between the original studies and the replications; insufficient levels of statistical power in the replications; non-representative sampling of the literature; and difficulties identifying specific statistical outcomes for replication success [@gilbert2016;@anderson2016]. In our view, many of these critiques have merit, and you can't simply interpret the results of this stufy as an unbiased estimate of the replicability of results in the literature, contra the title.^[Confusingly, the title of the paper is "Estimating the reproducibility of psychological science", not "the replicability of psychological science". This caused terminological confusion for several years; it seems like at this point people have decided it's just a mistake.]

And yet, the results are still important and compelling, and they undeniably changed the direction of the field of psychology. Many good studies are like this -- they have critical flaws but they lead the way towards other studies that pursue this new direction, often with greater technical precision and fewer issues. For several of us personally, working on this project was also transformative in that it showed us the power of collaborative work -- together we could do a study that no one of us had any hope of completing on our own, and potentially make a difference in our field.
:::




### Reproducibility

Critical notion of the "provenance chain" for specific numbers - that they can be traced back to analytic computations. 

### Replication

questionable research practices

Publication bias




::: {.ethics-box}
üåø Ethics box: Consequences for the study, consequences for the person

"Power posing" is the idea that changing your physical posture to make it more open and expansive might also change your confidence. @carney2010 reported a striking study of this phenomenon, in which 42 participants were told they were taking part in a study of physiological recording. They then held two poses, each for a minute. In one condition, the poses were expansive (e.g., legs out, hands on head) or contractive (e.g., arms and legs crossed). Participants in the expansive pose condition showed increases in testosterone and decreases in salivary cortisol (a stress marker), they took a greater number of risk in a gambling task, and they reported that they were more "in charge" in a survey. This result suggested that a two-minute induction could lead to striking physiological and psychological changes -- in turn leading to power posing becoming firmly enshrined as part of the set of recommended strategies in business and elsewhere. The original publication contributed to the rise of the researchers' careers, including becoming a principal piece of evidence in a hugely-popular TED talk by Amy Cuddy, one of the authors.

This result is likely not definitive, however. A replication study with a larger number of participants (N=200) failed to find evidence for physiological effects, even as it did find some effects on participants' own beliefs [@ranehill2015]. And a review of the published literature suggested that many findings appeared to be the result of some sort of publication bias, as far too many of them had $p$-values very close to the .05 threshold [@simmons2017]. In light of this evidence, the first author made a public statement that she [does not believe that "power pose" effects are real](https://faculty.haas.berkeley.edu/dana_carney/pdf_my%20position%20on%20power%20poses.pdf).


```{r replication-powerpose, fig.cap="Google trends time series for \"power pose\" from 2004-2021.", fig.margin=TRUE}
knitr::include_graphics("images/replication/powerpose.png")
```

From the scientific perspective, it's very tempting to take this example as a case in which the scientific ecosystem corrects itself. Although many people continue to cite the original power posing work, we suspect the issues are well-known throughout the social psychology community, and overall interest has gone down (see Figure \@ref(fig:replication-powerpose). But this narrative masks the very real human impacts of the self-correction process, which can raise ethical questions about the best way to address issues in the scientific record. 

The process of debate and discussion around individual findings can be bruising and complicated. In the case of power posing, Cuddy herself was tighly associated with the findings and many critiques of the findings became critiques of the individual. Several commentators used Cuddy's name as a synecdoche for low-quality psychological results, likely because of her prominence and perhaps because of her gender and age as well. These comments were likely harmful to Cuddy personally and her career more generally.^[For further reading, see ["When the Revolution Came for Amy Cuddy"](https://www.nytimes.com/2017/10/18/magazine/when-the-revolution-came-for-amy-cuddy.html).]

Scientists should critique, reproduce, and replicate -- these are all critical parts of the progress of normal science. But it's important to do this in a way that's sensitive to the people involved. Here are a few guidelines for courteous and ethical conduct: 

* Communicate personally before communicating publicly. As one critic said about the power posing debate, "‚ÄúI wish I‚Äôd had the presence of mind to pick up the phone and call [before publishing my critique].‚Äù
* Always communicate about the work, never the person. Try to use language that is specific to the analysis or design being critiqued, rather than the person who did the analysis or thought up the design.
* Avoid using language that assumes negative intentions, e.g. "the authors misleadingly state that ..."
* Ask for someone to read your paper, email, blogpost, or tweet before you hit send. It can be very difficult to figure out the tone of your writing without an external perspective.

As we will argue in the next chapter, we have an ethical duty as scientists to promote good science and critique low quality science. But we also have duties to our colleauges and communities to be good to one another.
:::


## A framework for replication 

- We describe the framework for replication in Zwaan et al. (2018), highlighting the idea of multiple dimensions on which a replication can differ from the original study and the ways that this complicates inferences about replication ‚Äúsuccess‚Äù (Mathur and VanderWeele 2019, 2020).
- We can get different epistemic value from doing direct replications (attempt to copy original study with as much fidelity as possible) vs. conceptual replications (attempt to somewhat perturb operationalization of original study).

::: {.accident-report}
‚ö†Ô∏è Accident report: The "small telescopes" case study of Simonsohn et al. (2015): what if weather really does affect mood, but the effect is too small for the original study to ever detect?

> Imagine I claimed our next-door neighbor was a billionaire oil sheik who kept thousands of boxes of gold and diamonds hidden in his basement. Later we meet the neighbor, and he is the manager of a small bookstore and has a salary 10% above the US average... Should we describe this as ‚Äúwe have confirmed the Wealthy Neighbor Hypothesis, though the effect size was smaller than expected‚Äù? Or as ‚ÄúI made up a completely crazy story, and in unrelated news there was an irrelevant deviation from literally-zero in the same space‚Äù?
:::

## Relation to theory building

How do reproducibility and replicability contribute to theory building? We draw on Hardwicke et al. (2018), considering these factors in the informativeness of an experiment.


Introduce p-hacking and publication biases as major sources of irreproducibility. These are biases - and we highlight bias reduction as a key goal.


::: {.accident-report}
‚ö†Ô∏è Accident report: When I'm 64?

Simmons, Simonsohn, Nelson introduce p-hacking into our lexicon. 

:::




::: {.case-study}
üî¨ Case study: Hardwicke et al. (2018; 2021) meta-studies of the analytic reproducibility of specific analytic findings in the empirical literature.
:::
