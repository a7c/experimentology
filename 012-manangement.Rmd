# Data management and sharing{#management}


::: {.learning-goals}
🍎 Learning goals: 

* Understand the risks of data identification, 
* List optimal data management practices, 
* describe important elements of version control
* define FAIR data
:::

> Your closest collaborator is you six months ago, but you don't reply to emails. 
>
> `r tufte::quote_footer('--- Karl Broman (2016)')`

Have you ever had a filename that ended in `-FINAL`? And was that file **really** the final version? Have you ever been asked for information or files from a project that you completed months or years ago, and been confused by what you sew when you looked back on your hard drive? These experiences may make you sympathetic to Karl Broman's quip: good practices for organizing your data help you first. But these same practices can enable open sharing of your research. This chapter is about the process of managing the documents -- in practice, computer files -- that result from research in ways that maximize their value to you and to the broader research community. 

When we talk about research products, often our minds go to scientific journal publications, which have been the main method of communication for scientific results since the scientific revolution in the 1600s.^[Arguably, this is the Transactions of the Royal Society, which first published in 1665.] But research produces many other products, from experimental stimulus items and scripts to data and analysis code. When shared appropriately, these other products can be at least as valuable as a paper reporting the primary result. Shared stimulus materials can be reused for new studies in creative ways; shared analysis scripts can allow for reproduction of reported results and become templates for new analyses; and shared data can enable new analyses or meta-analyses. 

In recent years, there have been widespread calls for data sharing [@lindsay2017]. Indeed, some form of data sharing is required of funded projects by many US and European funding agencies. Data sharing has been associated with benefits in terms of downstream citations [@piowar2013]. And in surveys, researchers report openness to data sharing. Yet many researchers report that they do not share data regularly, and that much of their data is shared exclusively via risky practices such as exclusive storing on a personal computer or external drive where they are easily lost [@tenopir2020]. And as we have discussed in Chapter \@ref(replication), even when data are shared they are not always formatted usably. 

The goal of this chapter is to provide a guide for the management of data and other research products with a dual emphasis on transparency and organization. If you organize your project well, it is easy to share it later, and if you assume that you will be sharing, you will be motivated to organize your work better! This is a virtuous cycle. 

We begin by discussing how to manage projects, with discussions of naming, organization, and version control. Then we zoom in specifically on data, which in many cases is the most valuable resarch product, and discuss best practices for data sharing. We end by discussing the question of what research products to share and some of the risks and benefits of sharing.^[This chapter draws heavily on @klein2018, an article on research transparency that several us of contributed to.]

::: {.case-study}
🔬 Case study: ManyBabies, ManySpreadsheetFormats!

The ManyBabies project is an example of the "Big Team Science" in psychology. A group of developmental psychology reseachers (including some of us) were worried about many of the issues of reproducibility, replicability, and experimental methods that we've been discussing throughout this book, so they set up a large-scale collaboration to replicate key effects in developmental science. The first of these studies was ManyBabies 1 [@manybabies2020], a study of infants' preference for babytalk (also known as Infant Directed Speech). 

The planning team expected a handful of labs to contribute but after a year-long data collection period, they ended up receiving data from 69 labs around the world! The outpouring of interest signaled a lot of enthusiasm from the community for this kind of collaborative science. But it also made for a tremendous data analysis headache. The organizers had created a set of spreadsheet templates for data upload, but they didn't bargain for what they got back from participating labs. As researchers tried to accommodate their lab's own idiosyncratic data formatting to the standard template, and then as the central analytic team tried to read all of these into a single analysis pipeline, all kinds of hilarity ensued [@byers-heinlein2020].

All of the changes that labs had made were reasonable -- altering column names for clarity, combining templates into a single Excel file, changing units (e.g., from seconds to milliseconds) -- but together they created a very challenging **data validation** problem for the analysis team, requiring many dozens of hours of coding and hand-checking. The data checking was critical, however -- one lab's data that had been flagged during validation was eventually identified as having a fundamental issue in the coding of infant videos, leading to the painful decision to drop those data from the final dataset. In future ManyBabies projects, the group has committed to using data validation software to ensure that uploaded data files conform to project-specific templates before going forward with analysis. You can see some of the analysis pipeline and validation process in the [ManyBabies 1 public repository](https://github.com/manybabies/mb1-analysis-public).
:::

## Project Management

There are a few basic ways that project management goes wrong for scientific projects. The first is that the project is not accessible, whether because a collaborator did not share it to begin with, because it was not backed up and a hard drive failed, because it was linked to one collaborator's login and that information has been lost, or otherwise. The second is that the project is accessible but incomprehensible, often because it is unclear what version of a file is definitive, or its function within the project is unknown. This lack of clarity can happen because the project's creator(s) are inaccessible for consultation or -- at least as often -- because the creators themselves can't remember what they were doing back then. These kinds of problems lead to a tremendous loss of scientific value. 

Further, they can be avoided via following a very simple schema for organizing project files. For those researchers that "grew up" managing their files locally and emailing versions of scripts and manuscripts back and forth with names like `analysis-FINAL-JS-rev1.xlsx`, a few aspects of this schema can be disconcerting. But if you have used a cloud-based provider to collaborate, this way of working will likely be fairly intuitive. Here are the principles:

1. There should be exactly one definitive copy of each document in the project, with its name denoting what it is. For example, `manuscript.Rmd` is the writeup of the project as a journal manuscript.
2. The location each document should be within a project folder whose sub-folders serve to identify uniquely the document's function within the project. For example, `/analysis/experiment1/eye_tracking_preprocesssing.Rmd` is clearly the Markdown containing eye-tracking preprocessing for the analysis of data from Experiment 1. 
3. The full project should be accessible to all collaborators archived across multiple storage devices, either via a version control platform (e.g., [github.com]()) or cloud provider (e.g., dropbox, box, google drive) if possible, or via a distributed, automatic backup system otherwise.
4. The revision history of all text- and text-based documents (minimally, data, analysis code, and manuscript files) should be archived automatically.

Keeping these principles in mind, we discuss best practices for project organization, version control, and file naming.^[We choose this order because our recommended file naming conventions make more sense in light of our organizational and versioning recommendations.]

### Organizing your project

To the greatest extent possible, all of your files related to a project should be stored in the same place. Ideally, they should all be in the same project folder (with appropriate subfolders), and they should be stored on the same provider.^[There are certainly cases where this is impractical due to the limitations of different software packages. For example, in many cases a team will manage its data and analysis code via github but decide to write collaboratively using google docs, overleaf, or another collaborative platform. (It can also be hard to ask all collaborators to buy into a version control system if they are unused to it.) In that case, the final paper should still be linked in some way to the project repository. The only issue that comes up in using a split workflow like this is the need to ensure reproducible written products, a process we cover in Chapter \@ref(writing).] 

Figure \@ref(fig:management-organization-ex) shows an example project stored using the Open Science Framework. The top level folder contains folders for analyses, materials, raw and processed data (kept separately). It also contains the paper manuscript, and, critically, a README file in a text format that describes the project, the license, and any other meta-data that the authors would like to be associated with the research products. 

```{r management-organization-ex, fig.cap="Sample top level folder structure for a project. From Klein et al., 2018. Original visible on the [Open Science Framework](https://osf.io/xf6ug/)."}
knitr::include_graphics("images/management/org-ex.png")
```
There's no one way to organize folders, but the broad categories of materials, data, analysis, and writing are typically present in almost any experimental project. Further, many projects -- especially those that include multiple experiments or complex data types -- require subfolders. In our projects, it's not uncommon to find paths like `/data/raw_data/exp1/surveys`. The key principle here is to create a hierarchical structure in which subfolders uniquely identify the part of the broader space of research products that are found inside them -- that is, `/data/raw_data/exp1` contains all the raw data from Experiment 1, and `/data/raw_data/exp1/surveys` contains all the raw *survey* data from that particular experiment.^[If you're interested, a more extensive guide to folder organization is found in the [online supplement](https://psych-transparency-guide.uni-koeln.de/folder-structure.html#root-folder) to @klein2018.]

### Versioning

Distributed version control is one of the great inventions of software engineering pratice. Probably everyone who has ever collaborated electronically has experienced the frustration of editing a document, only to find out that you are editing the wrong version -- perhaps some of the problems you are working on have already been corrected, or perhaps the section you are adding has already been written by someone else. A second source of frustration comes when you take a wrong turn in a project, perhaps by reorganizing a manuscript in a way that doesn't work or refactoring code in a way that turns out to be short-sighted. 

These two classes of problems are solved effectively by modern version control systems. Here we focus on the use of git, which is perhaps the most widely used version control system. Git is a tool for creating and managing projects, which are called **repositories**. A git repository is a directory whose revision history is tracked via a series of **commits** -- snapshots of the state of the project. These commits can form a tree with different **branches**, as when two contributors to the project are working on two different parts simultaneously. These branches can later be **merged** either automatically or via manual intervention in the case of conflicting changes. 

Practially speaking, a common git workflow is that the repository is hosted by an online service like [github](http://github.com). In that case, changes can be **pushed** to that hosted copy by one user and then **pulled** by another user. The hosted "origin" copy then is the definitive copy of the project. Appendix \@ref(git) provides a practical introduction to how to use Git and Github.

This model of collaboration is designed to solve many of the problems we've been discussing:

* A remotely hosted git repository is a cloud-based backup of your work, meaning it is less vulnerable to device failure.^[Hard drive failure is still a surprisingly common source of missed deadlines!] 
* By virtue of having versioning history, you have access to previous drafts in case you find you have been following a blind alley and want to roll back your changes. 
* By creating new branches, you can create another, parallel history for your project, so that you can try out major changes or additions without disturbing the main branch in the process. 
* A project's commit history is labeled with each commit's author and date, facilitating record keeping and collaboration.
* Automatic merging can allow synchronous editing of different parts of a manuscript or codebase.^[Version control isn't magic, and if you and a collaborator edit the same paragraph or function, you will likely have to merge your changes by hand. But git will at least show you where the conflict is!]

Organizing a project repository for collaboration and hosting on a remote platform is an important first step towards sharing! Many of our projects are actually "born open" in the sense that we do all of our work on a publicly hosted repository for everyone to see [@rouder2015].This practice can feel uncomfortable when you first begin, but this discomfort soon vanishes as you realize that no one is actively looking at your in-progress project.^[One concern that many people raise about sharing in-progress research openly is the possibility of "scooping" -- that is, other researchers getting an idea or even data from the repository and writing a paper before you do. We have two responses to this concern. First, the empirical frequency of this sort of scooping is difficult to determine but likely very low -- we don't know of any documented cases. Mostly, the problem is getting people to care about your experiment at all, not people caring so much that they would publish using your data or materials! In Gary King's [words](https://www.youtube.com/watch?v=jD6CcFxRelY], "The thing that matters the least is being scooped. The thing that matters the most is being ignored." On the other hand, if you are in an area of research that you perceive to be competitive or where there is some significant risk of this sort, it's very easy to make a repository private or even just to hold back key data files from the repository and share them only among collaborators. All of the benefits we described still accrue.] And the slightly greater level of scruitiny encourages good organization practices from the beginning. If this practice feels uncomfortable, most hosting services allow the creation of private repositories. Then, for an appropriately organized and hosted project, often the only steps required to share materials, data, and code is to make the hosted repository public and link it to an archival storage platform like the Open Science Framework.


### File names

As [Phil Karlton is said to have said](https://www.karlton.org/2017/12/naming-things-hard/), “There are only two hard things in Computer Science: cache invalidation and naming things.” We won't talk about cache invalidation here, but naming files is definitely hard (there is a guide to naming variables below). Naming a stand-alone file that you are emailing to someone is a mess -- our downloads folders are full of `manuscript-7 (copy).pdf` files. That's a nightmare because there is no context to disambiguate! Some very organized people survive on systems like `info-r1-draft-2020-07-13-js.docx` - meaning, "the info project revision 1 draft of July 13th, 2020, with edits by Jada Smith."^[This kind of system is not bad, but it needs ground rules! What happens when PF also edits -- should she change the date or the revision number? Only if it's major edits?]

On the other hand, if you are naming a file in a hierarchically organized version control repository, the naming problem gets dramatically easier. All of a sudden, you have a context in which names make sense. `data.csv` is a terrible name for a data file on its own. But the name is actually perfectly informative -- in the context of a project repository with a README that states that there is only a single experiment, a repository structure such that the file lives in a folder called `raw_data`, and a = commit history that indicates the file's commit date and author. 

As this example shows, naming is hard *out of context*. So here's our rule: name a file what it contains. Don't use the name to convey the context of who edited it, when, or where it should go in a project. 

## Data Management

We've just discussed how to manage projects in general; in this section we zoom in on datasets specifically. We make three primary recommendations, which we discuss in turn. First, save your raw data! Second, document your data collection process. And third, organize your raw data for later analysis. We provide guidance on organization for both spreadsheets and for data retrieved from software platforms. 

### Save your raw data

Save your raw data! Raw data take many forms. For many of us, the raw data are those returned by the experimental software; for others, the raw data are videos of the experiment being carried out. Regardless of the form of these data, they should be retained, as they are often the only way to check issues in whatever processing pipeline brings these data from their initial state to the form you analyze. They also can be invaluable for addressing critiques or questions about your methods or results later in the process. If you need to correct something about your raw data, *do not alter the original files*. Make a copy, and make a note about how the copy differs from the original. Future you will thank present you for explaining why there are two copies of subject 19's data.

Raw data are often not anonymized, and anonymizing them may involve altering them! That means they may need to be stored in a different way than anonymized data. 



A word about subject identifiers. These should be anonymous identifiers that cannot be linked to participant identities and are unique.^[You laugh! One of us was in a lab where all the subject IDs were the date of test and the initials of the participant. These were neither unique nor anonymous.] One common convention is to give your study a code-name and to number participants sequentially, so your first participant in a sequence of experiments on information processing might be `INFO-1-01`. 

## Document your data collection process

- Critical value of preserving (and sharing) specifics of data collection.
- Value of raw source video (Gilmore and Adolph 2017)

### Organize your data for later analysis (spreadsheet version)

There are many forms that data come in, but chances are that at some point during your project you will end up with a spreadsheet full of data. Good spreadsheets mean the difference between success and failure! If you organize your spreadsheet well, it will be easily accessible by future (meta-)analysts as well as your future self. If you organize it poorly, it will be difficult to extract data from automatically and may make it more likely for errors to be introduced into your dataset. 

A wonderful article by @broman2018 gives a guide to spreadsheet organization that lays out the principles of good spreadsheet design. We review some of the highlights of their principles here (giving our own, opinionated ordering):

```{r management-broman-nonrect, fig.cap="Examples of non-rectangular spreadsheet formats that are likely to cause problems in analysis. From Broman and Woo (2018)."}
knitr::include_graphics("images/management/broman2018.png")
```

1. Make it a rectangle. The first principle of organizing tabular data is 

2. Choose good names for your variables. No one convention for name formatting is best, but it's important to be consistent. We tend to follow the [tidyverse style guide](https://style.tidyverse.org) and use lowercase words separated by underscores (`_`). It's also polite (and good science) to give units where these are available, e.g. are reaction times in seconds or milliseconds. Table \@ref(tab:management-broman-ex) gives some examples of good and bad variable names. 

```{r management-broman-ex, fig.margin=TRUE}

names <- tribble(~`Good name`, ~`Good alternative`, ~`Avoid`,
                 "subject_id", "SubID", "subject #",
                 "sex", "female", "M/F",
                 "rt_msec","reaction_time_ms", "reaction time (millisec.)")

knitr::kable(names, booktabs = TRUE, caption = "Examples of good and bad variable names. Adapted from Broman and Woo (2018).")
```

3. Be consistent with your formatting. Write dates as YYY-MM-DD

4. One thing in each cell (and no empty cells). 



5. Formatting isn't data. If you highlight cells or color them, there is typically no legend for this formatting, so its interpretation is ambiguous. Further, this kind of formatting is typically program-specific so it will be missed by any user who opens the file in a different analysis program (e.g., by reading an Excel spreadsheet into R). 

6. Save data in plain text files. The CSV (comma-delimited) file format is a common standard for easy-to-read data.^[Be aware of some interesting differences in how these files are output by European vs. American versions of Microsoft Excel! You might find semi-colons instead of commas in some datasets.] The advantage of CSVs is that they are not proprietary to Microsoft or another tech company, can be inspected in a text editor, and will not allow you to save all the kinds of formulas. 

We advise that you don't analyze your data in Excel, as you've seen in our previous chapters. But if you do feel the need to analyze your data in Excel or another spreadsheet program, we urge you to save the raw data as a separate CSV and then create distinct analysis spreadsheets. 

### Organize your data for later analysis (software version)

For many  researchers who collect data using software. Some software packages have control over the form in which their data 

## Sharing Research Products

### If you like it then you've gotta put a license on it

Academic culture is (usually) unburdened by a lot of discussion of intellectual property and legal rights. In the world of ideas, there are scholarly norms about citation and attribution, and someone can be rightly miffed if you don't acknowledge their contributions to the literature. But they won't usually sue you. 

On the other hand, when you create research products and let people reuse them, you are entering into a slightly different space. Perhaps you created software that a company would like to use. Maybe a pediatrician would like to use a research instrument you've been working on to assess their patients. 

### What can you share

We begin by discussing some of the practices

```{r, fig.cap="A decision flow chart for thinking about sharing research products. From Klein et al. (2018)."}
knitr::include_graphics("images/management/kline1.png")
```


### Privacy risks

- What are the risks from data being identifiable to a particular individual?
- Regulatory and ethical frameworks including US DHHS (Department of Health and Human Services) “safe harbor” and European GDPR (General Data Protection Regulation). 


::: {.ethics-box}
🌿 Ethics box: Really anonymous? 

When we had first begun teaching Psych 251, our experimental methods course at Stanford, we 

Early on an awkward email from an MTurk study participant leads to the realization that “anonymous IDs” are not so anonymous after all. More generally, statistical reidentification is easy, even from “anonymized data” (Rocher, Hendrickx, and de Montjoye 2019).
:::


### Benefits of appropriate data management

Data archiving. Making data FAIR: Findable, Accessible, Interoperable, and Reusable (Wilkinson et al. 2016)

- Licensing to prevent legal obstacles for reuse. 
- Selecting an appropriate repository – highlight OSF as fulfilling the FAIR standards, but provide other alternatives (e.g. Dataverse). 



::: {.accident-report}
⚠️ Accidental report: security practices for databases (how not to get hit by a ransomware attack)
:::
