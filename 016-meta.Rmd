# (PART) Contextualizing your study {-}

# Meta-analysis {#meta}


::: {.learning-goals}
üçé Learning goals: practice being a critical reader of the literature; extract effect sizes; conduct a simple random-effects meta analysis; monitor within- and across-study biases that can decrease precision and increase bias. 
:::

::: {.case-study}
üî¨ Case study: Hotel towel reuse (Scheibehenne, Jamil, and Wagenmakers 2016). A simple example of aggregating noisy evidence from replications. 



## Meta-notes

### Undefined notation and terminology

- "Narrative review"
- Need clear terminology to distinguish estimates from population effects; keep synced with earlier chapters



## Introduction?

In a widely-cited study on the power of describing social norms, Goldstein, Cialdini, and Griskevicius (2008) gave one group of hotel guests a message that encouraged them to reuse their towels and another group of hotel guests a message that indicated that most other guests reused their towels. Across two studies, they found that guests who received the social norm message were more likely to reuse their towels than guests who received the control message (Study 1 *p* = .05, Study 2 *p* = .03). Five later replications, though, all failed to find significant evidence of increases in towel reuse amongst guests receiving the social norm message (all *p*s \> .05).

At first glance, you may be enticed to conclude that the effect of social norm messages does not reliably impact towel reuse. You may even go a step further and try to think of explanations for why one team found the effect, but the other did not. (Maybe the messages change behavior when guest receive nice fluffy towels, but doesn't work when guests receive sets of crappy towels.) Or, if you're a practitioner of experimentology, you may ask yourselves whether these findings are really all that different. You may even be so bold to do a meta-analysis!

If you did the latter, you would find that the results across these studies are, for the most part, quite consistent. Guests in four out of five of the studies were more likely to reuse their towels after receiving social norm messages, but the difference was not large enough in any one study to confidently conclude that this difference was larger than zero. When the results were statistically combined in a meta-analysis, Scheibehenne, Jamil, and Wagenmakers (2016) found that social norm messages *on average* do increase hotel towel reuse.

It's not magic--it's just meta-analysis! In this chapter, you'll learn more about this useful technique.

Notes to Maya: Scheibehenne et al. did not estimate heterogeneity, so I do not know how consistent these results actually are. One of the studies did actually yield a sizeable oppostite-direction effect, so I'm not really sure if their fixed-effect assumption is reasonable. (Note, this was a commentary and tutorial about Bayesian meta-analysis, not an actual meta-analysis. So some of the methodological details are somewhat weak.) We may want to look into a scenario where (a) a fixed effect meta-analysis is better justified, (b) the direction of the effects is consistent across studies, but (c) the statistical significance of the effects is not consistent across studies.
:::

What distinguishes meta-analysis from other types of reviews? Literature review typology, organized by your search, appraisal, synthesis, and analysis strategy (Grant and Booth 2009).

When many students hear the term "review," they probably have flashbacks to a time where they threw some search term into Google Scholar, downloaded a bunch of articles that looked interesting, spent a few hours reading those articles, and then wrote a summary of what has been learned from these studies. There's not necessarily anything wrong with this approach, but there are a few things that can be additionally learned from doing a meta-analysis.

Meta-analysis is a statistical technique for combining results from multiple studies.

In addition to reading articles, a researcher performing a meta-analysis would also extract information about the *effect size*.

After this information has been extracted, the researcher would perform an analysis where these effect sizes are combined in a single set of analyses. By combining information from multiple studies, the researcher can make a more precise estimate of the size of some effect. The researcher can also look at the extent to which this effect varies across studies. They may even them perform analyses indicating whether studies with certain characteristics systematically produce different results.

Think about how meta-analysis might give you a different impression of an area of study than a literature review. If you would have performed a literature review on studies examining the effect of social norm messaging on hotel towel re-use, you would have likely come away with the impression that there is an unreliable effect at best and no effect at worst. However, when statistically combining the results of these studies, a meta-analyst is able to see that the findings are more consistent than they appear at first glance--and that, when considering all the available evidence, it actually appears that social norm messaging does decrease hotel towel reuse. Considering that, for example, there were 1.7 billion overnight hotel bookings in the European Union alone in 2013 (Eurostat, 2015), the difference in the types of the conclusions formed by a literature review and a meta-analysis matter quite a bit!

Accident report: Money priming and vote counting.

- Reading the literature revisited: how to see the ‚Äúbones‚Äù of the study ‚Äì what is the manipulation, what‚Äôs the measure, how reliable and valid is it, how precise and generalizable is it, what are the inferences?
- How not to do a meta-analysis (approaches that are intuitive but wrong)
    - Don‚Äôt just count the significant positive studies (vote counting); don‚Äôt just average the estimates.
    
::: {.accident-report}
‚ö†Ô∏è Accident report: Money priming and vote counting. Vadillo et al. (2016) show a case where vote counting indicates that there is overwhelming support for money priming, but a meta-analysis reveals that the support is limited. 
:::

## Running example: The contact hypothesis

[Introduce the applied example]


## Intuitive, but wrong, approaches to evidence synthesis

We have seen why it is important to synthesize evidence across studies on the same topic rather than just focus on one study at a time. How should we actually do this in practice? What most of us do intuitively in this situation is to essentially count how many studies supported the hypothesis under investigation, versus how many did not support the hypothesis. Such a count usually comes down to counting the number of studies that are "significant", since (for better or for worse) "significance" is largely what drives studies' take-home conclusions (CITE, e.g., McShane). This is called "vote-counting" (CITE HEDGES). Indeed, many narrative reviews take essentially this approach, even if they do not say so explicitly. For example, in Paluck et al.'s meta-analysis on the contact hypothesis, 11/27 (EYEBALLED) studies were significant, all with positive point estimates. So it seems like most studies do not support the contact hypothesis. 

But despite its intuitive appeal, vote-counting can be very misleading. First, this approach characterizes evidence solely in terms of dichotomized p-values, while entirely ignoring effect sizes; this is the very same fetishism of statistical "significance" that we introduced in [Chapter XXX on replication] and is subject to all the same limitations. In the context of evidence synthesis, it could be the case that relatively few studies are "significant" (for example, in a literature of typically small studies), yet if the studies typically have large point estimates, then the consensus of evidence could actually be quite strong. Or inversely, it could be that nearly all studies are "significant", but if they typically have small point estimates, then the consensus of evidence might still be rather weak. 

So our evidence synthesis needs to account for effect sizes themselves. How about if we just average the studies' estimates? For example, in Paluck et al.'s meta-analysis, the mean of the studies' estimates was XXX. This approach is perhaps a step in the right direction, but still has some important limitations. In particular, this approach gives equal weight to each study, such that a small study (like Hull et al., with a sample size of XXX [note: this had the second-smallest RE weight]) contributes as much to the mean effect size as a large study (like Boisjoly et al., with a sample size of XXX [note: this had the highest RE weight]). That doesn't make sense: larger studies should clearly carry more weight in the analysis. 





## FROM OUTLINE

Basic meta-analysis approaches

- Extracting effect sizes. Effect sizes for continuous data, binary data, and correlations.
- Fixed and random effects meta-analysis. Distinction between fixed and random-effects meta-analysis, assumptions of both models, and the meaning of their summary effects.
- Heterogeneity as a feature rather than a bug; summarizing effect sizes in the presence of heterogeneity. 
- Meta-regression. Predicting heterogeneity by examining continuous and categorical moderators of effects.

Within-study biases 

- Selection bias, performance bias, detection bias, attrition bias, reporting bias (Mathur and VanderWeele, n.d.)
- Limiting within-study biases at the outset by carefully defining methodological inclusion criteria
- Characterizing within-study biases after the fact: the Cochrane Risk of Bias tool.

Across-study biases

::: {.case-study}
üî¨ Case study: Publication bias in the social sciences: Unlocking the file drawer (Franco, Malhotra, and Simonovits 2014)
:::

- Limiting across-study biases at the outset by casting a broad net, including grey literature
- Characterizing publication bias after the fact: funnel plot asymmetry, precision effects tests, weight-function modeling, sensitivity analyses

::: {.accident-report}
‚ö†Ô∏è Accident report: Garbage in, garbage out? Meta-analyzing bad research (Coles et al. 2019)
:::
