<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 11 Data collection | Experimentology" />
<meta property="og:type" content="book" />





<meta name="author" content="Michael C. Frank, Mika Braginsky, Julie Cachia, Nicholas Coles, Tom Hardwicke, Robert Hawkins, Maya Mathur, and Rondeline Williams" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Chapter 11 Data collection | Experimentology">

<title>Chapter 11 Data collection | Experimentology</title>

<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />




<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="toc/toc.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="part"><span><b>I Preliminaries</b></span></li>
<li><a href="1-intro.html#intro"><span class="toc-section-number">1</span> Introduction: theory and experiments</a></li>
<li><a href="2-replication.html#replication"><span class="toc-section-number">2</span> Replication, reproducibility and transparency</a></li>
<li><a href="3-ethics.html#ethics"><span class="toc-section-number">3</span> Ethics</a></li>
<li class="part"><span><b>II Design and Planning</b></span></li>
<li><a href="4-measurement.html#measurement"><span class="toc-section-number">4</span> Measurement</a></li>
<li><a href="5-design.html#design"><span class="toc-section-number">5</span> Design of experiments</a></li>
<li><a href="6-inference.html#inference"><span class="toc-section-number">6</span> Statistical inference for comparing groups</a></li>
<li><a href="7-models.html#models"><span class="toc-section-number">7</span> Statistical models for more complex designs</a></li>
<li><a href="8-sampling.html#sampling"><span class="toc-section-number">8</span> Sampling</a></li>
<li><a href="9-preregistration.html#preregistration"><span class="toc-section-number">9</span> Preregistration</a></li>
<li class="part"><span><b>III Execution</b></span></li>
<li><a href="10-selection.html#selection"><span class="toc-section-number">10</span> Replicating or extending an existing study</a></li>
<li><a href="11-collection.html#collection"><span class="toc-section-number">11</span> Data collection</a></li>
<li><a href="12-management.html#management"><span class="toc-section-number">12</span> Data management</a></li>
<li class="part"><span><b>IV Analysis and reporting</b></span></li>
<li><a href="13-viz.html#viz"><span class="toc-section-number">13</span> Visualization</a></li>
<li><a href="14-eda.html#eda"><span class="toc-section-number">14</span> Exploratory data analysis</a></li>
<li><a href="15-writing.html#writing"><span class="toc-section-number">15</span> Reproducible writing</a></li>
<li><a href="16-meta.html#meta"><span class="toc-section-number">16</span> Meta-analysis</a></li>
<li><a href="17-conclusions.html#conclusions"><span class="toc-section-number">17</span> Conclusions</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="collection" class="section level1" number="11">
<h1><span class="header-section-number">Chapter 11</span> Data collection</h1>
<div class="learning-goals">
<p>üçé Learning goals: Reason about the biases that can be introduced by different data collection methods; deploy manipulation checks.</p>
</div>
<div class="case-study">
<p>üî¨ Case study: Crump et al.¬†(2013) show through a set of beautiful experiments designed in the web browser how online data collection can replicate effects initially found in the lab.</p>
</div>
<div id="the-critical-importance-of-piloting" class="section level2" number="11.1">
<h2><span class="header-section-number">11.1</span> The critical importance of piloting</h2>
<p>A <strong>pilot study</strong> is a small study conducted before you collect your main sample. Smooth and successful data collection is typically difficult without piloting, at least the first time you do an experiment of a given type. Fundamentally, experiments induce a particular experience in their participants, and careful attention to the nature of that experience<label for="tufte-sn-31" class="margin-toggle sidenote-number">31</label><input type="checkbox" id="tufte-sn-31" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">31</span> Even if the experience is somewhat tedious, like searching for a T amongst Ls for hundreds of trials!</span> requires iterative development.</p>
<p>Pilot studies cannot tell you about expected effect size (as we discussed in Chapter <a href="8-sampling.html#sampling">8</a>). They also cannot tell you about the significance of your main result. What they <em>can</em> do is tell you about whether your paradigm works. They can reveal:
* if your code crashes under certain circumstances
* if your instructions confuse a substantial portion of your participants
* if you have a very high dropout rate
* if your data collection procedure fails to log variables of interest
* if participants are disgruntled by the end of the experiment</p>
<p>We recommend that all experimenters do ‚Äì at the very minimum ‚Äì two pilot studies before they launch their experiment.</p>
<p>The first pilot study, <strong>pilot A</strong><label for="tufte-sn-32" class="margin-toggle sidenote-number">32</label><input type="checkbox" id="tufte-sn-32" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">32</span> Good name, right?</span>, is a test with non-naive participants. Your parents can do this experiment, or in a pinch you can run yourself a bunch of times (though this isn‚Äôt preferable because you‚Äôre likely to miss a lot of aspects of the experience that you are habituated to, especially if you‚Äôve been debugging the software). The goal of pilot A is to ensure that your experiment is comprehensible, that participants can complete it, and that the data are logged appropriately. This last goal means that you must <em>analyze</em> the data from pilot A, at least to the point of checking that the relevant data about each trial is logged.<label for="tufte-sn-33" class="margin-toggle sidenote-number">33</label><input type="checkbox" id="tufte-sn-33" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">33</span> At a minimum, for each trial you need to know a subject ID, a trial ID, the state of any manipulation (condition, trial type, etc.), and the value for the measure.</span></p>
<p>The second pilot study, <strong>pilot B</strong>, consists of a test of a small set of naive participants. Pilot size will depend on the costliness of running the experiment (in time, money, and opportunity cost) as well as your worries about the paradigm. If we‚Äôre talking about a short online survey experiment, then running a pilot of 10‚Äì20 people is reasonable. A more extensive laboratory study might be better served by piloting just two or three people. The goal of this second study is to understand properties of the participant experience: for example, were they confused? Did they withdraw before the study finished? You won‚Äôt have the numbers to make robust statistical inferences about these questions, but even a small number of pilots can tell you that your dropout rate is likely too high: if 5 of 10 pilot participants withdraw you may need to reconsider aspects of your design. It‚Äôs critical for pilot B that you debrief more extensively with your participants. This debriefing often takes the form of an interview questionnaire after the study is over (‚Äúwhat did you think the study was about?‚Äù and ‚Äúis there any way we could improve the experience of being in the study?‚Äù can be helpful questions).</p>
<p>Piloting is often an iterative process. We frequently launch studies for a pilot B, then recognise from the data or from participant feedback that they can be improved. We make tweaks and pilot again. Be careful not to overfit to small differences in pilot data ‚Äì the samples are small and so inferences will not be robust. The process should be more like workshopping a manuscript to remove typos and make it read better than doing a study.</p>
<p>In the case of especially expensive experiments, it can be a dilemma whether to run a larger pilot to identify difficulties since such a pilot will be costly. In these cases, one possibility can be to preregister a contingent strategy. For example, in a planned sample of 100 participants, you could preregister running 20 as a pilot sample with the stipulation that you will look only at their dropout rate and not at any condition differences in the target measure. Then the registration could state that if the dropout rate is lower than 25%, you will collect the next 80 participants and analyze the whole dataset including the initial pilot. This sort of registration can help you split the difference between cautious piloting and conservation of rare or costly data.</p>
</div>
<div id="data-collection-online" class="section level2" number="11.2">
<h2><span class="header-section-number">11.2</span> Data collection online</h2>
<p>Online data collection is increasingly ubiquitous in the behavioral sciences. Further, the web browser ‚Äì alongside survey software like Qualtrics ‚Äì can be a major aid to transparency in sharing experimental materials.</p>
<ul>
<li>Validating the process of collecting data online. We briefly review studies suggesting that for general data collection across many paradigms, online data collection is valid.</li>
<li>When is online not enough? We describe cases where in-person data collection is necessary, highlighting psychophysical and physiological measurement and social interaction as two common classes of experiments that still cannot be done effectively online.</li>
</ul>
</div>
<div id="manipulation-checks" class="section level2" number="11.3">
<h2><span class="header-section-number">11.3</span> Manipulation checks</h2>
<p>Data collection in the field: An opinionated discussion of common pitfalls of field experiments in psychology.</p>
<ul>
<li>Blinding and randomization. Fieldwork makes it harder to maintain these critical principles of experimental design, potentially leading to bias.</li>
<li>Reasoning about and combatting selection bias.</li>
</ul>
</div>
<div id="experimental-practices-beliefs-and-superstitions" class="section level2" number="11.4">
<h2><span class="header-section-number">11.4</span> Experimental practices, beliefs, and superstitions</h2>
<div class="accident-report">
<p>‚ö†Ô∏è Accident report: Does data quality vary throughout the semester?</p>
<p>Every lab that collects empirical data repeatedly using the same population builds up lore about how that population varies. One infant development lab famously repainted their walls a particularly bright shade of blue and claimed that their studies did not yield significant findings (even replicating highly robust paradigms) until they went back to a more neutral color. ‚Ä¶</p>
<p>The ManyLabs studies were a series of large-scale, collaborative studies that involved the same experimental protocol being run at a variety of different sites.</p>
</div>
</div>
<div id="documenting-your-data-collection" class="section level2" number="11.5">
<h2><span class="header-section-number">11.5</span> Documenting your data collection</h2>
<p>Save your raw data! Raw data take many forms. For many of us, the raw data are those returned by the experimental software; for others, the raw data are videos of the experiment being carried out. Regardless of the form of these data, they should be retained, as they are often the only way to check issues in whatever processing pipeline brings these data from their initial state to the form you analyze. They also can be invaluable for addressing critiques or questions about your methods or results later in the process. If you need to correct something about your raw data, <em>do not alter the original files</em>. Make a copy, and make a note about how the copy differs from the original. Future you will thank present you for explaining why there are two copies of subject 19‚Äôs data.</p>
<p>Raw data are often not anonymized, and anonymizing them may involve altering them! That means they may need to be stored in a different way than anonymized data.</p>
<ul>
<li>Critical value of preserving (and sharing) specifics of data collection.</li>
<li>Value of raw source video (Gilmore and Adolph 2017)</li>
</ul>
<p>A word about subject identifiers. These should be anonymous identifiers that cannot be linekd to participant identities and are unique.<label for="tufte-sn-34" class="margin-toggle sidenote-number">34</label><input type="checkbox" id="tufte-sn-34" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">34</span> You laugh! One of us was in a lab where all the subject IDs were the date of test and the initials of the participant. These were neither unique nor anonymous.</span> One common convention is to give your study a code-name and to number participants sequentially, so your first participant in a sequence of experiments on information processing might be <code>INFO-1-01</code>.</p>

</div>
</div>
<p style="text-align: center;">
<a href="10-selection.html"><button class="btn btn-default">Previous</button></a>
<a href="12-management.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="toc/toc.js"></script>


</body>
</html>
