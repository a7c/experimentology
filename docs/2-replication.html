<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 2 Replication, reproducibility, and transparency | Experimentology" />
<meta property="og:type" content="book" />





<meta name="author" content="Michael C. Frank, Mika Braginsky, Julie Cachia, Nicholas Coles, Tom Hardwicke, Robert Hawkins, Maya Mathur, and Rondeline Williams" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Chapter 2 Replication, reproducibility, and transparency | Experimentology">

<title>Chapter 2 Replication, reproducibility, and transparency | Experimentology</title>

<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />




<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="toc/toc.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="part"><span><b>I Preliminaries</b></span></li>
<li><a href="1-intro.html#intro"><span class="toc-section-number">1</span> Introduction: theory and experiments</a></li>
<li><a href="2-replication.html#replication"><span class="toc-section-number">2</span> Replication, reproducibility, and transparency</a></li>
<li><a href="3-ethics.html#ethics"><span class="toc-section-number">3</span> Ethics</a></li>
<li class="part"><span><b>II Statistics</b></span></li>
<li><a href="4-estimation.html#estimation"><span class="toc-section-number">4</span> Estimation</a></li>
<li><a href="5-inference.html#inference"><span class="toc-section-number">5</span> Inference</a></li>
<li><a href="6-models.html#models"><span class="toc-section-number">6</span> Models</a></li>
<li class="part"><span><b>III Design and Planning</b></span></li>
<li><a href="7-measurement.html#measurement"><span class="toc-section-number">7</span> Measurement</a></li>
<li><a href="8-design.html#design"><span class="toc-section-number">8</span> Design of experiments</a></li>
<li><a href="9-sampling.html#sampling"><span class="toc-section-number">9</span> Sampling</a></li>
<li><a href="10-preregistration.html#preregistration"><span class="toc-section-number">10</span> Preregistration</a></li>
<li class="part"><span><b>IV Execution</b></span></li>
<li><a href="11-selection.html#selection"><span class="toc-section-number">11</span> Replicating or extending an existing study</a></li>
<li><a href="12-collection.html#collection"><span class="toc-section-number">12</span> Data collection</a></li>
<li><a href="13-management.html#management"><span class="toc-section-number">13</span> Data management and sharing</a></li>
<li class="part"><span><b>V Analysis and Reporting</b></span></li>
<li><a href="14-viz.html#viz"><span class="toc-section-number">14</span> Visualization</a></li>
<li><a href="15-eda.html#eda"><span class="toc-section-number">15</span> Exploratory data analysis</a></li>
<li><a href="16-writing.html#writing"><span class="toc-section-number">16</span> Reproducible writing</a></li>
<li><a href="17-meta.html#meta"><span class="toc-section-number">17</span> Meta-analysis</a></li>
<li><a href="18-conclusions.html#conclusions"><span class="toc-section-number">18</span> Conclusions</a></li>
<li class="part"><span><b>VI Appendices</b></span></li>
<li><a href="19-github.html#github"><span class="toc-section-number">19</span> Github Tutorial</a></li>
<li><a href="20-rmarkdown.html#rmarkdown"><span class="toc-section-number">20</span> R Markdown Tutorial</a></li>
<li><a href="21-tidyverse.html#tidyverse"><span class="toc-section-number">21</span> Tidyverse Tutorial</a></li>
<li><a href="22-ggplot.html#ggplot"><span class="toc-section-number">22</span> ggplot Tutorial</a></li>
<li><a href="23-instructors.html#instructors"><span class="toc-section-number">23</span> Instructor‚Äôs Guide</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="replication" class="section level1" number="2">
<h1><span class="header-section-number">Chapter 2</span> Replication, reproducibility, and transparency</h1>
<div class="learning-goals">
<p>üçé Learning goals:</p>
<ul>
<li>Understand the ‚Äúcrisis‚Äù narrative in psychology and the empirical evidence supporting it</li>
<li>Define the distinction between reproducibility and replicability</li>
<li>Consider types of replication</li>
<li>Reason about the relation of replication to theory building</li>
</ul>
</div>
<p>In the previous chapter, we gave a sober and considered introduction to the topic of experiments, their connection with causal inference, and their role in building psychological theory. In this chapter we‚Äôre going to change gears a little bit and tell the story of the period from 2011 ‚Äì 2021 and how it has given rise to a number of ‚Äúcrisis‚Äù narratives in psychology.</p>
<p>In order to set the terms of discussion, we need to describe some of the outcomes we are interested in. Figure <a href="2-replication.html#fig:replication-terms">2.1</a> gives us a basic starting point for our definitions.<label for="tufte-sn-1" class="margin-toggle sidenote-number">1</label><input type="checkbox" id="tufte-sn-1" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">1</span> These terms have been a bit of a problem for the field, at least at first, but it seems like people have been agreeing on them recently.</span> For some claim in a paper, if we can take the same data that were analyzed in that paper, do the same analysis, and get the same result, we call that result <strong>reproducible</strong> (sometimes, <strong>computationally reproducible</strong>). If we can collect new data in the same experiment, do the same analysis, and get the same result, we call that a <strong>replication</strong> and say that the experiment is <strong>replicable</strong>. If we can do a different analysis with the original dataset, we call this a <strong>robustness check</strong> and so if a claim passes it is <strong>robust</strong>.<label for="tufte-sn-2" class="margin-toggle sidenote-number">2</label><input type="checkbox" id="tufte-sn-2" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">2</span> These are less common in experimental psychology, but are very common in fields that work with large, complex observational datasets like sociology.</span> And if the same finding obtains in a different population, perhaps with a different analysis, the result is likely to be more <strong>generalizable</strong> beyond the initial conditions in which it was observed.<label for="tufte-sn-3" class="margin-toggle sidenote-number">3</label><input type="checkbox" id="tufte-sn-3" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">3</span> You might have observed that a lot of work is being done here by the word ‚Äúsame.‚Äù How do we operationalize same-ness for experimental procedures, statistical analyses, or samples? These are difficult questions that we‚Äôll address in part below, but there‚Äôs no single answer and so these terms are always going to helpful guides rather than exact labels.</span></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:replication-terms"></span>
<img src="images/replication/terms.png" alt="A terminological framework for meta-science discussions. Based on [https://figshare.com/articles/Publishing_a_reproducible_paper/5440621]()." width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 2.1: A terminological framework for meta-science discussions. Based on <a href="">https://figshare.com/articles/Publishing_a_reproducible_paper/5440621</a>.<!--</p>-->
<!--</div>--></span>
</p>
<p>We‚Äôre also going to abandon the sober tone of the introduction and try to get you a bit worked up. From an empirical perspective, things have been far from ideal in the psychology literature. Many classic findings may be wrong, or at least overstated. Their statistical tests are probably not trustworthy. The actual numbers are even wrong in many papers! And even when experimental findings are ‚Äúreal‚Äù they may not reflect deep psychological generalizations. And even if they do, they likely don‚Äôt reflect generalizations that are true about people in general, only some very specific groups of people. Your hair should be on fire, at least a little bit. If you by the end of this chapter, you don‚Äôt feel a little bit of despair about the published psychological literature, then we haven‚Äôt done our job.</p>
<p>On the other hand, you might be thinking, how do you know that all this bad stuff is true? Claims about a literature or field as a whole go beyond the kind of standard paradigmatic science that we were talking about in the previous chapter ‚Äì instead they are part of a new field called <strong>meta-science</strong>. Meta-science research is research <em>about research</em>, for example investigating the rate of replication in a literature, or trying to figure out how widespread some negative practice is within a subfield. Meta-science allows us to go beyond one-off anecdotes about particular results or rumors about bad practices. Critically, we can also evaluate the strength of claims about the field using the same tools we use to evaluate standard research ‚Äì that is, we can critique research designs and inferences systematically, rather than just accepting (or failing to accept) assertions.</p>
<p>In this</p>
<div class="case-study">
<p>üî¨ Case study: The Open Science Collaboration</p>
<p>Around 2011, we were teaching our Experimental Methods course for the first time, based on a course model that we had worked on with Rebeccca Saxe <span class="citation">(<label for="tufte-mn-1" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-1" class="margin-toggle">Frank and Saxe 2012<span class="marginnote">Frank, Michael C, and Rebecca Saxe. 2012. <span>‚ÄúTeaching Replication.‚Äù</span> <em>Perspectives on Psychological Science</em> 7: 595‚Äì99.</span>)</span>. The idea was to have a replication-based course that introduced students to the nuts and bolts of research.<label for="tufte-sn-4" class="margin-toggle sidenote-number">4</label><input type="checkbox" id="tufte-sn-4" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">4</span> One of the current author team was a student in the course that year!</span> A guy named Brian Nosek was on sabbatical nearby, and over coffee we learned that he was starting up an ambitious project to replicate a large sample of studies from top psychology journals in 2008.</p>
<p>Students in Experimental Methods that year chose replication projects from the sample that Nosek had told us about. Four of these projects were done at a high standard and were nominated by the course TAs for inclusion in the broader project on the basis of strong implementations of the paradigm and good sample sizes. A few years later, when the final group of 100 replication studies was completed, we got a look at the results, shown in Figure <a href="2-replication.html#fig:replication-osc-2015">2.2</a>.</p>
<div class="figure"><span id="fig:replication-osc-2015"></span>
<p class="caption marginnote shownote">
Figure 2.2: Results from the Open Science Collaboration (2015). Each point represents one of the studies in the sample, with the horizontal position giving the original effect size and the vertical position giving the replication effect size. Dot size shows estimated statistical power. The dotted line represents a perfect replication.
</p>
<img src="images/replication/osc-2015.png" alt="Results from the Open Science Collaboration (2015). Each point represents one of the studies in the sample, with the horizontal position giving the original effect size and the vertical position giving the replication effect size. Dot size shows estimated statistical power. The dotted line represents a perfect replication." width="\linewidth"  />
</div>
<p>The resulting meta-science paper made a substantial impression on both psychologists and the broader community, defining both a field of psychology meta-science studies and providing a template for many-author collaborative projects <span class="citation">(<label for="tufte-mn-2" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-2" class="margin-toggle">Collaboration 2015<span class="marginnote">Collaboration, Open Science. 2015. <span>‚ÄúEstimating the Reproducibility of Psychological Science.‚Äù</span> <em>Science</em> 349 (6251).</span>)</span>. But the most striking thing was the result: disappointingly, by several different criteria, only around a third of studies showed the same finding as the original. The others yielded smaller effects that were no significant in the replication sample. This study provided the first large-scale evidence that methodological issues in the psychological literature might lead to a bad outcome in the literature ‚Äì namely a literature with many findings that didn‚Äôt replicate.</p>
<p>The study‚Äôs results ‚Äì and their interpretation ‚Äì were controversial, however, and much ink was spilled on what these data showed. In particular, critics pointed to different degrees of fidelity between the original studies and the replications; insufficient levels of statistical power in the replications; non-representative sampling of the literature; and difficulties identifying specific statistical outcomes for replication success <span class="citation">(<label for="tufte-mn-3" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-3" class="margin-toggle">Gilbert et al. 2016<span class="marginnote">Gilbert, Daniel T, Gary King, Stephen Pettigrew, and Timothy D Wilson. 2016. <span>‚ÄúComment on <span>‚ÄòEstimating the Reproducibility of Psychological Science‚Äô</span>.‚Äù</span> <em>Science</em> 351 (6277): 1037‚Äì37.</span>; <label for="tufte-mn-4" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-4" class="margin-toggle">Anderson et al. 2016<span class="marginnote">Anderson, CJ, S Bahnik, M Barnett-Cowan, FA Bosco, J Chandler, CR Chartier, and otherss. 2016. <span>‚ÄúResponse to Comment on <span>‚ÄòEstimating the Reproducibility of Psychological Science‚Äô</span>.‚Äù</span> <em>Science</em> 351 (6277): 1037‚Äì37.</span>)</span>. In our view, many of these critiques have merit, and you can‚Äôt simply interpret the results of this stufy as an unbiased estimate of the replicability of results in the literature, contra the title.<label for="tufte-sn-5" class="margin-toggle sidenote-number">5</label><input type="checkbox" id="tufte-sn-5" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">5</span> Confusingly, the title of the paper is ‚ÄúEstimating the reproducibility of psychological science,‚Äù not ‚Äúthe replicability of psychological science.‚Äù This caused terminological confusion for several years; it seems like at this point people have decided it‚Äôs just a mistake.</span></p>
<p>And yet, the results are still important and compelling, and they undeniably changed the direction of the field of psychology. Many good studies are like this ‚Äì they have critical flaws but they lead the way towards other studies that pursue this new direction, often with greater technical precision and fewer issues. For several of us personally, working on this project was also transformative in that it showed us the power of collaborative work ‚Äì together we could do a study that no one of us had any hope of completing on our own, and potentially make a difference in our field.</p>
</div>
<div id="reproducibility" class="section level3" number="2.0.1">
<h3><span class="header-section-number">2.0.1</span> Reproducibility</h3>
<p>Critical notion of the ‚Äúprovenance chain‚Äù for specific numbers - that they can be traced back to analytic computations.</p>
</div>
<div id="replication-1" class="section level3" number="2.0.2">
<h3><span class="header-section-number">2.0.2</span> Replication</h3>
<p>questionable research practices</p>
<p>Publication bias</p>
<div class="ethics-box">
<p>üåø Ethics box: Consequences for the study, consequences for the person</p>
<p>‚ÄúPower posing‚Äù is the idea that changing your physical posture to make it more open and expansive might also change your confidence. <span class="citation"><label for="tufte-mn-5" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-5" class="margin-toggle">Carney, Cuddy, and Yap (2010)<span class="marginnote">Carney, Dana R, Amy JC Cuddy, and Andy J Yap. 2010. <span>‚ÄúPower Posing: Brief Nonverbal Displays Affect Neuroendocrine Levels and Risk Tolerance.‚Äù</span> <em>Psychological Science</em> 21 (10): 1363‚Äì68.</span></span> reported a striking study of this phenomenon, in which 42 participants were told they were taking part in a study of physiological recording. They then held two poses, each for a minute. In one condition, the poses were expansive (e.g., legs out, hands on head) or contractive (e.g., arms and legs crossed). Participants in the expansive pose condition showed increases in testosterone and decreases in salivary cortisol (a stress marker), they took a greater number of risk in a gambling task, and they reported that they were more ‚Äúin charge‚Äù in a survey. This result suggested that a two-minute induction could lead to striking physiological and psychological changes ‚Äì in turn leading to power posing becoming firmly enshrined as part of the set of recommended strategies in business and elsewhere. The original publication contributed to the rise of the researchers‚Äô careers, including becoming a principal piece of evidence in a hugely-popular TED talk by Amy Cuddy, one of the authors.</p>
<p>This result is likely not definitive, however. A replication study with a larger number of participants (N=200) failed to find evidence for physiological effects, even as it did find some effects on participants‚Äô own beliefs <span class="citation">(<label for="tufte-mn-6" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-6" class="margin-toggle">Ranehill et al. 2015<span class="marginnote">Ranehill, Eva, Anna Dreber, Magnus Johannesson, Susanne Leiberg, Sunhae Sul, and Roberto A Weber. 2015. <span>‚ÄúAssessing the Robustness of Power Posing: No Effect on Hormones and Risk Tolerance in a Large Sample of Men and Women.‚Äù</span> <em>Psychological Science</em> 26 (5): 653‚Äì56.</span>)</span>. And a review of the published literature suggested that many findings appeared to be the result of some sort of publication bias, as far too many of them had <span class="math inline">\(p\)</span>-values very close to the .05 threshold <span class="citation">(<label for="tufte-mn-7" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-7" class="margin-toggle">Simmons and Simonsohn 2017<span class="marginnote">Simmons, Joseph P, and Uri Simonsohn. 2017. <span>‚ÄúPower Posing: P-Curving the Evidence.‚Äù</span> <em>Psychological Science</em>.</span>)</span>. In light of this evidence, the first author made a public statement that she <a href="https://faculty.haas.berkeley.edu/dana_carney/pdf_my%20position%20on%20power%20poses.pdf">does not believe that ‚Äúpower pose‚Äù effects are real</a>.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:replication-powerpose"></span>
<img src="images/replication/powerpose.png" alt="Google trends time series for &quot;power pose&quot; from 2004-2021." width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 2.3: Google trends time series for ‚Äúpower pose‚Äù from 2004-2021.<!--</p>-->
<!--</div>--></span>
</p>
<p>From the scientific perspective, it‚Äôs very tempting to take this example as a case in which the scientific ecosystem corrects itself. Although many people continue to cite the original power posing work, we suspect the issues are well-known throughout the social psychology community, and overall interest has gone down (see Figure <a href="2-replication.html#fig:replication-powerpose">2.3</a>. But this narrative masks the very real human impacts of the self-correction process, which can raise ethical questions about the best way to address issues in the scientific record.</p>
<p>The process of debate and discussion around individual findings can be bruising and complicated. In the case of power posing, Cuddy herself was tighly associated with the findings and many critiques of the findings became critiques of the individual. Several commentators used Cuddy‚Äôs name as a synecdoche for low-quality psychological results, likely because of her prominence and perhaps because of her gender and age as well. These comments were likely harmful to Cuddy personally and her career more generally.<label for="tufte-sn-6" class="margin-toggle sidenote-number">6</label><input type="checkbox" id="tufte-sn-6" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">6</span> For further reading, see <a href="https://www.nytimes.com/2017/10/18/magazine/when-the-revolution-came-for-amy-cuddy.html">‚ÄúWhen the Revolution Came for Amy Cuddy‚Äù</a>.</span></p>
<p>Scientists should critique, reproduce, and replicate ‚Äì these are all critical parts of the progress of normal science. But it‚Äôs important to do this in a way that‚Äôs sensitive to the people involved. Here are a few guidelines for courteous and ethical conduct:</p>
<ul>
<li>Communicate personally before communicating publicly. As one critic said about the power posing debate, ‚Äú‚ÄúI wish I‚Äôd had the presence of mind to pick up the phone and call [before publishing my critique].‚Äù</li>
<li>Always communicate about the work, never the person. Try to use language that is specific to the analysis or design being critiqued, rather than the person who did the analysis or thought up the design.</li>
<li>Avoid using language that assumes negative intentions, e.g.¬†‚Äúthe authors misleadingly state that ‚Ä¶‚Äù</li>
<li>Ask for someone to read your paper, email, blogpost, or tweet before you hit send. It can be very difficult to figure out the tone of your writing without an external perspective.</li>
</ul>
<p>As we will argue in the next chapter, we have an ethical duty as scientists to promote good science and critique low quality science. But we also have duties to our colleauges and communities to be good to one another.</p>
</div>
</div>
<div id="a-framework-for-replication" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> A framework for replication</h2>
<ul>
<li>We describe the framework for replication in Zwaan et al.¬†(2018), highlighting the idea of multiple dimensions on which a replication can differ from the original study and the ways that this complicates inferences about replication ‚Äúsuccess‚Äù (Mathur and VanderWeele 2019, 2020).</li>
<li>We can get different epistemic value from doing direct replications (attempt to copy original study with as much fidelity as possible) vs.¬†conceptual replications (attempt to somewhat perturb operationalization of original study).</li>
</ul>
<div class="accident-report">
<p>‚ö†Ô∏è Accident report: The ‚Äúsmall telescopes‚Äù case study of Simonsohn et al.¬†(2015): what if weather really does affect mood, but the effect is too small for the original study to ever detect?</p>
<blockquote>
<p>Imagine I claimed our next-door neighbor was a billionaire oil sheik who kept thousands of boxes of gold and diamonds hidden in his basement. Later we meet the neighbor, and he is the manager of a small bookstore and has a salary 10% above the US average‚Ä¶ Should we describe this as ‚Äúwe have confirmed the Wealthy Neighbor Hypothesis, though the effect size was smaller than expected?‚Äù Or as ‚ÄúI made up a completely crazy story, and in unrelated news there was an irrelevant deviation from literally-zero in the same space?‚Äù</p>
</blockquote>
</div>
</div>
<div id="relation-to-theory-building" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Relation to theory building</h2>
<p>How do reproducibility and replicability contribute to theory building? We draw on Hardwicke et al.¬†(2018), considering these factors in the informativeness of an experiment.</p>
<p>Introduce p-hacking and publication biases as major sources of irreproducibility. These are biases - and we highlight bias reduction as a key goal.</p>
<div class="accident-report">
<p>‚ö†Ô∏è Accident report: When I‚Äôm 64?</p>
<p>Simmons, Simonsohn, Nelson introduce p-hacking into our lexicon.</p>
</div>
<div class="case-study">
<p>üî¨ Case study: Hardwicke et al.¬†(2018; 2021) meta-studies of the analytic reproducibility of specific analytic findings in the empirical literature.</p>
</div>

</div>
</div>
<p style="text-align: center;">
<a href="1-intro.html"><button class="btn btn-default">Previous</button></a>
<a href="3-ethics.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="toc/toc.js"></script>


</body>
</html>
