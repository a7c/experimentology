---
output:
  pdf_document: default
  html_document: default
---
# (PART) Statistics {.unnumbered}

# Estimation {#estimation}

Idea of a sampling distribution


## Meta-notes

### Topics not yet covered

### Not sure if there should be covered

* Measures of effect-size for binary outcomes (maybe a brief mention?)



## Introduction

An alternative(?) way to frame our statistical practice is to start from the idea of estimation.

<!-- ::: {.case-study} -->
<!-- 🔬 Case study: The lady tasting tea -->
## The lady tasting tea

The birth of modern statistical inference came from a single, epochal act of mansplaining.^[An important piece of context for the work of Ronald Fisher, Karl Pearson, and other early pioneers of statistical inference is that they were all strong proponents of eugenics. Fisher was the founding Chairman of the Cambridge Eugenics Society. Pearson was perhaps even worse, an avowed Social Darwinist who believed fervently in Eugenic legislation. These views are repugnant.] Sir Ronald Fisher was apparently at a party when a lady declared that she could tell the difference between cups when the tea was added to the milk vs. the milk to the tea. Rather than taking her at her word, Fisher devised an experimental and data analysis procedure to test her claim.^[If you're interested in this history, we recommend @salsburg2001's delightful book, "The Lady Tasting Tea," about the origins of modern statistics.]

The basic schema of the experiment was that the lady would have to judge a set of new cups of tea and sort them into milk-first vs. tea-first sets. Her data would then be analyzed to determine whether her level of correct choice exceeded that expected by chance. While this process now sounds like a quotidian experiment that might be done on a cooking reality show, in fact this is one of those touchstones that feels unremarkable because it literally established the way science was done for the next century. 

The first element of the experiment that was unusual was its treatment of design confounds such as pouring order or cup material. Prior experimental practice would have been to try to equate all of the cups as closely as possible, decreasing the influence of confounders. Fisher recognized that this strategy was insufficient and that random assignment was critical for making strong causal inferences about the treatment (milk then tea vs. tea then milk).^[We discussed the causal power of random assignment in Chapter \@ref(intro) but here's where this idea originates!]^[MM: Minor, but there were written records of RCTs well before this (e.g., to investigate scurvy treatments in the 18th c.).]

The second innovation was the idea of creating a model of what might happen during the experiment: specifically, a **null model** in which the nameless lady chose cups by chance rather than because of some tea sensitivity. The goal of Fisher's analysis strategy was then to compute the probability of the lady's choices under this null model. Fisher then declared that it is "usual and convenient for experimenters to take 5 percent as a standard level of convenience," establishing the .05 cutoff that has become gospel throughout the sciences.^[Actually, right after establishing .05 as a cutoff, Fisher then writes that "in the statistical sense, we thereby admit that no isolated experiment, however significant in itself, can suffice for the experimental demonstration of any natural phenomenon... in order to assert that a natural phenomenon is experimentally demonstrable we need, not an isolated record, but a reliable method of procedure. In relation to the test of significance, we may say that a phenomenon is experimentally demonstrable when we know how to conduct an experiment which will rarely fail to give us a statistically significant result." In other words, Fisher was all for replication!]

With apologies to Fisher, for our example, we'll design a slightly more modern version of his experiment using a forced-choice judgmeent measure. We present participants with a series of cups of tea (in random order, of course). Half are prepared milk-first, half tea-first. Then the participant must give an independent judgement about each. The null model is based on the idea that, if the participant knows nothing then they should choose at random between the two possibilities. Hence, we can use a simple binomial distribution to show expected chance performance for different sets of trials.^[We quantify our measure as the proportion of correct trials, since trials here are interchangeable.] 

These null distributions are shown in Figure \@ref(fig:inference-null-model).[^MM: The concept of a "null distribution" probably needs more scaffolding. In my experience with accidentally driving students crazy, I find that I have to go extremely slowly the first time I discuss sampling distributions.] We show cases with 6, 12, 24, and 48 trials, and highlight the .05 significance threshold -- the threshold above which you would fall only 5% of the time if you were guessing by chance.^[Note that we are effectively doing a "one-tailed" test here, since we're attending only to the right hand side of each plot and ignoring cases where the participant is exactly flipped in their judgments.] Conventionally, if you cross this threshold, you can say you "reject the null."

```{r inference-null-model, fig.cap="Probability of observing a particular number of trials correct under the null model of no sensitivity to tea preparation order. Each facet gives a different number of trials. The red line indicates a significance threshold above which data have less than a 5\\% chance of appearing."}
n_sims <- 10000
null_model <- expand_grid(n = c(6,12,24,48),
                          sim = 1:n_sims) %>%
  group_by(n) %>%
  mutate(correct = rbinom(n = n_sims, size = n, prob = .5)/n, 
         cutoff = quantile(correct, .95)) %>%
  group_by(n, correct) %>%
  summarise(n_correct = n(), 
            cutoff = cutoff[1]) %>%
  group_by(n) %>%
  mutate(prop_correct = n_correct / sum(n_correct))

ggplot(null_model, aes(x = correct, y = prop_correct)) + 
  geom_bar(stat = "identity") + 
  facet_wrap(~n) + 
  geom_vline(aes(xintercept = cutoff + .01), col = "red", lty = 2) + 
  ylab("Probability") + xlab("Proportion correct responses")
# dbinom(x = 8, size = 8, prob = .5)
```



## Maximum likelihood estimation: Fitting a model to the data

## Bayesian estimation: Incorporating prior beliefs

NEEDS TO BE FIXED WITH RESPECT TO CASE STUDY

Let's say we want to estimate some quantity, we'll call it $h$ -- our belief about the participant's accuracy.[^1] We observe some data $d$, consisting of the set of correct and incorrect responses in the experiment. Now we can use **Bayes' rule**, a tool from basic probability theory, to estimate this number.

[^1]: You can't get away from measurement and psychometrics! We said we were really interested in the effect of tea ordering on tea perception. But this number that we're estimating is something more like this particular participant's accuracy, and that's not the same thing that we actually wanted. To get from what we're estimating to our effect of interest, we'd need to establish some **linking hypotheses** about how the participant's accuracy can be derived from the participant's perception and the properties of the stimulus. That's perhaps not worth doing in this toy example, but more generally it's a critical part of getting from your measure to your construct of interest!

Bayes' rule says:

$$
\color{purple}{p(h | d)} = \frac{\color{red}{p(d | h)} \color{blue}{p(h)}}{\color{black}{p(d)}}.
$$ \noindent Each part of this equation has a name, and it's worth becoming familiar with them. The thing we want to compute ($p(h|d)$) is called the **posterior probability** -- it tell us what we should believe about the participant's ability given the data we observed. We then break that down into two terms in the numerator.[^2]

[^2]: We're making the posterior `r colorize("purple","purple")` to indicate the combination of likelihood (`r colorize("red","red")`) and prior (`r colorize("blue","blue")`).

The first part of the numerator is $p(d|h)$, the probability of the data we observed given our hypothesis about the participant's ability. This part is called the **likelihood**.[^3] This term tells us about the relationship between our hypothesis and the data we observed -- so if we think the participant has high ability (say $h = .9$) then the probability of a bunch of low accuracy observations will be fairly low.

[^3]: Speaking informally, "likelihood" is just a synonym for probability, but this is a technical meaning for the term, which can get a bit confusing.

The second term in the numerator, $p(h)$, is called the **prior**. This term encodes our beliefs about how likely our participant is to have different levels of ability. Intuitively, if we think that they are very unlikely to have high tea discrimination ability, we should require more evidence to convince us of a particular level of discrimination. In contrast, if we think they are likely to have this ability, we should be easier to convince.

```{r inference-bayes-demo, fig.cap="Examples of Bayesian inference about tea discrimination ability under three different priors (facets). Blue lines give the prior probability distribution, red lines give the likelihood of the data, and purple lines give the posterior distribution from combining likelihood and prior."}
bayes <- expand_grid(x = seq(0,1,.01), a = c(1, 3, 6)) %>%
  mutate(b = case_when(a == 1 ~ 1, 
                       a == 3 ~ 3, 
                       a == 6 ~ 2)) %>%
  rowwise %>%
  mutate(h = dbeta(x = x, shape1 = a, shape2 = b),
         d_given_h = dbeta(x = x, shape1 = 9, shape2 = 3),
         h_given_d = dbeta(x = x, shape1 = a + 9, shape2 = b + 3)) %>%
  group_by(a,b) %>%
  mutate(h_norm = h / sum(h), 
         h_given_d_norm = h_given_d / sum(h_given_d), 
         d_given_h_norm = d_given_h / sum(d_given_h), 
         map = x[h_given_d == max(h_given_d)],
         label = case_when(a == 1 ~ "Flat",
                           a == 3 ~ "Prior: no discrimination",
                           a == 6 ~ "Prior: discrimination"), 
         label = fct_relevel(label, "Flat", "prior: no discrimination"))

ggplot(bayes, aes(x = x, y = h_norm)) + 
  geom_line(col = "blue") +
  geom_line(aes(y = h_given_d_norm), col = "purple") +
  geom_line(aes(y = d_given_h_norm), col = "red") +
  geom_vline(xintercept = .5, lty = 2) + 
  geom_vline(aes(xintercept = map), col = "purple", lty = 3) + 
  facet_wrap(~label) + 
  ylab("Normalized probability") +
  xlab("Discrimination level")
```

Figure \@ref(fig:inference-bayes-demo) gives an example of the combination of prior and data.[^4] For the sake of this example, we assume that we have run 12 tea discrimination trials and observed 9 successes and 3 failures. The evidence alone -- with no prior -- suggests a discrimination estimate of $9/12 = .75$.[^5] When we use a flat prior, we get the same estimate of `r bayes$map[bayes$label == "Flat"][1]`.[^6] In contrast, if we go in assuming that discrimination is likely to be absent or weak, we are biased downward in our eventual estimate of `r bayes$map[bayes$label == "Prior: no discrimination"][1]`; if we go in assuming good discrimination, we end up biased upwards to `r bayes$map[bayes$label == "Prior: discrimination"][1]`.

[^4]: The model we use for this example is called a **Beta-Binomial conjugate model** and is a very convenient model for working with count data representing successes and failures.

[^5]: Technically this is known as the **maximum a posteriori** estimate, or the MAP. We won't use that term though. (MM: I'd rephrase. This sounds like "MAP" = "MLE".)

[^6]: MM: I think it may be illuminating to say more explicitly that Bayes estimates and frequentist estimates will exactly coincide either under a flat prior or as $n \to \infty$.

Fisher's original framework for significance testing focused only on the **null hypothesis** of no discrimination. In contrast, the Bayesian estimation method here focuses on the magnitude of accuracy.^[If you're reading carefully, you might have noticed that we *could* have discovered that the estimate of accuracy was very similar to chance -- more about this below.][^MM: I entirely agree with the spirit of this, but I worry about framing this as "Fisherian NHST vs. Bayesian estimation" when the issue really is "Fisherian NHST" vs "any form of estimation with continuous inference". We wouldn't want to inadvertently reinforce the misconception that Bayesian methods *inherently* alleviate the central issues with NHST, even though Bayesian methods of course have numerous important merits.] The intuition we'd like you to get is that, if you are an experimentalist who cares about the magnitude of causal relationships (and we hope you are), then Fisher's statistical approach isn't ideally suited to your goals.[\^MM: Yes, I like this framing in the final sentence much better.]

<!-- TODO: HERE WOULD BE A GREAT PLACE FOR AN INTERACTIVE -->

## Measures of effect size

With all our talk about estimation above, we didn't say much about what precisely was being estimated. Often, researchers seek out some sort of common standardized way for describing the relationships they observe in the study.

For example, imagine that Mika and Nicholas are interested in examining whether a dog-petting intervention can reduce depression relative to a placebo. They use the same self-report depression measure, but Mika decides to make it a 10-point self-report scale (0 = "I feel blissful" to 10 = "I feel extremely depressed"), whereas Nicholas decides to make it a 100-point scale (1 = "I feel blissful" to 100 = "I feel extremely depressed"). Observing that the intervention led to a 1-point decrease in Mika's scale is quite impressive. But observing a 1-point decrease in Nicholas' scale? Not so much!

To ensure that we have a common currency, many researchers describe their observations using standardized metrics. A common example of a such a metric is Cohen's *d*, which provides a standardized estimate of the difference between two means. There are many different ways to calculate Cohen's *d* (Lakens, 2013), but there usually some variant of the following formula:

$$d = \frac{M_1- M_2}{SD_1}$$

In the above example, $M_1$ could be the depression scores of patients who were assigned to pet dogs, whereas $M_2$ would be the scores of patients who were assigned to a placebo condition. $SD_1$ refers refers to standard deviation of the participants who were randomly assigned to pet dogs. Note: researchers usually seek to pool the standard deviations of *both* groups, but (for simplicity) we will assume that both groups have the same standard deviation (and thus $SD_1 = SD_2 = SD_{pooled}$).

Because Mika and Nicholas used different scales, they will nature observe different amount of variability. For example, if all participants indicate that they are between 2-8 on a 10-point scale, this should translate to a range of 20-80 on a 100-point scale. Similarly, a standard deviation of 2 on Mika's scale should correspond to a standard deviation of 20 on Nicholas' scale.

So let's compare their results, assuming that participants in the placebo groups have a mean depression score at the center of the scale (5 out of 10; 50 out of 100) and that both Mika and Nicholas observe a 1-point decrease is depression in the dog-petting group.

$${d_{Mika}} = \frac{M_1- M_2}{SD_1} = \frac{5- 4}{2} = \frac{1}{2} = 0.5$$

$${d_{Nicholas}} = \frac{M_1- M_2}{SD_1} = \frac{50- 49}{20} = \frac{1}{20} =  .05$$

When using Cohen's d, a value of .50 is often considered a strong effect, whereas a value of .05 would often be considered negligible (at least in the context of an intervention designed to improve depression).

Of course, there are many different standardized effect sizes that researchers can use. Although we described a common standardized effect size to describe differences in means, there are also standardized effect sizes for describing the amount of variance explained (e.g., Pearson's *r*, R^2^, and $\eta^2$) or relationships involving categorical variables (e.g., Odds Ratio). For a review, see... (Any recommendations?)

### Pros and cons of standardization

Pro: comparability across studies (e.g., school interventions to improve achievement?). Useful for meta-analysis (e.g., MA of facial feedback, which used a lot of different measures of emotion; infant consonant discrimination (cross method comparison)). Useful for a lot of power analysis software and packages (e.g., GPower and many packages in R).

Con: loss of information about measures and real-world predictions; dependence on baseline variability. Power analysis can only be done via annoying simulations. Not related to any real units. Often not very intuitive (have fun trying to explain what Cohen's *d* is to a very curious non-scientist. Fun fact: the Wikipedia article on effect sizes has been flagged as "too technical for most readers to understand" since 2014!)

[STILL VERBATIM FROM MRM]
Standardized effect-size measures have limitations. For example, if two interventions produce the same absolute change in the same outcome measure, but are studied in different populations in which the variability on the outcome differs substantially, the interventions would produce different standardized mean differences.\citep{greenland1986fallacy,cummings2011arguments} \black{Some meta-analysts argue against the use of $SMD$s\citep{greenland1986fallacy,cummings2011arguments} or feel that the scale should never be used in any context (a point raised, for example, during the peer review of this paper). Whether and how $SMD$s should be used is a current debate in the field of meta-analysis. Our views are as follows.} When meta-analyzing studies that measure the outcome on the same scale (e.g., blood pressure in terms of mmHg), it may often be preferable to use raw mean differences.\citep{cummings2011arguments} However, in many scientific fields, studies do not measure outcomes on exactly the same scale, as in both applied examples provided here; in such cases, using standardized mean differences may enable some approximate comparison and synthesis of effect sizes across studies. Additionally, when outcomes use arbitrary or unitless raw measures (e.g., points on a Likert scale), expressing effect sizes using standardized mean differences may provide some sense of effect sizes relative to variability in that sample, similar to measures of genetic heritability. \black{For some outcomes, such as income or grades in high school, absolute changes may in fact be less substantively meaningful than effects relative to variability in the population, for example expressed by $SMD$s with appropriately chosen denominators.}


<!-- ::: {.interactive} -->

<!-- ⌨️ Interactive box: non-parametric simulations where you can shuffle data across groups a bunch of times and see what kind of distribution it produces by chance -->

<!-- ::: -->
