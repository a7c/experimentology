<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 2 Replication, reproducibility, and transparency | Experimentology" />
<meta property="og:type" content="book" />





<meta name="author" content="Michael C. Frank, Mika Braginsky, Julie Cachia, Nicholas Coles, Tom Hardwicke, Robert Hawkins, Maya Mathur, and Rondeline Williams" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Chapter 2 Replication, reproducibility, and transparency | Experimentology">

<title>Chapter 2 Replication, reproducibility, and transparency | Experimentology</title>

<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />




<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="toc/toc.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="part"><span><b>I Preliminaries</b></span></li>
<li><a href="1-intro.html#intro"><span class="toc-section-number">1</span> Introduction: theory and experiments</a></li>
<li><a href="2-replication.html#replication"><span class="toc-section-number">2</span> Replication, reproducibility, and transparency</a></li>
<li><a href="3-ethics.html#ethics"><span class="toc-section-number">3</span> Ethics</a></li>
<li class="part"><span><b>II Statistics</b></span></li>
<li><a href="4-estimation.html#estimation"><span class="toc-section-number">4</span> Estimation</a></li>
<li><a href="5-inference.html#inference"><span class="toc-section-number">5</span> Inference</a></li>
<li><a href="6-models.html#models"><span class="toc-section-number">6</span> Models</a></li>
<li class="part"><span><b>III Design and Planning</b></span></li>
<li><a href="7-measurement.html#measurement"><span class="toc-section-number">7</span> Measurement</a></li>
<li><a href="8-design.html#design"><span class="toc-section-number">8</span> Design of experiments</a></li>
<li><a href="9-sampling.html#sampling"><span class="toc-section-number">9</span> Sampling</a></li>
<li><a href="10-preregistration.html#preregistration"><span class="toc-section-number">10</span> Preregistration</a></li>
<li class="part"><span><b>IV Execution</b></span></li>
<li><a href="11-selection.html#selection"><span class="toc-section-number">11</span> Replicating or extending an existing study</a></li>
<li><a href="12-collection.html#collection"><span class="toc-section-number">12</span> Data collection</a></li>
<li><a href="13-management.html#management"><span class="toc-section-number">13</span> Data management and sharing</a></li>
<li class="part"><span><b>V Analysis and Reporting</b></span></li>
<li><a href="14-viz.html#viz"><span class="toc-section-number">14</span> Visualization</a></li>
<li><a href="15-eda.html#eda"><span class="toc-section-number">15</span> Exploratory data analysis</a></li>
<li><a href="16-writing.html#writing"><span class="toc-section-number">16</span> Reproducible writing</a></li>
<li><a href="17-meta.html#meta"><span class="toc-section-number">17</span> Meta-analysis</a></li>
<li><a href="18-conclusions.html#conclusions"><span class="toc-section-number">18</span> Conclusions</a></li>
<li class="part"><span><b>VI Appendices</b></span></li>
<li><a href="19-github.html#github"><span class="toc-section-number">19</span> Github Tutorial</a></li>
<li><a href="20-rmarkdown.html#rmarkdown"><span class="toc-section-number">20</span> R Markdown Tutorial</a></li>
<li><a href="21-tidyverse.html#tidyverse"><span class="toc-section-number">21</span> Tidyverse Tutorial</a></li>
<li><a href="22-ggplot.html#ggplot"><span class="toc-section-number">22</span> ggplot Tutorial</a></li>
<li><a href="23-instructors.html#instructors"><span class="toc-section-number">23</span> Instructor‚Äôs Guide</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="replication" class="section level1" number="2">
<h1><span class="header-section-number">Chapter 2</span> Replication, reproducibility, and transparency</h1>
<div class="learning-goals">
<p>üçé Learning goals:</p>
<ul>
<li>Understand the ‚Äúcrisis‚Äù narrative in psychology and the empirical evidence supporting it</li>
<li>Define the distinction between reproducibility and replicability</li>
<li>Consider types of replication</li>
<li>Reason about the relation of replication to theory building</li>
</ul>
</div>
<p>In the previous chapter, we gave a sober and considered introduction to the topic of experiments, their connection with causal inference, and their role in building psychological theory. In this chapter we‚Äôre going to change gears a little bit and tell the story of the period from 2011 ‚Äì 2021 and how it has given rise to a number of ‚Äúcrisis‚Äù narratives in psychology.</p>
<p>In order to set the terms of discussion, we need to describe some of the outcomes we are interested in. Figure <a href="2-replication.html#fig:replication-terms">2.1</a> gives us a basic starting point for our definitions.<label for="tufte-sn-1" class="margin-toggle sidenote-number">1</label><input type="checkbox" id="tufte-sn-1" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">1</span> These terms have been a bit of a problem for the field, at least at first, but it seems like people have been agreeing on them recently.</span> For some claim in a paper, if we can take the same data that were analyzed in that paper, do the same analysis, and get the same result, we call that result <strong>reproducible</strong> (sometimes, <strong>analytically</strong> or <strong>computationally reproducible</strong>). If we can collect new data in the same experiment, do the same analysis, and get the same result, we call that a <strong>replication</strong> and say that the experiment is <strong>replicable</strong>. If we can do a different analysis with the original dataset, we call this a <strong>robustness check</strong> and so if a claim passes it is <strong>robust</strong>.<label for="tufte-sn-2" class="margin-toggle sidenote-number">2</label><input type="checkbox" id="tufte-sn-2" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">2</span> These are less common in experimental psychology, but are very common in fields that work with large, complex observational datasets like sociology.</span> And if the same finding obtains in a different population, perhaps with a different analysis, the result is likely to be more <strong>generalizable</strong> beyond the initial conditions in which it was observed.<label for="tufte-sn-3" class="margin-toggle sidenote-number">3</label><input type="checkbox" id="tufte-sn-3" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">3</span> You might have observed that a lot of work is being done here by the word ‚Äúsame.‚Äù How do we operationalize same-ness for experimental procedures, statistical analyses, or samples? These are difficult questions that we‚Äôll address in part below, but there‚Äôs no single answer and so these terms are always going to helpful guides rather than exact labels.</span></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:replication-terms"></span>
<img src="images/replication/terms.png" alt="A terminological framework for meta-science discussions. Based on [https://figshare.com/articles/Publishing_a_reproducible_paper/5440621]()." width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 2.1: A terminological framework for meta-science discussions. Based on <a href="">https://figshare.com/articles/Publishing_a_reproducible_paper/5440621</a>.<!--</p>-->
<!--</div>--></span>
</p>
<p>We‚Äôre also going to abandon the sober tone of the introduction and try to get you a bit worked up. From an empirical perspective, things have been far from ideal in the psychology literature. Many classic findings may be wrong, or at least overstated. Their statistical tests are probably not trustworthy. The actual numbers are even wrong in many papers! And even when experimental findings are ‚Äúreal‚Äù they may not reflect deep psychological generalizations. And even if they do, they likely don‚Äôt reflect generalizations that are true about people in general, only some very specific groups of people. Your hair should be on fire, at least a little bit. If you by the end of this chapter, you don‚Äôt feel a little bit of despair about the published psychological literature, then we haven‚Äôt done our job.</p>
<p>On the other hand, you might be thinking, how do you know that all this bad stuff is true? Claims about a literature or field as a whole go beyond the kind of standard paradigmatic science that we were talking about in the previous chapter ‚Äì instead they are part of a new field called <strong>meta-science</strong>. Meta-science research is research <em>about research</em>, for example investigating the rate of replication in a literature, or trying to figure out how widespread some negative practice is within a subfield. Meta-science allows us to go beyond one-off anecdotes about particular results or rumors about bad practices. Critically, we can also evaluate the strength of claims about the field using the same tools we use to evaluate standard research ‚Äì that is, we can critique research designs and inferences systematically, rather than just accepting (or failing to accept) assertions.</p>
<p>In this chapter, we‚Äôll discuss reproducibility and replicability primarily ‚Äì discussions of robustness and generalizability will be taken up in Chapters <a href="6-models.html#models">6</a> and <a href="9-sampling.html#sampling">9</a> respectively. We‚Äôll start out by reviewing some of the key concepts around reproducibility and replicability as well as the key meta-science findings. We‚Äôll then discuss some of the proposed sources of problems in replicability, focusing especially on how these might guide our study design and planning. We end by taking up the issue of how reproducibility and replicability relate to theory building in psychology.</p>
<div class="case-study">
<p>üî¨ Case study: The Open Science Collaboration</p>
<p>Around 2011, we were teaching our Experimental Methods course for the first time, based on a course model that we had worked on with Rebeccca Saxe <span class="citation">(<label for="tufte-mn-1" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-1" class="margin-toggle">Frank and Saxe 2012<span class="marginnote">Frank, Michael C, and Rebecca Saxe. 2012. <span>‚ÄúTeaching Replication.‚Äù</span> <em>Perspectives on Psychological Science</em> 7: 595‚Äì99.</span>)</span>. The idea was to have a replication-based course that introduced students to the nuts and bolts of research.<label for="tufte-sn-4" class="margin-toggle sidenote-number">4</label><input type="checkbox" id="tufte-sn-4" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">4</span> One of the current author team was a student in the course that year!</span> A guy named Brian Nosek was on sabbatical nearby, and over coffee we learned that he was starting up an ambitious project to replicate a large sample of studies from top psychology journals in 2008.</p>
<p>Students in Experimental Methods that year chose replication projects from the sample that Nosek had told us about. Four of these projects were done at a high standard and were nominated by the course TAs for inclusion in the broader project on the basis of strong implementations of the paradigm and good sample sizes. A few years later, when the final group of 100 replication studies was completed, we got a look at the results, shown in Figure <a href="2-replication.html#fig:replication-osc-2015">2.2</a>.</p>
<div class="figure"><span id="fig:replication-osc-2015"></span>
<p class="caption marginnote shownote">
Figure 2.2: Results from the Open Science Collaboration (2015). Each point represents one of the studies in the sample, with the horizontal position giving the original effect size and the vertical position giving the replication effect size. Dot size shows estimated statistical power. The dotted line represents a perfect replication.
</p>
<img src="images/replication/osc-2015.png" alt="Results from the Open Science Collaboration (2015). Each point represents one of the studies in the sample, with the horizontal position giving the original effect size and the vertical position giving the replication effect size. Dot size shows estimated statistical power. The dotted line represents a perfect replication." width="\linewidth"  />
</div>
<p>The resulting meta-science paper, which we and others refer to as the ‚Äúreplication project in psychology‚Äù (RPP) made a substantial impression on both psychologists and the broader community, defining both a field of psychology meta-science studies and providing a template for many-author collaborative projects <span class="citation">(<label for="tufte-mn-2" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-2" class="margin-toggle">Collaboration 2015<span class="marginnote">Collaboration, Open Science. 2015. <span>‚ÄúEstimating the Reproducibility of Psychological Science.‚Äù</span> <em>Science</em> 349 (6251).</span>)</span>. But the most striking thing was the result: disappointingly, by several different criteria, only around a third of studies showed the same finding as the original. The others yielded smaller effects that were no significant in the replication sample. This study provided the first large-scale evidence that methodological issues in the psychological literature might lead to a bad outcome in the literature ‚Äì namely a literature with many findings that didn‚Äôt replicate.</p>
<p>RPP‚Äôs results ‚Äì and their interpretation ‚Äì were controversial, however, and much ink was spilled on what these data showed. In particular, critics pointed to different degrees of fidelity between the original studies and the replications; insufficient levels of statistical power in the replications; non-representative sampling of the literature; and difficulties identifying specific statistical outcomes for replication success <span class="citation">(<label for="tufte-mn-3" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-3" class="margin-toggle">Gilbert et al. 2016<span class="marginnote">Gilbert, Daniel T, Gary King, Stephen Pettigrew, and Timothy D Wilson. 2016. <span>‚ÄúComment on <span>‚ÄòEstimating the Reproducibility of Psychological Science‚Äô</span>.‚Äù</span> <em>Science</em> 351 (6277): 1037‚Äì37.</span>; <label for="tufte-mn-4" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-4" class="margin-toggle">Anderson et al. 2016<span class="marginnote">Anderson, CJ, S Bahnik, M Barnett-Cowan, FA Bosco, J Chandler, CR Chartier, and otherss. 2016. <span>‚ÄúResponse to Comment on <span>‚ÄòEstimating the Reproducibility of Psychological Science‚Äô</span>.‚Äù</span> <em>Science</em> 351 (6277): 1037‚Äì37.</span>)</span>. In our view, many of these critiques have merit, and you can‚Äôt simply interpret the results of this stufy as an unbiased estimate of the replicability of results in the literature, contra the title.<label for="tufte-sn-5" class="margin-toggle sidenote-number">5</label><input type="checkbox" id="tufte-sn-5" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">5</span> Confusingly, the title of the paper is ‚ÄúEstimating the reproducibility of psychological science,‚Äù not ‚Äúthe replicability of psychological science.‚Äù This caused terminological confusion for several years; it seems like at this point people have decided it‚Äôs just a mistake.</span></p>
<p>And yet, RPP‚Äôs results are still important and compelling, and they undeniably changed the direction of the field of psychology. Many good studies are like this ‚Äì they have critical flaws but they lead the way towards other studies that pursue this new direction, often with greater technical precision and fewer issues. For several of us personally, working on this project was also transformative in that it showed us the power of collaborative work ‚Äì together we could do a study that no one of us had any hope of completing on our own, and potentially make a difference in our field.</p>
</div>
<div id="reproducibility" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Reproducibility</h2>
<p>As one of their primary purposes, scientific papers report measurements, statistical results, and more complex analytic findings and visualizations. For these results to be subject to scrutiny, readers and reviewers need to be able to access some aspects of the set of steps from the original raw measures all the way to the final products. For much of the history of the scientific paper, complete verification of the <strong>provenance</strong> of a particular reported number in a paper was impossible ‚Äì at best, a reader was presented with a verbal or mathematical description of the computations that were performed on the raw data, and the raw data themselves were not available.<label for="tufte-sn-6" class="margin-toggle sidenote-number">6</label><input type="checkbox" id="tufte-sn-6" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">6</span> In practice, for many years data have been available ‚Äúon request,‚Äù and professional societies like the American Psychological Association have <a href="https://www.apa.org/ethics/code">mandated data sharing</a> for purposes of verification. But in practice data are rarely made available <span class="citation">(<label for="tufte-mn-5" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-5" class="margin-toggle">Wicherts et al. 2006<span class="marginnote">Wicherts, Jelte M, Denny Borsboom, Judith Kats, and Dylan Molenaar. 2006. <span>‚ÄúThe Poor Availability of Psychological Research Data for Reanalysis.‚Äù</span> <em>American Psychologist</em> 61 (7): 726.</span>)</span>. We believe this is untenable, and we provide a longer argument justifying data sharing in Chapter <a href="3-ethics.html#ethics">3</a> and discuss some of the practicalities of sharing in Chapter <a href="13-management.html#management">13</a>.</span></p>
<p>Data sharing (and the sharing of analytic code) is increasing, and we believe this is a very good thing for science as a whole.<label for="tufte-sn-7" class="margin-toggle sidenote-number">7</label><input type="checkbox" id="tufte-sn-7" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">7</span> Psychology still likely lags behind other fields, however <span class="citation">(<label for="tufte-mn-6" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-6" class="margin-toggle">Tedersoo et al. 2021<span class="marginnote">Tedersoo, Leho, Rainer K√ºngas, Ester Oras, Kajar K√∂ster, Helen Eenmaa, √Ñli Leijen, Margus Pedaste, et al. 2021. <span>‚ÄúData Sharing Practices and Data Availability Upon Request Differ Across Scientific Disciplines.‚Äù</span> <em>Scientific Data</em> 8 (1): 1‚Äì11.</span>)</span>.</span> But because sharing has been relatively limited in the past, the reproducibility of numbers in nearly all published papers cannot be checked, leaving open a number of negative possibilities. Most prominently, errors in calculation or reporting could lead to disparities between the reported result and the actual result. Mis-specification of analytic computations could mislead readers about the computations that were actually performed. And the robustness of analyses cannot be checked.<label for="tufte-sn-8" class="margin-toggle sidenote-number">8</label><input type="checkbox" id="tufte-sn-8" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">8</span> We‚Äôre focusing on data sharing here, because much experimental reseach uses relatively straightforward analyses. But the same points apply to code sharing as well! In computational research, the relevant position is nicely summed up by a prescient quote from <span class="citation"><label for="tufte-mn-7" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-7" class="margin-toggle">Buckheit and Donoho (1995)<span class="marginnote">Buckheit, Jonathan B, and David L Donoho. 1995. <span>‚ÄúWavelab and Reproducible Research.‚Äù</span> In <em>Wavelets and Statistics</em>, 55‚Äì81. Springer.</span></span>: ‚ÄúAn article about computational science in a scientific publication is not the scholarship itself, it is merely advertising of the scholarship. The actual scholarship is the complete software development environment and the complete set of instructions which generated the figures.‚Äù</span></p>
<p>Are errors common? There are plenty of individual instances of errors that are corrected in the published literature <span class="citation">(e.g., <label for="tufte-mn-8" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-8" class="margin-toggle">Cesana-Arlotti et al. 2018<span class="marginnote">Cesana-Arlotti, N, A Martƒ±ÃÅn, E T√©gl√°s, L Vorobyova, R Cetnarski, and L L Bonatti. 2018. <span>‚ÄúErratum for the Report <span>‚ÄòPrecursors of Logical Reasoning in Preverbal Human Infants‚Äô</span>.‚Äù</span> <em>Science</em> 361 (6408).</span>)</span>, and we ourselves have made significant analytic errors <span class="citation">(<label for="tufte-mn-9" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-9" class="margin-toggle">Frank et al. 2013<span class="marginnote">Frank, Michael C, Jonathan A Slemmer, Gary F Marcus, and Scott P Johnson. 2013. <span>‚Äú" Information from Multiple Modalities Helps 5-Month-Olds Learn Abstract Rules": Erratum.‚Äù</span></span>)</span>. But these kinds of experiences don‚Äôt tell us about the frequency of error (or the consequences of error for the conclusions that researchers draw).<label for="tufte-sn-9" class="margin-toggle sidenote-number">9</label><input type="checkbox" id="tufte-sn-9" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">9</span> There is a very interesting discussion of the pernicious role of scientific error on theory building in <span class="citation"><label for="tufte-mn-10" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-10" class="margin-toggle">Gould, Gold, and others (1996)<span class="marginnote">Gould, Stephen Jay, Steven James Gold, and others. 1996. <em>The Mismeasure of Man</em>. WW Norton &amp; company.</span></span>‚Äôs ‚ÄúThe Mismeasure of Man.‚Äù Gould examines research on racial differences in intelligence and documents how scientific errors that supported racial differences were often overlooked. Errors are often caught asymmetrically; we are more motivated to double-check a result that contradicts our biases.</span> This question about frequency is a meta-scientific question that a variety of researchers have attempted to answer over the years. If errors are frequent, that would suggest a need for changes in our policies and practices to reduce their frequency!</p>
<p>Unfortunately, the lack of data availability creates a problem: it‚Äôs hard to figure out if calculations are wrong if you can‚Äôt reproduce them in the first place. One meta-scientific research program has taken a clever approach to this issue. In standard American Psychological Association (APA) reporting format, inferential statistics must be reported with three pieces of information: the test statistic, the degrees of freedom for the test, and the <span class="math inline">\(p\)</span>-value (e.g., <span class="math inline">\(t(18) = -0.74\)</span>, <span class="math inline">\(p = 0.47\)</span>). Yet these pieces of information are redundant with one another. Thus, reported statistics can be checked for consistency simply by evaluating whether they line up with one another ‚Äì that is, whether the <span class="math inline">\(p\)</span>-value recomputed from the <span class="math inline">\(t\)</span> and degrees of freedom matches the reported value.</p>
<p><span class="citation"><label for="tufte-mn-11" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-11" class="margin-toggle">Bakker and Wicherts (2011)<span class="marginnote">Bakker, Marjan, and Jelte M Wicherts. 2011. <span>‚ÄúThe (Mis) Reporting of Statistical Results in Psychology Journals.‚Äù</span> <em>Behavior Research Methods</em> 43 (3): 666‚Äì78.</span></span> performed precisely this analysis on a sample of 281 papers, and found that around 18% of statistical results were incorrectly reported. Even more worrisome, around 15% of articles contained at least decision error ‚Äì that is, a case where the error changed the direction of the inference that was made (e.g., from significant to insignificant).<label for="tufte-sn-10" class="margin-toggle sidenote-number">10</label><input type="checkbox" id="tufte-sn-10" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">10</span> Confirming Gould‚Äôs speculation, most of the reporting errors that led to decision errors were in line with the researchers‚Äô own hypotheses.</span> <span class="citation"><label for="tufte-mn-12" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-12" class="margin-toggle">Nuijten et al. (2016)<span class="marginnote">Nuijten, Mich√®le B, Chris H J Hartgerink, Marcel A L M van Assen, Sacha Epskamp, and Jelte M Wicherts. 2016. <span>‚ÄúThe Prevalence of Statistical Reporting Errors in Psychology (1985‚Äì2013).‚Äù</span> <em>Behav. Res. Methods</em> 48 (4): 1205‚Äì26.</span></span> used an automated method called ‚Äústatcheck‚Äù<label for="tufte-sn-11" class="margin-toggle sidenote-number">11</label><input type="checkbox" id="tufte-sn-11" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">11</span> Statcheck is now available as a <a href="http://statcheck.io">web app</a> and an R package so that you can check your own manuscripts!</span> to confirm and extend this analysis. They checked <span class="math inline">\(p\)</span>-values for more than 250,000 psychology papers in the period 1985‚Äì2013 and found that around half of all papers contained at least one incorrect <span class="math inline">\(p\)</span>-value!</p>
<div class="figure"><span id="fig:replication-hardwicke"></span>
<p class="caption marginnote shownote">
Figure 2.3: Analytic reproducibility of results from open-data articles in <em>Cognition</em> and <em>Psychological Science</em>. From Hardwicke et al.¬†(2021).
</p>
<img src="images/replication/hardwicke2021.png" alt="Analytic reproducibility of results from open-data articles in *Cognition* and *Psychological Science*. From Hardwicke et al. (2021)." width="\linewidth"  />
</div>
<p>There may be many more errors than are detected via this method: incorrect computation can be accompanied by correct reporting. While there is probably no general way to check reproducibility across the literature, a group of us conducted some more targeted studies of two journals with open-data policies. <span class="citation"><label for="tufte-mn-13" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-13" class="margin-toggle">Hardwicke et al. (2018)<span class="marginnote">Hardwicke, Tom E, Maya B Mathur, Kyle Earl MacDonald, Gustav Nilsonne, George Christopher Banks, Mallory Kidwell, Alicia Hofelich Mohr, et al. 2018. <span>‚ÄúData Availability, Reusability, and Analytic Reproducibility: Evaluating the Impact of a Mandatory Open Data Policy at the Journal Cognition.‚Äù</span></span></span> and <span class="citation"><label for="tufte-mn-14" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-14" class="margin-toggle">Hardwicke et al. (2021)<span class="marginnote">Hardwicke, Tom E, Manuel Bohn, Kyle MacDonald, Emily Hembacher, Mich√®le B Nuijten, Benjamin N Peloquin, Benjamin E deMayo, Bria Long, Erica J Yoon, and Michael C Frank. 2021. <span>‚ÄúAnalytic Reproducibility in Articles Receiving Open Data Badges at the Journal Psychological Science : An Observational Study.‚Äù</span> <em>Royal Society Open Science</em>.</span></span> identified datasets with reusable data (not all datasets were complete and comprehensible) and then downloaded the data and attempted to reproduce the main statistical results from 60 of these articles. This process was incredibly labor-intensive, with articles requiring 5‚Äì10 hours of work each. Only about a third of articles were completely reproducible without help from the original authors; around 62% were successfully reproduced after ‚Äì sometimes extensive ‚Äì correspondence (Figure <a href="2-replication.html#fig:replication-hardwicke">2.3</a>). A good number of the remaining papers appeared to have some irreproducible results due to errors of one type or another (and some reproducible numbers were reproducible despite errors identified by the team). Although none of the errors we identified invalidated the conclusions of the original studies, several authors were still motivated to submit corrections to their articles!</p>
<p>This body of evidence suggests to us that reporting and computation errors are frequent in the published literature, and the identification of these errors depends on the findings being reproducible. If data are not available, then errors simply cannot be found. This body of evidence leads us to believe that transparency is a critical imperative for decreasing the frequency of errors in the published literature.<label for="tufte-sn-12" class="margin-toggle sidenote-number">12</label><input type="checkbox" id="tufte-sn-12" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">12</span> Does the meta-scientific literature provide strong support for transparency as a policy intervention leading to better reproducibility outcomes? To our knowledge, this experiment hasn‚Äôt been run yet, perhaps because it would be quite difficult to execute. On the other hand, we can‚Äôt help but mention that there is an <em>association</em> between data sharing and reporting errors, such that author teams who were less likely to provide data on request also had more reporting errors in their papers <span class="citation">(<label for="tufte-mn-15" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-15" class="margin-toggle">Wicherts, Bakker, and Molenaar 2011<span class="marginnote">Wicherts, Jelte M, Marjan Bakker, and Dylan Molenaar. 2011. <span>‚ÄúWillingness to Share Research Data Is Related to the Strength of the Evidence and the Quality of Reporting of Statistical Results.‚Äù</span> <em>PloS One</em> 6 (11): e26828.</span>)</span>.</span></p>
</div>
<div id="replication-1" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Replication</h2>
<p>As we discussed in Chapter <a href="1-intro.html#intro">1</a>, organized skepticism and independent verifiability of observations are key features of scientific inquiry. Reproducibility is thus a key desideratum for scientific work. But beyond verifying the analyses reported in a paper, we are often interested in understanding whether the measurements can be replicated. To quote from <span class="citation"><label for="tufte-mn-16" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-16" class="margin-toggle">Popper (2005)<span class="marginnote">Popper, Karl. 2005. <em>The Logic of Scientific Discovery</em>. Routledge.</span></span>, ‚Äúthe scientifically significant‚Ä¶ effect may be defined as that which can be regularly reproduced by anyone who carries out the appropriate experiment in the way prescribed.‚Äù</p>
<div id="conceptual-frameworks-for-replication" class="section level3" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Conceptual frameworks for replication</h3>
<p>The key challenge of replication is <strong>invariance</strong> ‚Äì Popper‚Äôs ‚Äúin the way prescribed.‚Äù That is, what are the features of the world over which a particular observation should be relatively constant, and what are those that are specified as the key ingredients for the effect? Replication is relatively straightforward in the physical and biological sciences, in part because of presupposed theoretical background that allows us to make strong inferences about invariance. If a biologist reports an observation about a particular cell type from an organism, the color of the microscope is presumed not to matter to the observation.</p>
<p>These invariances are far harder to state in psychology, for both the procedure of an experiment and its sample. Procedurally, how much should the color of the experimental stimulus matter to the measured effect? In some cases yes, in some cases no.<label for="tufte-sn-13" class="margin-toggle sidenote-number">13</label><input type="checkbox" id="tufte-sn-13" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">13</span> A fascinating study by <span class="citation"><label for="tufte-mn-17" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-17" class="margin-toggle">Baribault et al. (2018)<span class="marginnote">Baribault, Beth, Chris Donkin, Daniel R Little, Jennifer S Trueblood, Zita Oravecz, Don Van Ravenzwaaij, Corey N White, Paul De Boeck, and Joachim Vandekerckhove. 2018. <span>‚ÄúMetastudies for Robust Tests of Theory.‚Äù</span> <em>Proceedings of the National Academy of Sciences</em> 115 (11): 2607‚Äì12.</span></span> proposes a method for empirically understanding psychological invariances. Treating a subliminal priming effect as their model system, they sampled thousands of ‚Äúmicro-experiments‚Äù in which small parameters of their experimental procedure were randomly sampled. These parameters allowed for measurement of their effect of interest, averaging across this irrelevant variation. Turned out in their case, color did not in fact matter.</span> If color does not matter, how about the context of presentation for an experiment ‚Äì should presentation to a participant at home on a web browser produce the same effect as presentation in a laboratory setting using custom software <span class="citation">(<label for="tufte-mn-18" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-18" class="margin-toggle">Crump, McDonnell, and Gureckis 2013<span class="marginnote">Crump, Matthew J C, John V McDonnell, and Todd M Gureckis. 2013. <span>‚ÄúEvaluating Amazon‚Äôs Mechanical Turk as a Tool for Experimental Behavioral Research.‚Äù</span> <em>PLoS One</em> 8 (3): e57410.</span>)</span>? Without a theory of human responses to guide our decision-making, these invariances are quite hard to state. Yet problems of invariances in procedure pale in comparison to the problem of stating invariances across samples from human populations! In some sense, the research program of some branches of the social sciences amounts to an understanding of invariances across human cognition <span class="citation">(<label for="tufte-mn-19" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-19" class="margin-toggle">Chomsky 1967<span class="marginnote">Chomsky, Noam. 1967. <em>Aspects of the Theory of Syntax</em>. MIT Press.</span>)</span>.</p>
<p>A lot is at stake in this discussion. If Frog publishes a finding with US undergraduates and Toad then ‚Äúreplicates‚Äù the procedure in Germany, to what extent should we be perturbed if the effect is different in magnitude or absent?<label for="tufte-sn-14" class="margin-toggle sidenote-number">14</label><input type="checkbox" id="tufte-sn-14" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">14</span> Presumably not very much if Toad gave the original instructions in English instead of in German ‚Äì that‚Äôs another one of these pesky invariances that we are always worrying about!</span> People have amde a number of replication taxonomies to try and quantify the ‚Äúdistance‚Äù between two experiments. One influential one is the distinction between <strong>direct replications</strong> and <strong>conceptual replications</strong> <span class="citation">(<label for="tufte-mn-20" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-20" class="margin-toggle">Zwaan et al. 2018<span class="marginnote">Zwaan, Rolf Antonius, Alexander Etz, Richard E Lucas, and Brent Donnellan. 2018. <span>‚ÄúMaking Replication Mainstream.‚Äù</span></span>)</span>.</p>
<ul>
<li>We describe the framework for replication in Zwaan et al.¬†(2018), highlighting the idea of multiple dimensions on which a replication can differ from the original study and the ways that this complicates inferences about replication ‚Äúsuccess‚Äù (Mathur and VanderWeele 2019, 2020).</li>
<li>We can get different epistemic value from doing direct replications (attempt to copy original study with as much fidelity as possible) vs.¬†conceptual replications (attempt to somewhat perturb operationalization of original study).</li>
</ul>
<div class="accident-report">
<p>‚ö†Ô∏è Accident report: ‚ÄúSmall Telescopes‚Äù</p>
<p>We‚Äôve been discussing the question of invariance with respect to procedure and sample, but we haven‚Äôt really discussed invariance with respect to <em>result</em>. To what extent can we consider two measurements to be ‚Äúthe same?‚Äù Several obvious metrics, including those used by RPP, don‚Äôt generalize well <span class="citation">(<label for="tufte-mn-21" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-21" class="margin-toggle">Simonsohn 2015<span class="marginnote">Simonsohn, Uri. 2015. <span>‚ÄúSmall Telescopes: Detectability and the Evaluation of Replication Results.‚Äù</span> <em>Psychol. Sci.</em> 26 (5): 559‚Äì69.</span>)</span>. For example, if one finding is statistically significant and the other isn‚Äôt, they still could have effect sizes that are actually quite close to one another, in part because one might have a larger sample size than the other.<label for="tufte-sn-15" class="margin-toggle sidenote-number">15</label><input type="checkbox" id="tufte-sn-15" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">15</span> This is one reason we are not very fond of binary inferences about statistical inference, as you‚Äôll see in Chapter <a href="5-inference.html#inference">5</a>.</span> Or you could have two significant findings that nevertheless are pretty obviously different.</p>
<div class="figure"><span id="fig:replication-telescopes"></span>
<p class="caption marginnote shownote">
Figure 2.4: The original finding by Schwarz &amp; Clore (1983) and two replications with much larger samples. From Simonsohn (2015).
</p>
<img src="images/replication/telescopes.png" alt="The original finding by Schwarz &amp; Clore (1983) and two replications with much larger samples. From Simonsohn (2015)." width="\linewidth"  />
</div>
<p>In a classic study, <span class="citation"><label for="tufte-mn-22" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-22" class="margin-toggle">Schwarz and Clore (1983)<span class="marginnote">Schwarz, Norbert, and Gerald L Clore. 1983. <span>‚ÄúMood, Misattribution, and Judgments of Well-Being: Informative and Directive Functions of Affective States.‚Äù</span> <em>Journal of Personality and Social Psychology</em> 45 (3): 513.</span></span> reported that participants (N=28) rated their life satisfaction as higher on sunny days than rainy days, suggesting that they misattributed temporary happiness about the weather to longer-term life satisfaction. However, when several more recent studies examined very large samples of survey responses, they yielded estimates of the effect that were much smaller but significant in one case)and essentially zero in the other (Figure <a href="2-replication.html#fig:replication-telescopes">2.4</a>. Using statistical significance as the metric of replication success, you might be tempted to say that the first of these was a successful replication and the second was a failed replication.</p>
<p>Simonsohn points out that this doesn‚Äôt make sense, using the vivid analogy of the experimental sample as a telescope. Following this analogy, Schwarz and Clore had a very small telescope, and they pointed it in a particular direction and claimed to have observed a planet. Now it might turn out that there was a planet at that location when you look with a much larger telescope (first replication), and it might turn out that there wasn‚Äôt (second replication). Regardless, however, the original telescope was simply <em>too small</em> to have seen whatever was there!<label for="tufte-sn-16" class="margin-toggle sidenote-number">16</label><input type="checkbox" id="tufte-sn-16" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">16</span> Scott Alexander has a <a href="https://astralcodexten.substack.com/p/on-hreha-on-behavioral-economics">more graphic version</a> of this metaphor: ‚ÄúImagine I claimed our next-door neighbor was a billionaire oil sheik who kept thousands of boxes of gold and diamonds hidden in his basement. Later we meet the neighbor, and he is the manager of a small bookstore and has a salary 10% above the US average‚Ä¶ Should we describe this as ‚Äúwe have confirmed the Wealthy Neighbor Hypothesis, though the effect size was smaller than expected?‚Äù"</span></p>
</div>
</div>
<div id="the-meta-science-of-replication" class="section level3" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> The meta-science of replication</h3>
<p>The RPP we described above found a top-line replication rate of 33%. If we can‚Äôt take the RPP‚Äôs estimate seriously, how replicable <em>is</em> psychological research? Based on the discussion above, we hope we‚Äôve made you skeptical that this is a well-posed question without a lot of additional details. Any answer is going to have to provide details about the scope of this claim, the definition of replication being used, and the metric for replication success. On the other hand, versions of this question have led to a number of empirical studies that help us better understand the scope of replication issues. We‚Äôll review these briefly here because we think a good understanding of the meta-science literature on replication can help us decide how worried we should be about the state of the psychology literature.</p>
<p>One set of follow-up studies to the OSC2015 study has focused on evaluating replication success within subsamples. Many of these have focused on particular subfields or journals, with the goal of informing particular field-specific practices or questions. For example, <span class="citation"><label for="tufte-mn-23" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-23" class="margin-toggle">Camerer et al. (2016)<span class="marginnote">Camerer, Colin F, Anna Dreber, Eskil Forsell, Teck-Hua Ho, J√ºrgen Huber, Magnus Johannesson, Michael Kirchler, et al. 2016. <span>‚ÄúEvaluating Replicability of Laboratory Experiments in Economics.‚Äù</span> <em>Science</em> 351 (6280): 1433‚Äì36.</span></span> largely adopted the methodological choices of OSC2015, but applied the procedure to all of the between-subject laboratory articles published in two top economics journals in the period 2011‚Äì2014. They found a top-line replication rate of 61%, higher than in OSC2015 but lower than the naive expectation based on their level of statistical power. Another followup to OSC2015 replicated the full set of 21 behavioral experiments published in the journals <em>Science</em> and <em>Nature</em> from 2010‚Äì2015, finding a replication rate of 62%. This study was notable because they followed a two-step procedure ‚Äì after an initial round of replications, they followed up on the failures by consulting with the original authors and pursuing extremely large sample sizes. The resulting estimate thus is not subject to many of the critiques of the original OSC2015 paper. While these types of studies do not answer all the questions that were raised about RPP, they suggest that replication rates for top experiments are not as high as we‚Äôd like them to be, even when greater care is taken with the sampling and individual study protocols.</p>
<p>Another set of large-scale replication projects called the ‚ÄúManyLabs‚Äù projects abandons systematic sampling and instead asks about variability in replication success across a set of interesting and important findings. In the most exciting and ambitious of these studies, ManyLabs 2, <span class="citation"><label for="tufte-mn-24" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-24" class="margin-toggle">Klein et al. (2018)<span class="marginnote">Klein, Richard A, Michelangelo Vianello, Fred Hasselman, Byron G Adams, Reginald B Adams Jr, Sinan Alper, Mark Aveyard, et al. 2018. <span>‚ÄúMany Labs 2: Investigating Variation in Replicability Across Samples and Settings.‚Äù</span> <em>Advances in Methods and Practices in Psychological Science</em> 1 (4): 443‚Äì90.</span></span> replicated 28 findings, distributed across 125 different samples and more than 15,000 participants. The topline replication rate was 54%, but the main goal was to examine variability in these effects. Many people had worried that failures to replicate in RPP and other studies might have been due to differences between the original context of administration and the context of the replication, a hypothesis sometimes known as ‚Äúcontext sensitivity‚Äù <span class="citation">(<label for="tufte-mn-25" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-25" class="margin-toggle">Van Bavel et al. 2016<span class="marginnote">Van Bavel, Jay J, Peter Mende-Siedlecki, William J Brady, and Diego A Reinero. 2016. <span>‚ÄúContextual Sensitivity in Scientific Reproducibility.‚Äù</span> <em>Proceedings of the National Academy of Sciences</em> 113 (23): 6454‚Äì59.</span>)</span>. ManyLabs 3 found almost no support for this hypothesis. In general, when effects failed to replicate they did so in the lab and online, and these failures were consistent across many cultures.</p>
<p>Other scientists working in the same field can often predict when an experiment will fail to replicate. <span class="citation"><label for="tufte-mn-26" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-26" class="margin-toggle">Dreber et al. (2015)<span class="marginnote">Dreber, Anna, Thomas Pfeiffer, Johan Almenberg, Siri Isaksson, Brad Wilson, Yiling Chen, Brian A Nosek, and Magnus Johannesson. 2015. <span>‚ÄúUsing Prediction Markets to Estimate the Reproducibility of Scientific Research.‚Äù</span> <em>Proceedings of the National Academy of Sciences</em> 112 (50): 15343‚Äì47.</span></span> showed that prediction markets (where participants bet small sums of real money on replication outcomes) made fairly accurate estimates of replication success in the aggregate. This result has itself now been replicated several times (e.g., in the Camerer et al., 2018 study described earlier). Maybe even more surprisingly, there‚Äôs some evidence that machine learning models trained on the text of papers can predict replication success fairly accurately <span class="citation">(<label for="tufte-mn-27" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-27" class="margin-toggle">Yang, Youyou, and Uzzi 2020<span class="marginnote">Yang, Yang, Wu Youyou, and Brian Uzzi. 2020. <span>‚ÄúEstimating the Deep Replicability of Scientific Findings Using Human and Artificial Intelligence.‚Äù</span> <em>Proceedings of the National Academy of Sciences</em> 117 (20): 10762‚Äì68.</span>)</span>. All this points to the possibility of isolating consistent factors that lead to replication success or failure. In the next section we consider what these factors are in more depth.</p>
<p>The meta-science studies reviewed above are remarkably impressive, and provide some clarity on what we should expect from the literature. In particular, the odds of replicability for ageneric social and cognitive psychology finding is likely to be between one third and two thirds. Further, this literature has done a good job of ruling out explanations of replication failure like context sensitivity, expertise, and advice from peer reviewers <span class="citation">(<label for="tufte-mn-28" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-28" class="margin-toggle">Ebersole et al. 2020<span class="marginnote">Ebersole, Charles R, Maya B Mathur, Erica Baranski, Diane-Jo Bart-Plange, Nicholas R Buttrick, Christopher R Chartier, Katherine S Corker, et al. 2020. <span>‚ÄúMany Labs 5: Testing Pre-Data-Collection Peer Review as an Intervention to Increase Replicability.‚Äù</span> <em>Advances in Methods and Practices in Psychological Science</em> 3 (3): 309‚Äì31.</span>)</span>.</p>
<p>On the other hand, they have substantial limitations as well. With relatively few exceptions, they have focused on short, computerized tasks that mostly would fall into the categories of social and cognitive psychology. Further, and perhaps most troubling from the perspective of theory development, they tell us only whether a particular experimental effect can be replicated. They tell us almost nothing about whether the construct that the effect was meant to operationalize is in fact real! We‚Äôll return to the difficult issue of how replication and theory construction relate to one another in the final section of this chapter.</p>
<p>Some have called the narrative that emerges from the sum of these meta-science studies the ‚Äúreplication crisis.‚Äù For us, we think of it as a major tempering of expectations with respect to the published literature. Your naive expectation might reasonably be that you could read a typical journal article, select a paradigm from it, and build on that paradigm in our own research. The upshot of this literature is that your expectation would be wrong about half the time.</p>
<div class="ethics-box">
<p>üåø Ethics box: Consequences for the study, consequences for the person</p>
<p>‚ÄúPower posing‚Äù is the idea that changing your physical posture to make it more open and expansive might also change your confidence. <span class="citation"><label for="tufte-mn-29" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-29" class="margin-toggle">Carney, Cuddy, and Yap (2010)<span class="marginnote">Carney, Dana R, Amy JC Cuddy, and Andy J Yap. 2010. <span>‚ÄúPower Posing: Brief Nonverbal Displays Affect Neuroendocrine Levels and Risk Tolerance.‚Äù</span> <em>Psychological Science</em> 21 (10): 1363‚Äì68.</span></span> reported a striking study of this phenomenon, in which 42 participants were told they were taking part in a study of physiological recording. They then held two poses, each for a minute. In one condition, the poses were expansive (e.g., legs out, hands on head) or contractive (e.g., arms and legs crossed). Participants in the expansive pose condition showed increases in testosterone and decreases in salivary cortisol (a stress marker), they took a greater number of risk in a gambling task, and they reported that they were more ‚Äúin charge‚Äù in a survey. This result suggested that a two-minute induction could lead to striking physiological and psychological changes ‚Äì in turn leading to power posing becoming firmly enshrined as part of the set of recommended strategies in business and elsewhere. The original publication contributed to the rise of the researchers‚Äô careers, including becoming a principal piece of evidence in a hugely-popular TED talk by Amy Cuddy, one of the authors.</p>
<p>This result is likely not definitive, however. A replication study with a larger number of participants (N=200) failed to find evidence for physiological effects, even as it did find some effects on participants‚Äô own beliefs <span class="citation">(<label for="tufte-mn-30" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-30" class="margin-toggle">Ranehill et al. 2015<span class="marginnote">Ranehill, Eva, Anna Dreber, Magnus Johannesson, Susanne Leiberg, Sunhae Sul, and Roberto A Weber. 2015. <span>‚ÄúAssessing the Robustness of Power Posing: No Effect on Hormones and Risk Tolerance in a Large Sample of Men and Women.‚Äù</span> <em>Psychological Science</em> 26 (5): 653‚Äì56.</span>)</span>. And a review of the published literature suggested that many findings appeared to be the result of some sort of publication bias, as far too many of them had <span class="math inline">\(p\)</span>-values very close to the .05 threshold <span class="citation">(<label for="tufte-mn-31" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-31" class="margin-toggle">Simmons and Simonsohn 2017<span class="marginnote">Simmons, Joseph P, and Uri Simonsohn. 2017. <span>‚ÄúPower Posing: P-Curving the Evidence.‚Äù</span> <em>Psychological Science</em>.</span>)</span>. In light of this evidence, the first author made a public statement that she <a href="https://faculty.haas.berkeley.edu/dana_carney/pdf_my%20position%20on%20power%20poses.pdf">does not believe that ‚Äúpower pose‚Äù effects are real</a>.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:replication-powerpose"></span>
<img src="images/replication/powerpose.png" alt="Google trends time series for &quot;power pose&quot; from 2004-2021." width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 2.5: Google trends time series for ‚Äúpower pose‚Äù from 2004-2021.<!--</p>-->
<!--</div>--></span>
</p>
<p>From the scientific perspective, it‚Äôs very tempting to take this example as a case in which the scientific ecosystem corrects itself. Although many people continue to cite the original power posing work, we suspect the issues are well-known throughout the social psychology community, and overall interest has gone down (see Figure <a href="2-replication.html#fig:replication-powerpose">2.5</a>. But this narrative masks the very real human impacts of the self-correction process, which can raise ethical questions about the best way to address issues in the scientific record.</p>
<p>The process of debate and discussion around individual findings can be bruising and complicated. In the case of power posing, Cuddy herself was tighly associated with the findings and many critiques of the findings became critiques of the individual. Several commentators used Cuddy‚Äôs name as a synecdoche for low-quality psychological results, likely because of her prominence and perhaps because of her gender and age as well. These comments were likely harmful to Cuddy personally and her career more generally.<label for="tufte-sn-17" class="margin-toggle sidenote-number">17</label><input type="checkbox" id="tufte-sn-17" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">17</span> For further reading, see <a href="https://www.nytimes.com/2017/10/18/magazine/when-the-revolution-came-for-amy-cuddy.html">‚ÄúWhen the Revolution Came for Amy Cuddy‚Äù</a>.</span></p>
<p>Scientists should critique, reproduce, and replicate ‚Äì these are all critical parts of the progress of normal science. But it‚Äôs important to do this in a way that‚Äôs sensitive to the people involved. Here are a few guidelines for courteous and ethical conduct:</p>
<ul>
<li>Communicate personally before communicating publicly. As one critic said about the power posing debate, ‚Äú‚ÄúI wish I‚Äôd had the presence of mind to pick up the phone and call [before publishing my critique].‚Äù</li>
<li>Always communicate about the work, never the person. Try to use language that is specific to the analysis or design being critiqued, rather than the person who did the analysis or thought up the design.</li>
<li>Avoid using language that assumes negative intentions, e.g.¬†‚Äúthe authors misleadingly state that ‚Ä¶‚Äù</li>
<li>Ask for someone to read your paper, email, blogpost, or tweet before you hit send. It can be very difficult to figure out the tone of your writing without an external perspective.</li>
</ul>
<p>As we will argue in the next chapter, we have an ethical duty as scientists to promote good science and critique low quality science. But we also have duties to our colleauges and communities to be good to one another.</p>
</div>
</div>
</div>
<div id="causes-of-the-replication-crisis" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Causes of the ‚Äúreplication crisis‚Äù</h2>
<p>What are the</p>
<div class="accident-report">
<p>‚ö†Ô∏è Accident report: When I‚Äôm 64?</p>
<p>Simmons, Simonsohn, Nelson introduce p-hacking into our lexicon.</p>
</div>
</div>
<div id="replication-and-theory-building" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> Replication and theory building</h2>
<p>Crisis is a term in Kuhn for the period preceding a paradigm shift.</p>
<p>How do reproducibility and replicability contribute to theory building? We draw on Hardwicke et al.¬†(2018), considering these factors in the informativeness of an experiment.</p>
<p>Introduce p-hacking and publication biases as major sources of irreproducibility. These are biases - and we highlight bias reduction as a key goal.</p>
<!-- ::: {.case-study} -->
<!-- üî¨ Case study: Hardwicke et al. (2018; 2021) meta-studies of the analytic reproducibility of specific analytic findings in the empirical literature. -->
<!-- ::: -->

</div>
</div>
<p style="text-align: center;">
<a href="1-intro.html"><button class="btn btn-default">Previous</button></a>
<a href="3-ethics.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="toc/toc.js"></script>


</body>
</html>
