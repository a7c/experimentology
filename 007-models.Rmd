# Statistical models {#models} 

::: {.learning-goals}
üçé Learning goals: Describe what it means to ‚Äòcontrol for‚Äô something; explore what kinds of ‚Äòclustered variance‚Äô are present in our designs (e.g. selecting appropriate fixed vs. random effect structures); select models appropriate for different kinds of dependent variables.
:::

::: {.case-study}
üî¨ Case study: Sometimes effects are driven by specific stimuli. The ‚Äúlanguage as fixed effects fallacy‚Äù (Clark, 1973) means that many early psycholinguistic effects didn‚Äôt take into account stimulus variability. This critique extends to a wide variety of other domains, e.g. task fMRI (Westfall, Nichols, and Yarkoni 2016).
:::



## Inference and estimation for two-group designs

Throughout this book we've taken the position that the goal of experiments is to estimate a causal effect of interest, ideally as part of some theory of how different constructs relate to one another. All this talk of hypotheses and inferences above is only indirectly related to that goal. 





- Intuition builder: For very large n, or flat prior, Bayes and frequentist coincide. 


### Simple models of between-group differences

Introducing simple inference models:

- The chi-squared test for inferring whether two samples come from the same distribution
- The t-test for inferring whether a single group‚Äôs effect differs from 0
- The t-test for inferring whether two groups differ from one another
- The paired t-test as a first glimpse at how we might account for participant-level random effects (see Chapter 7).


::: {.accident-report}
‚ö†Ô∏è Accident report: Once you have the basic t-test under your belt, it might feel natural to compare each group to 0 and conclude that one group is different from 0 and the other one isn‚Äôt. But ‚Äúthe difference between significant and not significant is not necessarily itself statistically significant‚Äù (Nieuwenhuis, Forstmann, and Wagenmakers 2011). 
:::



How to go from theory to hypotheses to statistical model


## Effect size

With all our talk about estimation above, we didn't say nuch about what precisely was being estimated. 

Effect size: a common language for describing group differences. Pros and cons of this approach. Pro: comparability across studies. Con: loss of information about measures and real-world predictions; dependence on baseline variability. 

Effect sizes (ES):
Cohen‚Äôs = m1 - m2 / SD
Units of ES easy to compare across studies
Good for comparison 
Example: school interventions to improve achievement
Necessary for meta-analysis
Example: MA of infant consonant discrimination (cross method comparison)
Necessary for power analysis 
Negative: not related to any real units
Many measures of ES, e.g. r2, n2 (eta), log odds

Precision

Error bar:
 - standard deviation (why is this bad)?
- SEM
- CI

CIs for inference

Confidence intervals:
95% of these regions will contain the TRUE parameter
	Remember frequentists - there is a TRUE parameter

https://istats.shinyapps.io/ExploreCoverage/

But this is not our typical interpretation, which is that 95% chance parameter is in this interval
	That‚Äôs the BAYESIAN interpretation

Bayesian Estimation

Find the posterior distribution of the parameter of interest
	You can take its mean
	Its HPD (highest posterior density)

Confidence in confidence intervals: 
https://link.springer.com/article/10.3758/s13423-015-0947-8


::: {.interactive}
‚å®Ô∏è Interactive box: non-parametric simulations where you can shuffle data across groups a bunch of times and see what kind of distribution it produces by chance
:::


Re-casting the t-test as a regression model 

::: {.interactive}
‚å®Ô∏è Interactive box: Visualizing how different tests are variants of linear models.
:::

- Discrete data and logistic regression. Same thing, different linking function. (lead-in to GLM: probit, Poisson, beta, etc.).

Multilevel regression models of a difference between two groups, controlling for experimental items and subject

  - Sidebar: what should you control for? Different subcultures in psychology either post-hoc control for or look for moderation by demographic factors. We discuss the consequences of these decisions for both precision and causal inference. 

Causality revisited: what can and can‚Äôt be concluded from an experiment

::: {.accident-report}
‚ö†Ô∏è Accident report: Mediation going wrong: even when you have a randomized experiment, you can still mess up your causal inference (Montgomery, Nyhan, and Torres 2018). 
:::

- Dropping subjects who fail a manipulation check can be problematic (Aronow, Baron, and Pinson 2019).
- Mediation requires more confounding assumptions than causal inference about total effects, and these assumptions may be violated even in randomized experiments. 
