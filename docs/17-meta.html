<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 17 Meta-analysis | Experimentology" />
<meta property="og:type" content="book" />





<meta name="author" content="Michael C. Frank, Mika Braginsky, Julie Cachia, Nicholas Coles, Tom Hardwicke, Robert Hawkins, Maya Mathur, and Rondeline Williams" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Chapter 17 Meta-analysis | Experimentology">

<title>Chapter 17 Meta-analysis | Experimentology</title>

<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.5.3/htmlwidgets.js"></script>
<script src="libs/jquery-3.5.1/jquery.min.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.18/datatables.js"></script>
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.1.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.1/js/crosstalk.min.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="toc/toc.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="part"><span><b>I Preliminaries</b></span></li>
<li><a href="1-intro.html#intro"><span class="toc-section-number">1</span> Introduction: theory and experiments</a></li>
<li><a href="2-replication.html#replication"><span class="toc-section-number">2</span> Replication, reproducibility and transparency</a></li>
<li><a href="3-ethics.html#ethics"><span class="toc-section-number">3</span> Ethics</a></li>
<li class="part"><span><b>II Statistics</b></span></li>
<li><a href="4-estimation.html#estimation"><span class="toc-section-number">4</span> Estimation</a></li>
<li><a href="5-inference.html#inference"><span class="toc-section-number">5</span> Inference</a></li>
<li><a href="6-models.html#models"><span class="toc-section-number">6</span> Models</a></li>
<li class="part"><span><b>III Design and Planning</b></span></li>
<li><a href="7-measurement.html#measurement"><span class="toc-section-number">7</span> Measurement</a></li>
<li><a href="8-design.html#design"><span class="toc-section-number">8</span> Design of experiments</a></li>
<li><a href="9-sampling.html#sampling"><span class="toc-section-number">9</span> Sampling</a></li>
<li><a href="10-preregistration.html#preregistration"><span class="toc-section-number">10</span> Preregistration</a></li>
<li class="part"><span><b>IV Execution</b></span></li>
<li><a href="11-selection.html#selection"><span class="toc-section-number">11</span> Replicating or extending an existing study</a></li>
<li><a href="12-collection.html#collection"><span class="toc-section-number">12</span> Data collection</a></li>
<li><a href="13-management.html#management"><span class="toc-section-number">13</span> Data management and sharing</a></li>
<li class="part"><span><b>V Analysis and Reporting</b></span></li>
<li><a href="14-viz.html#viz"><span class="toc-section-number">14</span> Visualization</a></li>
<li><a href="15-eda.html#eda"><span class="toc-section-number">15</span> Exploratory data analysis</a></li>
<li><a href="16-writing.html#writing"><span class="toc-section-number">16</span> Reproducible writing</a></li>
<li><a href="17-meta.html#meta"><span class="toc-section-number">17</span> Meta-analysis</a></li>
<li><a href="18-conclusions.html#conclusions"><span class="toc-section-number">18</span> Conclusions</a></li>
<li class="part"><span><b>VI Appendices</b></span></li>
<li><a href="19-github.html#github"><span class="toc-section-number">19</span> Github Tutorial</a></li>
<li><a href="20-rmarkdown.html#rmarkdown"><span class="toc-section-number">20</span> R Markdown Tutorial</a></li>
<li><a href="21-tidyverse.html#tidyverse"><span class="toc-section-number">21</span> Tidyverse Tutorial</a></li>
<li><a href="22-ggplot.html#ggplot"><span class="toc-section-number">22</span> ggplot Tutorial</a></li>
<li><a href="23-instructors.html#instructors"><span class="toc-section-number">23</span> Instructor’s Guide</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="meta" class="section level1" number="17">
<h1><span class="header-section-number">Chapter 17</span> Meta-analysis</h1>
<div id="meta-notes" class="section level2" number="17.1">
<h2><span class="header-section-number">17.1</span> Meta-notes</h2>
<div id="notes-from-nicholas" class="section level3" number="17.1.1">
<h3><span class="header-section-number">17.1.1</span> Notes from Nicholas:</h3>
<p>Given that we are going to discuss the contact hypothesis meta-analysis quite extensively, I recommend that we remove the hotel towel study case study and focus on introducing the contact hypothesis meta. We could still use the similar structure of the hotel study writing, though, wherein we:</p>
<ol style="list-style-type: decimal">
<li>Explain that some studies found the effect but some didn’t</li>
<li>Mention that you may be inclined to vote count and conclude it’s unreliable (or moderated by something)</li>
<li>But when evidence is synthesized, there appears to be an effect.</li>
<li>It’s not magic–just meta-analysis. (Idk, this line made me laugh)</li>
</ol>
<p>The introduction I drafted up seems redundant with the section on vote counting. Perhaps we can just jump straight into your section?</p>
</div>
<div id="undefined-notation-and-terminology" class="section level3" number="17.1.2">
<h3><span class="header-section-number">17.1.2</span> Undefined notation and terminology</h3>
<ul>
<li>“Narrative review”</li>
<li>Need clear terminology to distinguish estimates from population effects; keep synced with earlier chapters</li>
</ul>
</div>
<div id="knowledge-prerequisites" class="section level3" number="17.1.3">
<h3><span class="header-section-number">17.1.3</span> Knowledge prerequisites</h3>
<p>If the following topics aren’t covered in earlier chapters, they will need to be added to this material:</p>
<ul>
<li>Standardized mean differences</li>
<li>“Marginal variance”</li>
</ul>
</div>
<div id="topics-that-should-probably-be-covered" class="section level3" number="17.1.4">
<h3><span class="header-section-number">17.1.4</span> Topics that should probably be covered</h3>
<ul>
<li>Forest plots (perhaps through applied example)</li>
<li>Extracting effect sizes</li>
<li>Systematic review methods</li>
</ul>
<div class="learning-goals">
<p>🍎 Learning goals: practice being a critical reader of the literature; extract effect sizes; conduct a simple random-effects meta analysis; monitor within- and across-study biases that can decrease precision and increase bias.</p>
</div>
<div class="case-study">
<p>🔬 Case study: Hotel towel reuse (Scheibehenne, Jamil, and Wagenmakers 2016). A simple example of aggregating noisy evidence from replications.</p>
<div id="introduction-1" class="section level2" number="17.2">
<h2><span class="header-section-number">17.2</span> Introduction?</h2>
<p>In a widely-cited study on the power of describing social norms, Goldstein, Cialdini, and Griskevicius (2008) gave one group of hotel guests a message that encouraged them to reuse their towels and another group of hotel guests a message that indicated that most other guests reused their towels. Across two studies, they found that guests who received the social norm message were more likely to reuse their towels than guests who received the control message (Study 1 <em>p</em> = .05, Study 2 <em>p</em> = .03). Five later replications, though, all failed to find significant evidence of increases in towel reuse amongst guests receiving the social norm message (all <em>p</em>s &gt; .05).</p>
<p>At first glance, you may be enticed to conclude that the effect of social norm messages does not reliably impact towel reuse. You may even go a step further and try to think of explanations for why one team found the effect, but the other did not. (Maybe the messages change behavior when guest receive nice fluffy towels, but doesn’t work when guests receive sets of crappy towels.) Or, if you’re a practitioner of experimentology, you may ask yourselves whether these findings are really all that different. You may even be so bold to do a meta-analysis!</p>
<p>If you did the latter, you would find that the results across these studies are, for the most part, quite consistent. Guests in four out of five of the studies were more likely to reuse their towels after receiving social norm messages, but the difference was not large enough in any one study to confidently conclude that this difference was larger than zero. When the results were statistically combined in a meta-analysis, Scheibehenne, Jamil, and Wagenmakers (2016) found that social norm messages <em>on average</em> do increase hotel towel reuse.</p>
<p>It’s not magic–it’s just meta-analysis! In this chapter, you’ll learn more about this useful technique.</p>
<p>Notes to Maya: Scheibehenne et al. did not estimate heterogeneity, so I do not know how consistent these results actually are. One of the studies did actually yield a sizeable oppostite-direction effect, so I’m not really sure if their fixed-effect assumption is reasonable. (Note, this was a commentary and tutorial about Bayesian meta-analysis, not an actual meta-analysis. So some of the methodological details are somewhat weak.) We may want to look into a scenario where (a) a fixed effect meta-analysis is better justified, (b) the direction of the effects is consistent across studies, but (c) the statistical significance of the effects is not consistent across studies.</p>
</div>
</div>
<p>What distinguishes meta-analysis from other types of reviews? Literature review typology, organized by your search, appraisal, synthesis, and analysis strategy (Grant and Booth 2009).</p>
<p>When many students hear the term “review,” they probably have flashbacks to a time where they threw some search term into Google Scholar, downloaded a bunch of articles that looked interesting, spent a few hours reading those articles, and then wrote a summary of what has been learned from these studies. There’s not necessarily anything wrong with this approach, but there are a few things that can be additionally learned from doing a meta-analysis.</p>
<p>Meta-analysis is a statistical technique for combining results from multiple studies.</p>
<p>In addition to reading articles, a researcher performing a meta-analysis would also extract information about the <em>effect size</em>.</p>
<p>After this information has been extracted, the researcher would perform an analysis where these effect sizes are combined in a single set of analyses. By combining information from multiple studies, the researcher can make a more precise estimate of the size of some effect. The researcher can also look at the extent to which this effect varies across studies. They may even them perform analyses indicating whether studies with certain characteristics systematically produce different results.</p>
<p>Think about how meta-analysis might give you a different impression of an area of study than a literature review. If you would have performed a literature review on studies examining the effect of social norm messaging on hotel towel re-use, you would have likely come away with the impression that there is an unreliable effect at best and no effect at worst. However, when statistically combining the results of these studies, a meta-analyst is able to see that the findings are more consistent than they appear at first glance–and that, when considering all the available evidence, it actually appears that social norm messaging does decrease hotel towel reuse. Considering that, for example, there were 1.7 billion overnight hotel bookings in the European Union alone in 2013 (Eurostat, 2015), the difference in the types of the conclusions formed by a literature review and a meta-analysis matter quite a bit!</p>
<p>Accident report: Money priming and vote counting.</p>
<ul>
<li><p>Reading the literature revisited: how to see the “bones” of the study – what is the manipulation, what’s the measure, how reliable and valid is it, how precise and generalizable is it, what are the inferences?</p></li>
<li><p>How not to do a meta-analysis (approaches that are intuitive but wrong)</p>
<ul>
<li>Don’t just count the significant positive studies (vote counting); don’t just average the estimates.</li>
</ul></li>
</ul>
<div class="accident-report">
<p>⚠️ Accident report: Money priming and vote counting. Vadillo et al. (2016) show a case where vote counting indicates that there is overwhelming support for money priming, but a meta-analysis reveals that the support is limited.</p>
</div>
</div>
</div>
<div id="running-example-the-contact-hypothesis" class="section level2" number="17.3">
<h2><span class="header-section-number">17.3</span> Running example: The contact hypothesis</h2>
<blockquote>
<p>[Introduce the applied example]</p>
</blockquote>
</div>
<div id="intuitive-but-problematic-approaches-to-evidence-synthesis" class="section level2" number="17.4">
<h2><span class="header-section-number">17.4</span> Intuitive, but problematic, approaches to evidence synthesis</h2>
<p>We have seen why it is important to synthesize evidence across studies on the same topic rather than just focus on one study at a time. How should we actually do this in practice?</p>
<p>When considering evidence across multiple studies, what most of us do intuitively is simply to count how many studies supported the hypothesis under investigation, versus how many did not support the hypothesis. Such a count usually amounts to counting the number of studies with “significant” <span class="math inline">\(p\)</span>-values, since (for better or for worse) “significance” is largely what drives studies’ take-home conclusions (<span class="citation"><label for="tufte-mn-138" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-138" class="margin-toggle">McShane and Gal (2017)<span class="marginnote">McShane, Blakeley B, and David Gal. 2017. <span>“Statistical Significance and the Dichotomization of Evidence.”</span> <em>Journal of the American Statistical Association</em> 112 (519): 885–95. <a href="https://doi.org/10.1080/01621459.2017.1289846">https://doi.org/10.1080/01621459.2017.1289846</a>.</span></span>; <span class="citation"><label for="tufte-mn-139" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-139" class="margin-toggle">Nelson, Rosenthal, and Rosnow (1986)<span class="marginnote">Nelson, Nanette, Robert Rosenthal, and Ralph L Rosnow. 1986. <span>“Interpretation of Significance Levels and Effect Sizes by Psychological Researchers.”</span> <em>American Psychologist</em> 41 (11): 1299.</span></span>). This is called “<strong>vote-counting</strong>” (<span class="citation"><label for="tufte-mn-140" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-140" class="margin-toggle">Borenstein et al. (2021)<span class="marginnote">Borenstein, Michael, Larry V Hedges, Julian PT Higgins, and Hannah R Rothstein. 2021. <em>Introduction to Meta-Analysis</em>. John Wiley &amp;amp; Sons.</span></span>). For example, in Paluck et al.’s meta-analysis on the contact hypothesis, 11 of 27 (EYEBALLED) studies were significant, all with positive point estimates. So, based on this vote-count, we would have the impression that most studies do not support the contact hypothesis. Indeed, many narrative reviews take essentially this vote-counting approach as well, albeit often not explicitly.</p>
<p>Despite its intuitive appeal, vote-counting can be very misleading because this approach characterizes evidence solely in terms of dichotomized <span class="math inline">\(p\)</span>-values, while entirely ignoring effect sizes. In Chapter <a href="2-replication.html#replication">2</a> on replication, we saw how this fetishism of statistical “significance” can mislead us when we consider individual studies, and these problems propagate if we vote-count statistical “significance” across studies as well. For example, it could be the case that relatively few studies are “significant” (for example, in a literature of typically small studies), yet if the studies typically have large point estimates, then the consensus of evidence could actually be quite strong. Or inversely, it could be that nearly all studies are “significant,” but if they typically have small point estimates, then the consensus of evidence might still be rather weak. In these cases, vote-counting could lead us badly astray (<span class="citation"><label for="tufte-mn-141" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-141" class="margin-toggle">Borenstein et al. (2021)<span class="marginnote">Borenstein, Michael, Larry V Hedges, Julian PT Higgins, and Hannah R Rothstein. 2021. <em>Introduction to Meta-Analysis</em>. John Wiley &amp;amp; Sons.</span></span>).</p>
<blockquote>
<p><strong>NOTE FROM NC</strong> It seems that the discussion of averaging studies should be pushed lower in the text . Perhaps we end here by saying: “Ok, but how do you actually do a meta-analysis?” In the next section, we can then focus first on the idea that you can’t just combine unstandardized effects (e.g., a 1 point shift on a 5 point scale is more impressive than a 1-point shift on a 100 point scale). We can then introduce one approach to standardizing mean differences and calculating the SE for this statistic, using Paluck as an example.</p>
<p>After we introduce standardization, I think we can introduce weighting.</p>
</blockquote>
<blockquote>
<p><strong>Original text saved here</strong></p>
<p>To avoid these pitfalls, a principled evidence synthesis needs to account for effect sizes themselves. How about if we just average the studies’ estimates? For example, in Paluck et al.’s meta-analysis, the mean of the studies’ estimates is XXX. This approach is perhaps a step in the right direction, but still has some important limitations. In particular, this approach gives equal weight to each study, such that a small study (like Hull et al., with a sample size of XXX [note: this had the second-smallest RE weight]) contributes as much to the mean effect size as a large study (like Boisjoly et al., with a sample size of XXX [note: this had the highest RE weight]). That doesn’t make sense: larger studies should clearly carry more weight in the analysis. This brings us to our first principled approach to meta-analysis, namely fixed-effects meta-analysis.</p>
</blockquote>
</div>
<div id="comparing-results-across-studies-via-standardization" class="section level2" number="17.5">
<h2><span class="header-section-number">17.5</span> Comparing results across studies via standardization</h2>
</div>
<div id="combining-results-across-studies-using-fixed-effects-meta-analysis" class="section level2" number="17.6">
<h2><span class="header-section-number">17.6</span> Combining results across studies using fixed-effects meta-analysis</h2>
<p>Fixed-effects meta-analysis is a simple way to average studies’ estimates while giving more weight to larger studies. Specifically, fixed-effects meta-analysis is a weighted average in which each study’s estimate is weighted by its inverse-variance (i.e., the inverse of its squared standard error). This makes sense because larger, more precise studies have smaller variances, so they should get more weight in the analysis. The fixed-effects pooled estimate is:</p>
<p><span class="math display">\[\widehat{\mu} = \frac{ \sum_{i=1}^k w_i \widehat{\theta}_i}{\sum_{i=1}^k w_i}\]</span> where <span class="math inline">\(k\)</span> is the number of studies, <span class="math inline">\(\widehat{\theta}_i\)</span> is the point estimate of the <span class="math inline">\(i^{th}\)</span> study, and <span class="math inline">\(w_i = 1/\widehat{\sigma}^2_i\)</span> is study <span class="math inline">\(i\)</span>’s weight in the analysis (i.e., the inverse of its variance).</p>
<blockquote>
<p>[Do we want to show the SE for <span class="math inline">\(\widehat{\mu}\)</span>?] NC: I am conflicted about this. On one hand, it seems odd that we don’t give any information about how one would perform a hypothesis test with the overall effect size estimate. On the other hand, I don’t want to scare people with too many equations. Maybe we can address this concern by explaining in simple language what the equations are doing and then putting the actual equation in the margin?</p>
</blockquote>
<div id="applied-example" class="section level3" number="17.6.1">
<h3><span class="header-section-number">17.6.1</span> Applied example</h3>
<p>In Paluck et al.’s meta-analysis, we would calculate the fixed-effects estimate, <span class="math inline">\(\widehat{\mu}\)</span>, as:</p>
<blockquote>
<p>[SHOW ARITHMETIC FOR THE FIRST FEW STUDIES].</p>
</blockquote>
<p>We thus estimate that the effect size in this studies is a standardized mean difference of <span class="math inline">\(\widehat{\mu} = XXX\)</span>; 95% confidence interval: [XXX]; <span class="math inline">\(p=\)</span>XXX. That is, we estimated that intergroup contact was associated with a decrease in prejudice of XXX standard deviations.</p>
</div>
<div id="limitations-of-fixed-effects-meta-analysis" class="section level3" number="17.6.2">
<h3><span class="header-section-number">17.6.2</span> Limitations of fixed-effects meta-analysis</h3>
<p>Fixed-effects meta-analysis does have important limitations. To see this intuitively, imagine we actually had the full dataset of individual observations from each study. (This is what meta-analysts dream of, but it’s rarely feasible in practice.) Fixed-effects meta-analysis is like simply concatenating all the studies’ datasets and then calculating the effect size, ignoring that observations came from different studies.</p>
<p>But what if the studies differ from one another? For example, in Paluck et al.’s meta-analysis, many design elements differed in ways that could make the interventions more or less effective in different studies: some studies recruited adult participants while others recruited children, some studies considered contact between participants of different races while others considered contact between participants of different ages, and so on. This means that the contact effect could actually differ across studies: the effects could be <strong>heterogeneous</strong>.</p>
<p>Does this presence of heterogeneity remind you of anything from when we analyzed repeated-measures data in Chapter <a href="6-models.html#models">6</a> on models? Recall that, with repeated-measures data, we had dealt with the possibility of heterogeneity across participants by introducing random intercepts by participant to our regression model. We can do much the same thing in meta-analysis; let’s turn to this now.</p>
</div>
</div>
<div id="random-effects-meta-analysis" class="section level2" number="17.7">
<h2><span class="header-section-number">17.7</span> Random-effects meta-analysis</h2>
<p>Whereas fixed-effects meta-analysis essentially postulates that all studies in the meta-analysis have the same population effect size, <span class="math inline">\(\mu\)</span>, random-effects meta-analysis instead postulates that studies’ population effects come from a normal distribution<label for="tufte-sn-90" class="margin-toggle sidenote-number">90</label><input type="checkbox" id="tufte-sn-90" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">90</span> Technically, other specifications of random-effects meta-analysis are possible. For example, robust variance estimation does not require making assumptions about the distribution of effects across studies (<span class="citation"><label for="tufte-mn-142" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-142" class="margin-toggle">Hedges, Tipton, and Johnson (2010)<span class="marginnote">Hedges, Larry V, Elizabeth Tipton, and Matthew C Johnson. 2010. <span>“Robust Variance Estimation in Meta-Regression with Dependent Effect Size Estimates.”</span> <em>Research Synthesis Methods</em> 1 (1): 39–65.</span></span>). These approaches also have other substantial advantages, like their ability to handle effects that are clustered (e.g., because some papers contribute multiple estimates; <span class="citation"><label for="tufte-mn-143" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-143" class="margin-toggle">Hedges, Tipton, and Johnson (2010)<span class="marginnote">Hedges, Larry V, Elizabeth Tipton, and Matthew C Johnson. 2010. <span>“Robust Variance Estimation in Meta-Regression with Dependent Effect Size Estimates.”</span> <em>Research Synthesis Methods</em> 1 (1): 39–65.</span></span>; <span class="citation"><label for="tufte-mn-144" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-144" class="margin-toggle">Pustejovsky and Tipton (2021)<span class="marginnote">Pustejovsky, James E, and Elizabeth Tipton. 2021. <span>“Meta-Analysis with Robust Variance Estimation: Expanding the Range of Working Models.”</span> <em>Prevention Science</em>, 1–14.</span></span>) and their ability to provide better inference in meta-analyses with relatively few studies (<span class="citation"><label for="tufte-mn-145" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-145" class="margin-toggle">Tipton (2015)<span class="marginnote">Tipton, Elizabeth. 2015. <span>“Small Sample Adjustments for Robust Variance Estimation with Meta-Regression.”</span> <em>Psychological Methods</em> 20 (3): 375.</span></span>). For these reasons, we tend to use these methods by default when conducting meta-analyses.</span> with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\tau\)</span>. The larger the standard deviation, <span class="math inline">\(\tau\)</span>, the more heterogeneous the effects are across studies. A random-effects model then estimates both <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau\)</span>, for example by maximum likelihood (<span class="citation"><label for="tufte-mn-146" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-146" class="margin-toggle">DerSimonian and Laird (1986)<span class="marginnote">DerSimonian, Rebecca, and Nan Laird. 1986. <span>“Meta-Analysis in Clinical Trials.”</span> <em>Controlled Clinical Trials</em> 7 (3): 177–88.</span></span>; <span class="citation"><label for="tufte-mn-147" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-147" class="margin-toggle">Brockwell and Gordon (2001)<span class="marginnote">Brockwell, Sarah E, and Ian R Gordon. 2001. <span>“A Comparison of Statistical Methods for Meta-Analysis.”</span> <em>Statistics in Medicine</em> 20 (6): 825–40.</span></span>).</p>
<p>The resulting estimate <span class="math inline">\(\widehat{\mu}\)</span> is still a weighted average of studies’ estimates, as for fixed-effects meta-analysis:</p>
<p><span class="math display">\[\widehat{\mu} = \frac{ \sum_{i=1}^k w_i \widehat{\theta}_i}{\sum_{i=1}^k w_i}\]</span> but the inverse-variance weights now incorporate the heterogeneity as well: <span class="math inline">\(w_i = 1/\left(\widehat{\tau}^2 + \widehat{\sigma}^2_i \right)\)</span>. These weights represent the inverse of studies’ <em>marginal</em> variances, comprising not only statistical error due to their finite sample sizes (<span class="math inline">\(\widehat{\sigma}^2_i\)</span>) but also genuine effect heterogeneity (<span class="math inline">\(\widehat{\tau}^2\)</span>).<label for="tufte-sn-91" class="margin-toggle sidenote-number">91</label><input type="checkbox" id="tufte-sn-91" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">91</span> The estimate of <span class="math inline">\(\widehat{\tau}^2\)</span> is a bit more complicated, but is essentially a weighted average of studies’ residuals, <span class="math inline">\(\widehat{\theta_i} - \widehat{\mu}\)</span>, while subtracting away variation due to statistical error, <span class="math inline">\(\widehat{\sigma}^2_i\)</span> (<span class="citation"><label for="tufte-mn-148" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-148" class="margin-toggle">DerSimonian and Laird (1986)<span class="marginnote">DerSimonian, Rebecca, and Nan Laird. 1986. <span>“Meta-Analysis in Clinical Trials.”</span> <em>Controlled Clinical Trials</em> 7 (3): 177–88.</span></span>; <span class="citation"><label for="tufte-mn-149" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-149" class="margin-toggle">Brockwell and Gordon (2001)<span class="marginnote">Brockwell, Sarah E, and Ian R Gordon. 2001. <span>“A Comparison of Statistical Methods for Meta-Analysis.”</span> <em>Statistics in Medicine</em> 20 (6): 825–40.</span></span>).</span></p>
<p>The upshot is that small studies contribute more strongly to a random-effects meta-analysis (i.e., they have larger weights) than they do to a fixed-effects meta-analysis. Additionally, confidence intervals are typically wider in random-effects meta-analysis than in fixed-effects meta-analysis; heuristically, this is because random-effects meta-analysis has to draw inference to a distribution of heterogeneous effects rather than to only one population effect that is shared by all studies.</p>
<div id="reporting-on-heterogeneity" class="section level3" number="17.7.1">
<h3><span class="header-section-number">17.7.1</span> Reporting on heterogeneity</h3>
<p>Remember that in random-effects meta-analysis, the estimate <span class="math inline">\(\widehat{\mu}\)</span> represents only the <em>mean</em> population effect across studies. It tells us nothing about how <em>variable</em> the effects are across studies. In practice, we would recommend always reporting the heterogeneity estimate <span class="math inline">\(\widehat{\tau}\)</span> as well, perhaps supplemented by other related metrics (<span class="citation"><label for="tufte-mn-150" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-150" class="margin-toggle">Riley, Higgins, and Deeks (2011)<span class="marginnote">Riley, Richard D, Julian PT Higgins, and Jonathan J Deeks. 2011. <span>“Interpretation of Random Effects Meta-Analyses.”</span> <em>BMJ</em> 342.</span></span>, <span class="citation"><label for="tufte-mn-151" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-151" class="margin-toggle">Wang and Lee (2019)<span class="marginnote">Wang, Chia-Chun, and Wen-Chung Lee. 2019. <span>“A Simple Method to Estimate Prediction Intervals and Predictive Distributions: Summarizing Meta-Analyses Beyond Means and Confidence Intervals.”</span> <em>Research Synthesis Methods</em> 10 (2): 255–66.</span></span>, <span class="citation"><label for="tufte-mn-152" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-152" class="margin-toggle">Mathur and VanderWeele (2019)<span class="marginnote">Mathur, Maya B, and Tyler J VanderWeele. 2019. <span>“New Metrics for Meta-Analyses of Heterogeneous Effects.”</span> <em>Statistics in Medicine</em> 38 (8): 1336–42.</span></span>, <span class="citation"><label for="tufte-mn-153" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-153" class="margin-toggle">Mathur and VanderWeele (2020)<span class="marginnote">Mathur, Maya B, and Tyler J VanderWeele. 2020. <span>“Robust Metrics and Sensitivity Analyses for Meta-Analyses of Heterogeneous Effects.”</span> <em>Epidemiology</em> 31 (3): 356–58.</span></span>). Reporting on the heterogeneity can help indicate how consistent or inconsistent the effects are across studies, which may point to the need to investigate moderators of the effect.<label for="tufte-sn-92" class="margin-toggle sidenote-number">92</label><input type="checkbox" id="tufte-sn-92" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">92</span> For example, one approach to investigate moderators in meta-analysis is meta-regression, in which moderators (e.g., type of intergroup contact) are included as covariates in a random-effects meta-analysis model (<span class="citation"><label for="tufte-mn-154" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-154" class="margin-toggle">Thompson and Higgins (2002)<span class="marginnote">Thompson, Simon G, and Julian PT Higgins. 2002. <span>“How Should Meta-Regression Analyses Be Undertaken and Interpreted?”</span> <em>Statistics in Medicine</em> 21 (11): 1559–73.</span></span>). As in standard regression, coefficients can then be estimated for each moderator, representing the mean difference in population effect between studies with versus without the moderator.</span></p>
</div>
<div id="applied-example-1" class="section level3" number="17.7.2">
<h3><span class="header-section-number">17.7.2</span> Applied example</h3>
<p>Conducting a random-effects meta-analysis of Paluck et al.’s dataset yields <span class="math inline">\(\widehat{\mu} = XXX\)</span>; 95% confidence interval: [XXX]; <span class="math inline">\(p=\)</span>XXX. That is, we estimated that, <em>on average across studies</em>, intergroup contact was associated with a decrease in prejudice of XXX standard deviations. Additionally, we estimated that the standard deviation of population effects was <span class="math inline">\(\widehat{\tau}=XXX\)</span>; 95% confidence interval: [XXX].</p>
<blockquote>
<p>[COULD INCLUDE A NORMAL DENSITY TO ILLUSTRATE THE FITTED DISTRIBUTION]</p>
</blockquote>
</div>
</div>
<div id="bias-in-meta-analysis" class="section level2" number="17.8">
<h2><span class="header-section-number">17.8</span> Bias in meta-analysis</h2>
<p>Meta-analysis is an invaluable tool to synthesize evidence across studies. However, meta-analyses can be compromised by two categories of bias: <strong>within-study biases</strong> and <strong>across-study biases</strong>. Either type of bias can lead to meta-analysis estimates that are too large, too small, or in the wrong direction. We will now discuss examples of each type of bias. We will also discuss ways to address these biases when conducting a meta-analysis, including mitigating the biases at the outset through sound meta-analysis design and also assessing the robustness of the ultimate conclusions to possible remaining bias.</p>
<div id="within-study-biases" class="section level3" number="17.8.1">
<h3><span class="header-section-number">17.8.1</span> Within-study biases</h3>
<blockquote>
<p>I think it can be useful to directly point the reader to the garbage-in-garbage-out analogy here.<br />
<br />
For example, we could invite the reader to imagine a problematic study. E.g., a within-subject investigation of whether people prefer Meal A or Meal B but does not randomize the order in which people eat the meals. Assuming that the meals are equally tasty but that people are full after eating one meal, we would consistently find that people prefer Meal A (which is eaten first).</p>
<p>If we replicated that biased study 5 times and then put it in a meta-analysis, we would once again likely find that people prefer Meal A. But this is, of course, driven by within-study bias!</p>
</blockquote>
<p>Within-study biases affect the internal validity of the meta-analyzed studies’ internal validity and include, for example, XXX (Chapter XXX; [use examples of types of bias that were discussed earlier]). For example, in Paluck et al.’s meta-analysis, XXX.</p>
<p>These within-study biases can propagate to the results of the meta-analysis. To mitigate this problem, meta-analysts should try to define study eligibility criteria that reduce within-study biases and, after data have been collected, should qualitatively assess studies’ risks of bias using established rating tools (<span class="citation"><label for="tufte-mn-155" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-155" class="margin-toggle">Mathur and VanderWeele (2022)<span class="marginnote">Mathur, Maya B, and Tyler J VanderWeele. 2022. <span>“Methods to Address Confounding and Other Biases in Meta-Analyses: Review and Recommendations.”</span> <em>Annual Review of Public Health</em>.</span></span>). Additionally, it is often informative to conduct quantitative quantitatively assess how sensitive meta-analysis results may be to residual bias that cannot be eliminated by limiting study eligibility (<span class="citation"><label for="tufte-mn-156" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-156" class="margin-toggle">Mathur and VanderWeele (2022)<span class="marginnote">Mathur, Maya B, and Tyler J VanderWeele. 2022. <span>“Methods to Address Confounding and Other Biases in Meta-Analyses: Review and Recommendations.”</span> <em>Annual Review of Public Health</em>.</span></span>).</p>
</div>
<div id="across-study-biases" class="section level3" number="17.8.2">
<h3><span class="header-section-number">17.8.2</span> Across-study biases</h3>
<p>Across-study biases affect the meta-analyzed literature holistically and include, for example, publication bias and selective reporting, as discussed in Chapter XXX. Often, these across-study biases favor statistically significant positive results; in this case, results that survive to publication and inclusion in the meta-analysis will typically have inflated estimates (they are too large compared to the truth) and so result in an inflated meta-analytic estimate.</p>
<p>As in the context of within-study biases, meta-analysts should try to mitigate across-study biases during the design process. That is, meta-analysis inclusion criteria should be designed to cast a broad net, capturing not only high-profile, published studies on the topic, but also studies published in low-profile journals, and “gray literature” such as unpublished dissertations and theses (<span class="citation"><label for="tufte-mn-157" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-157" class="margin-toggle">Lefebvre et al. (2019)<span class="marginnote">Lefebvre, Carol, Julie Glanville, Simon Briscoe, Anne Littlewood, Chris Marshall, Maria-Inti Metzendorf, Anna Noel-Storr, et al. 2019. <span>“Searching for and Selecting Studies.”</span> <em>Cochrane Handbook for Systematic Reviews of Interventions</em>, 67–107.</span></span>).<label for="tufte-sn-93" class="margin-toggle sidenote-number">93</label><input type="checkbox" id="tufte-sn-93" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">93</span> However, evidence is mixed regarding whether including gray literature actually reduces across-study biases in meta-analysis (<span class="citation">(<a href="#ref-tsuji2019addressing" role="doc-biblioref"><strong>tsuji2019addressing?</strong></a>)</span>, <span class="citation"><label for="tufte-mn-158" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-158" class="margin-toggle">Mathur and VanderWeele (2021)<span class="marginnote">Mathur, Maya B, and Tyler J VanderWeele. 2021. <span>“Estimating Publication Bias in Meta-Analyses of Peer-Reviewed Studies: A Meta-Meta-Analysis Across Disciplines and Journal Tiers.”</span> <em>Research Synthesis Methods</em> 12 (2): 176–91.</span></span>).</span> A broad search will also help capture not only studies in which the analysis that is relevant to the meta-analysis was primary “headline” finding, but also studies that reported the relevant analysis only as a secondary or supplemental result (perhaps because it wasn’t “significant!”) (<span class="citation"><label for="tufte-mn-159" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-159" class="margin-toggle">Mathur and VanderWeele (2021)<span class="marginnote">Mathur, Maya B, and Tyler J VanderWeele. 2021. <span>“Estimating Publication Bias in Meta-Analyses of Peer-Reviewed Studies: A Meta-Meta-Analysis Across Disciplines and Journal Tiers.”</span> <em>Research Synthesis Methods</em> 12 (2): 176–91.</span></span>).</p>
<p>There are also statistical methods to help assess how robust the results may be to across-study biases. Among the most popular tools to assess and correct for publication bias is the <strong>funnel plot</strong> (<span class="citation"><label for="tufte-mn-160" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-160" class="margin-toggle">Duval and Tweedie (2000)<span class="marginnote">Duval, Sue, and Richard Tweedie. 2000. <span>“Trim and Fill: A Simple Funnel-Plot–Based Method of Testing and Adjusting for Publication Bias in Meta-Analysis.”</span> <em>Biometrics</em> 56 (2): 455–63. <a href="https://doi.org/10.1111/j.0006-341X.2000.00455.x">https://doi.org/10.1111/j.0006-341X.2000.00455.x</a>.</span></span>; <span class="citation"><label for="tufte-mn-161" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-161" class="margin-toggle">Egger et al. (1997)<span class="marginnote">Egger, Matthias, George Davey Smith, Martin Schneider, and Christoph Minder. 1997. <span>“Bias in Meta-Analysis Detected by a Simple, Graphical Test.”</span> <em>BMJ</em> 315 (7109): 629–34. <a href="https://doi.org/10.1136/bmj.315.7109.629">https://doi.org/10.1136/bmj.315.7109.629</a>.</span></span>), a graph relating the meta-analyzed studies’ point estimates to some measure of their precision, such as sample size or standard error. Here is an example of a funnel plot for a hypothetical meta-analysis with no publication bias:</p>
<blockquote>
<p>[Insert generic funnel plot]</p>
</blockquote>
<p>Notice that larger studies (those with smaller standard errors) cluster more closely around the mean than do smaller studies, but large and small studies alike would have point estimates centered around the mean. That is, the funnel plot is symmetric. In contrast, a funnel plot might look asymmetric:</p>
<blockquote>
<p>[Insert example of asymmetric funnel plot. I think Paluck’s will work, and it’s a great example of asymmetry that could reflect substantive differences between large and small studies.]</p>
</blockquote>
<p>Here, there appears to be a correlation between studies’ estimates and their standard errors, such that smaller studies tend to have larger estimates than do larger studies. Such a correlation can indicate the presence of <strong>“small-study effects”</strong>, meaning that effect sizes differ systematically between small and large studies (<span class="citation"><label for="tufte-mn-162" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-162" class="margin-toggle">Sterne et al. (2011)<span class="marginnote">Sterne, Jonathan AC, Alex J Sutton, John PA Ioannidis, Norma Terrin, David R Jones, Joseph Lau, James Carpenter, et al. 2011. <span>“Recommendations for Examining and Interpreting Funnel Plot Asymmetry in Meta-Analyses of Randomised Controlled Trials.”</span> <em>BMJ</em> 343. <a href="https://doi.org/10.1136/bmj.d4002">https://doi.org/10.1136/bmj.d4002</a>.</span></span>).</p>
<p>Several popular statistical methods, such as Trim-and-Fill (<span class="citation"><label for="tufte-mn-163" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-163" class="margin-toggle">Duval and Tweedie (2000)<span class="marginnote">Duval, Sue, and Richard Tweedie. 2000. <span>“Trim and Fill: A Simple Funnel-Plot–Based Method of Testing and Adjusting for Publication Bias in Meta-Analysis.”</span> <em>Biometrics</em> 56 (2): 455–63. <a href="https://doi.org/10.1111/j.0006-341X.2000.00455.x">https://doi.org/10.1111/j.0006-341X.2000.00455.x</a>.</span></span>) and Egger’s regression (<span class="citation"><label for="tufte-mn-164" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-164" class="margin-toggle">Egger et al. (1997)<span class="marginnote">Egger, Matthias, George Davey Smith, Martin Schneider, and Christoph Minder. 1997. <span>“Bias in Meta-Analysis Detected by a Simple, Graphical Test.”</span> <em>BMJ</em> 315 (7109): 629–34. <a href="https://doi.org/10.1136/bmj.315.7109.629">https://doi.org/10.1136/bmj.315.7109.629</a>.</span></span>) are designed to quantify this type of asymmetry. It is critical to note that small-study effects need not always represent publication bias: they can also reflect genuine substantive differences between small and large studies (<span class="citation"><label for="tufte-mn-165" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-165" class="margin-toggle">Egger et al. (1997)<span class="marginnote">Egger, Matthias, George Davey Smith, Martin Schneider, and Christoph Minder. 1997. <span>“Bias in Meta-Analysis Detected by a Simple, Graphical Test.”</span> <em>BMJ</em> 315 (7109): 629–34. <a href="https://doi.org/10.1136/bmj.315.7109.629">https://doi.org/10.1136/bmj.315.7109.629</a>.</span></span>; <span class="citation"><label for="tufte-mn-166" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-166" class="margin-toggle">Lau et al. (2006)<span class="marginnote">Lau, Joseph, John PA Ioannidis, Norma Terrin, Christopher H Schmid, and Ingram Olkin. 2006. <span>“The Case of the Misleading Funnel Plot.”</span> <em>BMJ</em> 333 (7568): 597–600. <a href="https://doi.org/10.1136/bmj.333.7568.597">https://doi.org/10.1136/bmj.333.7568.597</a>.</span></span>). For example, in a meta-analysis of intervention studies, if the most effective interventions are also the most expensive or difficult to implement, these highly effective interventions might be used primarily in the smallest studies. Funnel plot methods detect these types of small-study effects as well as certain limited forms of publication bias (<span class="citation"><label for="tufte-mn-167" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-167" class="margin-toggle">Maier, VanderWeele, and Mathur (2021)<span class="marginnote">Maier, Maximilian, Tyler J VanderWeele, and Maya B Mathur. 2021. <span>“Using Selection Models to Assess Sensitivity to Publication Bias: A Tutorial and Call for More Routine Use.”</span> <em>Under Review</em>.</span></span>).<label for="tufte-sn-94" class="margin-toggle sidenote-number">94</label><input type="checkbox" id="tufte-sn-94" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">94</span> Essentially, funnel plots and most related methods can detect publication bias in which (1) small studies with large positive point estimates are more likely to be published than small studies with small or negative point estimates; and (2) the largest studies are published regardless of the magnitude of their point estimates. Funnel plots may not detect publication bias that favors significant results. For more detail on these points, see <span class="citation"><label for="tufte-mn-168" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-168" class="margin-toggle">Maier, VanderWeele, and Mathur (2021)<span class="marginnote">Maier, Maximilian, Tyler J VanderWeele, and Maya B Mathur. 2021. <span>“Using Selection Models to Assess Sensitivity to Publication Bias: A Tutorial and Call for More Routine Use.”</span> <em>Under Review</em>.</span></span>.</span></p>
<p>For these reasons, assessments of publication bias in meta-analysis should rely not only on funnel plots (<span class="citation"><label for="tufte-mn-169" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-169" class="margin-toggle">Maier, VanderWeele, and Mathur (2021)<span class="marginnote">Maier, Maximilian, Tyler J VanderWeele, and Maya B Mathur. 2021. <span>“Using Selection Models to Assess Sensitivity to Publication Bias: A Tutorial and Call for More Routine Use.”</span> <em>Under Review</em>.</span></span>). There are numerous other methods that can be applied as well, such as <strong>selection models</strong>.<label for="tufte-sn-95" class="margin-toggle sidenote-number">95</label><input type="checkbox" id="tufte-sn-95" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">95</span> High-level overviews of selection models are given in <span class="citation"><label for="tufte-mn-170" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-170" class="margin-toggle">McShane, Böckenholt, and Hansen (2016)<span class="marginnote">McShane, Blakeley B, Ulf Böckenholt, and Karsten T Hansen. 2016. <span>“Adjusting for Publication Bias in Meta-Analysis: An Evaluation of Selection Methods and Some Cautionary Notes.”</span> <em>Perspectives on Psychological Science</em> 11 (5): 730–49. <a href="https://doi.org/10.1177/1745691616662243">https://doi.org/10.1177/1745691616662243</a>.</span></span> and <span class="citation"><label for="tufte-mn-171" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-171" class="margin-toggle">Maier, VanderWeele, and Mathur (2021)<span class="marginnote">Maier, Maximilian, Tyler J VanderWeele, and Maya B Mathur. 2021. <span>“Using Selection Models to Assess Sensitivity to Publication Bias: A Tutorial and Call for More Routine Use.”</span> <em>Under Review</em>.</span></span>. for more methodological detail, see for example <span class="citation"><label for="tufte-mn-172" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-172" class="margin-toggle">Hedges (1984)<span class="marginnote">Hedges, Larry V. 1984. <span>“Estimation of Effect Size Under Nonrandom Sampling: The Effects of Censoring Studies Yielding Statistically Insignificant Mean Differences.”</span> <em>Journal of Educational Statistics</em> 9 (1): 61–85. <a href="https://doi.org/10.3102/10769986009001061">https://doi.org/10.3102/10769986009001061</a>.</span></span>, <span class="citation">(<a href="#ref-iyengar1988selection" role="doc-biblioref"><strong>iyengar1988selection?</strong></a>)</span>, and <span class="citation">(<a href="#ref-vevea1995general" role="doc-biblioref"><strong>vevea1995general?</strong></a>)</span>. For a tutorial on fitting and interpreting selection models, see <span class="citation"><label for="tufte-mn-173" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-173" class="margin-toggle">Maier, VanderWeele, and Mathur (2021)<span class="marginnote">Maier, Maximilian, Tyler J VanderWeele, and Maya B Mathur. 2021. <span>“Using Selection Models to Assess Sensitivity to Publication Bias: A Tutorial and Call for More Routine Use.”</span> <em>Under Review</em>.</span></span>.</span> These models can detect other forms of publication bias that funnel plots may not detect, such as publication bias that favors significant results.</p>
<p>You may also have heard of “<span class="math inline">\(p\)</span>-methods” to detect across-study biases such as <span class="math inline">\(p\)</span>-curve and <span class="math inline">\(p\)</span>-uniform (<span class="citation"><label for="tufte-mn-174" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-174" class="margin-toggle">Simonsohn, Nelson, and Simmons (2014a)<span class="marginnote">Simonsohn, Uri, Leif D Nelson, and Joseph P Simmons. 2014a. <span>“P-Curve: A Key to the File-Drawer.”</span> <em>Journal of Experimental Psychology: General</em> 143 (2): 534.</span></span>, <span class="citation"><label for="tufte-mn-175" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-175" class="margin-toggle">Simonsohn, Nelson, and Simmons (2014b)<span class="marginnote">Simonsohn, Uri, Leif D Nelson, and Joseph P Simmons. 2014b. <span>“P-Curve and Effect Size: Correcting for Publication Bias Using Only Significant Results.”</span> <em>Perspectives on Psychological Science</em> 9 (6): 666–81.</span></span>, <span class="citation"><label for="tufte-mn-176" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-176" class="margin-toggle">Van Assen, Aert, and Wicherts (2015)<span class="marginnote">Van Assen, Marcel ALM, Robbie van Aert, and Jelte M Wicherts. 2015. <span>“Meta-Analysis Using Effect Size Distributions of Only Statistically Significant Studies.”</span> <em>Psychological Methods</em> 20 (3): 293.</span></span>). These methods essentially assess whether the significant <span class="math inline">\(p\)</span>-values “bunch up” just under 0.05, which is taken to indicate publication bias. These methods are increasingly popular in psychology and have merits. However, it is important to note that these methods are actually simplified versions of selection models (e.g., <span class="citation"><label for="tufte-mn-177" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-177" class="margin-toggle">Hedges (1984)<span class="marginnote">Hedges, Larry V. 1984. <span>“Estimation of Effect Size Under Nonrandom Sampling: The Effects of Censoring Studies Yielding Statistically Insignificant Mean Differences.”</span> <em>Journal of Educational Statistics</em> 9 (1): 61–85. <a href="https://doi.org/10.3102/10769986009001061">https://doi.org/10.3102/10769986009001061</a>.</span></span>) that work only under considerably more restrictive settings than do the original selection models (for example, when there is not heterogeneity across studies; <span class="citation"><label for="tufte-mn-178" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-178" class="margin-toggle">McShane, Böckenholt, and Hansen (2016)<span class="marginnote">McShane, Blakeley B, Ulf Böckenholt, and Karsten T Hansen. 2016. <span>“Adjusting for Publication Bias in Meta-Analysis: An Evaluation of Selection Methods and Some Cautionary Notes.”</span> <em>Perspectives on Psychological Science</em> 11 (5): 730–49. <a href="https://doi.org/10.1177/1745691616662243">https://doi.org/10.1177/1745691616662243</a>.</span></span>). For this reason, it is usually (although not always) better to use selection models in place of the more restrictive <span class="math inline">\(p\)</span>-methods.</p>
<div class="case-study">
<p>🔬 Case study: Publication bias in the social sciences: Unlocking the file drawer (Franco, Malhotra, and Simonovits, 2014)</p>
<p>In 2014, Franco and colleagues examined the population of 221 studies conducted through a funding initiative that helps researchers run experiments on nationally-representative samples in the U.S.. First, Franco and colleagues examined the records of these studies to determine whether the researchers found statistically significant results, a mixture of statistically significant and nonsignificant results, or only nonsignificant results. Then, Franco and colleagues examined the likelihood that these results were published in the scientific literature.</p>
<p>What results do you think were lease likely to be published? If you guessed “nonsignificant results” you are correct. Less than 25% of studies that produced only statistically non-significant results were published in the scientific literature, compared to over 60% of studies that produced statistically significant results. These results are consistent with fears about publication bias: that studies yielding non-significant results are less likely to be published than those that produce significant results.</p>
</div>
</div>
</div>
<div id="summary" class="section level2" number="17.9">
<h2><span class="header-section-number">17.9</span> Summary</h2>
<div class="exercise">
<p><span id="exr:unlabeled-div-2" class="exercise"><strong>Exercise 17.1  </strong></span>Exercises:</p>
<ol style="list-style-type: decimal">
<li><p>a.) You read the following result in the abstract of a meta-analysis: “In 83 randomized studies of middle school children, replacing one hour of class time with mindfulness meditation significantly improved standardized test scores (standardized mean difference <span class="math inline">\(\widehat{\mu} = 0.05\)</span>; 95% confidence interval: [<span class="math inline">\(0.01, 0.09\)</span>]; <span class="math inline">\(p&lt;0.05\)</span>).” Why is this a problematic way to report on meta-analysis results? Suggest a better sentence to replace this one.</p>
<p>b.) You read the rest of the meta-analysis, which concludes with: “These findings demonstrate robust benefits of meditation for children, suggesting that test scores improve even when the meditation is introduced as a replacement for normal class time.” You recall that the heterogeneity estimate was <span class="math inline">\(\widehat{\tau} = 0.90\)</span>. Do you think that this result regarding the heterogeneity tends to support, or rather tends to undermine, the concluding sentence of the meta-analysis? Why?</p>
<p>c.) What kinds of within-study biases would concern you in this meta-analysis? How might you assess the credibility of the meta-analyzed studies and of the meta-analysis as whole in light of these possible biases?</p></li>
<li><p>You do a meta-analysis on a literature in which statistically significant results in either direction are much more likely to be published that nonsignificant results. Draw the funnel plot you would expect to see. Is the plot symmetric or asymmetric?</p></li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-3" class="exercise"><strong>Exercise 17.2  </strong></span>Challenge exercises:</p>
<ol style="list-style-type: decimal">
<li>Why do you think small studies receive more weight in random-effects meta-analysis than in fixed-effects meta-analysis? Can you see why this is true mathematically based on the equations given above, and can you also explain the intuition in simple language?</li>
</ol>
</div>
<div class="accident-report">
<p>⚠️ Quick draft of accident report: Garbage in, garbage out? Meta-analyzing bad research (Coles et al. 2019)</p>
<p>You may have heard that Botox can help eliminate wrinkles. But some researchers have also suggested that it may help treat clinical depression when used to paralyze the muscles associated with frowning. As crazy as they may sound, a quick examination of the literature would lead many to conclude that this treatment works. Studies that randomly assign depressed patients to either receive Botox injections or saline injections do indeed find that Botox recipients exhibit decreases in depression. And when you combine all available evidence in a meta-analysis, you find that this difference is quite large: <span class="math inline">\(\widehat{\mu} = 0.83\)</span>, 95% CI [0.52, 1.14].</p>
<p>However, Coles and colleagues (2019) pointed out that there is a problem with within-study bias. Participants are not supposed to know whether they have been randomly assigned to receive Botox or a control saline injections. However, only one of these treatments leads the upper half of your face to be paralyzed, and after a couple of weeks you’re likely to figure out whether you received the treatment or control injection. Thus, it is possible that the apparent effect of Botox is actually an effect of placebo. The meta-analytic conclusions are potentially undermined by within-study bias.</p>
<p>Coles and colleagues (2019) also found evidence of between-study bias. When coding the effect sizes in the literature, they found that 51% of the outcomes measured were not reported by the study authors. For example, researchers may have collected two measures of depression, but only reported one in the manuscript. This raises concerns about selective reporting: that researchers examining the effects of Botox on depression are only reporting the outcomes that demonstrate an effect, while hiding away the outcomes that do not. In this scenario, any meta-analytic conclusions are potentially undermined by between-study bias.</p>
<p>This is sometimes called the Garbage In, Garbage Out problem in meta-analysis. If within- and between-study bias is not properly mitigated, it is difficult to conclude make valid inferences in a meta-analysis.</p>
</div>

</div>
</div>
<p style="text-align: center;">
<a href="16-writing.html"><button class="btn btn-default">Previous</button></a>
<a href="18-conclusions.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="toc/toc.js"></script>


</body>
</html>
