--- 
title: "Experimentology"
subtitle: "An Open Science Approach to Experimental Psychology Methods "
author: "Michael C. Frank, Mika Braginsky, Julie Cachia, Nicholas Coles, Tom Hardwicke, Robert Hawkins, Maya Mathur, and Rondeline Williams"

site: bookdown::bookdown_site
documentclass: book

bibliography: experimentology.bib
biblio-style: apalike
link-citations: yes
---

# {.unlisted .unnumbered}

Placeholder


## Introduction {-}
## What this book is and isn't about {-}
## Themes {-}
## The software toolkit of the behavioral researcher (and of this book) {-}

<!--chapter:end:index.Rmd-->


# (PART) Preliminaries {-}
# Introduction: theory and experiments {#intro}

Placeholder


## What is an experiment?
### Causal inference
## What is a theory? 
### Popper, Kuhn, and Lakatos, oh my!
## How to do experiments that help build theory?
## What does an ideal experiment look like?

<!--chapter:end:001-intro.Rmd-->


# Replication, reproducibility and transparency  {#replication}

Placeholder



<!--chapter:end:002-replication.Rmd-->


# Ethics

Placeholder


## Case study: Diederick Stapel
## Ethical Frameworks
### The Deontological Approach
### The Utilitarian Approach
### The Virtue Approach
## Experimental Ethics
### The IRB
### Debriefing Participants
### Special Considerations for Vulnerable Populations 
## Publication Ethics
### Authorship
### Disclosures
### Generalizability Statements
### Post-publication Errors
### Intellectual Humility
## The Ethical Duty for Transparency
### The negatives of openness
### Individuals should consider openness as a default
### Institutions can mitigate the risks and costs of openness
### Conclusion

<!--chapter:end:003-ethics.Rmd-->


# (PART) Design and Planning {-}
# Measurement {#measurement}

Placeholder


## Reliability
### Measurement scales
### Paradoxes in reliability
### Practical advice for computing reliability
## Validity
## How to select a good measure?
### What to measure? 
### Survey measures
## The temptation to measure lots of things
## Chapter summary

<!--chapter:end:004-measurement.Rmd-->


# Design of experiments {#design}

Placeholder


## Manipulation validity
## How to design a manipulation to test a theory.
## Confounds and covariates
## The temptation to manipulate lots of things

<!--chapter:end:005-design.Rmd-->

```{r include=FALSE, cache=FALSE}
library(tidyverse)
library(glue)
library(knitr)
library(shiny)

opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  error = FALSE,
  comment = "#>",
  collapse = TRUE,
  cache = TRUE,
  echo = FALSE,
  cache.lazy = FALSE,
  dev = "png",
  dpi = 300,
  out.width = "\\linewidth"
)
kable <- function(...) knitr::kable(..., booktabs = TRUE, linesep = "")

set.seed(42)

.font <- "Source Sans Pro"
theme_set(theme_bw(base_size = 14, base_family = .font))
theme_update(panel.grid = ggplot2::element_blank(),
             strip.background = ggplot2::element_blank(),
             legend.key = ggplot2::element_blank(),
             panel.border = ggplot2::element_blank(),
             axis.line = ggplot2::element_line(),
             strip.text = ggplot2::element_text(face = "bold"))
.grey <- "grey70"
.refline <- "dotted"
.coef_line <- element_line(colour = .grey, size = 0.1)

.pal <- ggthemes::ptol_pal
.scale_colour_discrete <- ggthemes::scale_colour_ptol
.scale_color_discrete <- .scale_colour_discrete
.scale_fill_discrete <- ggthemes::scale_fill_ptol

.scale_colour_continuous <- viridis::scale_colour_viridis
.scale_color_continuous <- .scale_colour_continuous
.scale_fill_continuous <- viridis::scale_fill_viridis

.scale_colour_numerous <- scale_colour_discrete
.scale_color_numerous <- .scale_colour_numerous
.scale_fill_numerous <- scale_fill_discrete

# from https://bookdown.org/yihui/rmarkdown-cookbook/font-color.html
colorize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
            x)
  } else x
}
```
# Estimation and inference {#inference}

::: {.learning-goals}
üçé Learning goals: 

* Discuss differences between frequentist and Bayesian perspectives
* Reconceptualize statistical ‚Äútests‚Äù as models of data
* Build intuitions about how specific ‚Äútests‚Äù (e.g., t-tests) relate to more general frameworks (e.g., regression, mixed effects models)
* Identify which models are best suited for which research questions 
* Reason about effect effect size as estimated by statistical models
:::

We've been arguing that experiments are about measuring effects. You might ask then, why does this book even need a chapter about statistical inference? Why can't we just report our measurements and be done? 

Statistical inference is critical to experimental methods because experiments -- especially experiments with human participants -- tend to yield noisy and variable data. Statistical methods allow us to make sense of the data and ask questions like:

1. How likely is it that this pattern of measurements was produced by chance variation? 
2. Do these data provide more support for one hypothesis or another? 
3. How big is the effect of a manipulation, and how precise is our estimate of that effect?
4. What portion of the variation in the data is due to a particular manipulation (as opposed to variation between participants, stimulus items, or other manipulations)?

Question (1) is associated with one particular type of statistical testing -- **null hypothesis significance testing** (NHST) in the **frequentist** statistical tradition. NHST has become synonymous with data analysis, such that in the vast majority of research papers (and research  methods courses), all of the reported analyses are tests of this type. Yet the equivalence of NHST and data analysis more generally has been quite problematic. The instinct to "go test your data for significance" before visualizing your data and trying to understand how it relates to the various sources of variation in your design (participants, items, manipulations, etc.) is one of the most unhelpful things an experimenter can do. Whether $p < .05$ or not, a test of this sort gives you literally *one bit* of information about your data.^[In the information theoretic sense, as well as the common sense!] The kinds of visualizations we advocate in Chapter \@ref(viz) give you a much richer sense of what happened in your experiment!

In this chapter, we will describe NHST, the conventional method that many students still learn (and many scientists still use) as their primary method for engaging with data. All practicing experimentalists need to understand NHST both to read the literature and also to apply this method in appropriate situations -- for example, tests of interventions where the key question for informing practice is whether there is a significant difference between a treatment condition and an appropriate control. But we will also try to contextualize NHST as a very special case of a broader set of strategies around modeling and inference. Further, we will continue to flesh out our account of how some of the pathologies of NHST have been a driver of the replication crisis. 

```{marginfigure, echo=TRUE}
<img src="images/inference/krushke.png"/>
Clarifying the distinctions between Bayesian and Frequentist paradigms and the ways that they approach inference and estimation. For many settings, we think the estimation mindset is more useful. From Kruschke and Liddell (2018). 
```

What should replace NHST? There has been a recent move towards the use of Bayes Factors to quantify the evidence in support of a hypothesis. Bayes Factors can help answer questions like (2). We introduce these tools, and believe that they have broader applicability than the NHST framework and should be known by students. Both NHST and Bayes Factors are examples of what are called **inference** strategies, which are centered around drawing conclusions from data: significant or not, hypothesis one or hypothesis two.^[An important but subtle point is that "inference" is an ambiguous term. We are using it in the sense of drawing conclusions from data. But the term is used more broadly as well in the phrase "statisical inference," which is often contrasted with purely descriptive approaches. In this usage, "inference" means making generalizations from data that extend beyond the current set of observations. We do want to do that!]

We contrast inference strategies with **estimation** and **modeling** strategies, which are more suited towards questions (3) and (4) [@kruschke2018]. The goal of these strategies is to yield a precise estimate of the relationships underlying observed variation in the data. Critically, one of these estimates is the causal effect of the experimental manipulation(s). That explains our affection for these approaches: if a good theory predicts these kinds of causal effects, it makes sense that we'd want to estimate them precisely! 

Estimating one quantity in isolation is not maximally effective, though, since often there will be variation in the estimate that has to do with other known sources. Imagine the Stroop effect, which has a fairly consistent effect on both fast and slow readers [@haaf2017]. But estimates of this effect will be more precise if we take into account that some readers are slower or faster, rather than just averaging across all this variation. That's why we need models that take into account different sources of variation.

This isn't a statistics book and we won't attempt to teach the full array of important statistical concepts that will allow students to build good models of a broad array of datasets. (Sorry!).^[If you're interested in going deeper, here are two books that have been really influential for us. The first is @gelman2006, which teaches regression and multi-level modeling from the persepective of data description and modeling. The second is @mcelreath2018. a course on building Bayesian models of the causal structure of your data. Honestly, neither is an easy book to sit down and read (unless you are the kind of person who reads stats books on the subway for fun) but both really reward detailed study. We encourage you to get together a reading group and go through the exercises together. It'll be well worth while in its impact on your statistical and scientific thinking.] But we do want you to be able to reason about inference and modeling. In this chapter, we'll start by looking at a case study that established modern statistical inference, using this example to build up intuitions about inference and estimation. Then in Chapter \@ref(models), we'll focus on the case where there is a manipulation (experimental treatment vs. control) and we want to estimate its effect. 

<!-- ::: {.case-study} -->
<!-- üî¨ Case study: The lady tasting tea -->
## The lady tasting tea

The birth of modern statistical inference came from a single, epochal act of mansplaining.^[An important piece of context for the work of Ronald Fisher, Karl Pearson, and other early pioneers of statistical inference is that they were all strong proponents of eugenics. Fisher was the founding Chairman of the Cambridge Eugenics Society. Pearson was perhaps even worse, an avowed social darwinist who believed fervently in Eugenic legislation. These views are repugnant.] Sir Ronald Fisher was apparently at a party when a lady declared that she could tell the difference between cups when the tea was added to the milk vs. the milk to the tea. Rather than taking her at her word, Fisher devised an experimental and data analysis procedure to test her claim.^[If you're interested in this history, we recommend @salsburg2001's delightful book, "The Lady Tasting Tea," about the origins of modern statistics.]

The basic schema of the experiment was that the lady would have to judge a set of new cups of tea and sort them into milk-first vs. tea-first sets. Her data would then be analyzed to determine whether her level of correct choice exceeded that expected by chance. While this process now sounds like a quotidian experiment that might be done on a cooking reality show, in fact this is one of those touchstones that feels unremarkable because it literally established the way science was done for the next century. 

The first element of the experiment that was unusual was its treatment of design confounds such as pouring order or cup material. Prior experimental practice would have been to try to equate all of the cups as closely as possible, decreasing the influence of confounders. Fisher recognized that this strategy was insufficient and that random assignment was critical for making strong causal inferences about the treatment (milk then tea vs. tea then milk).^[We discussed the causal power of random assignment in Chapter \@ref(intro) but here's where this idea originates!]

The second innovation was the idea of creating a model of what might happen during the experiment: specifically, a **null model** in which the nameless lady chose cups by chance rather than because of some tea sensitivity. The goal of Fisher's analysis strategy was then to compute the probability of the lady's choices under this null model. Fisher then declared that it is "usual and convenient for experimenters to take 5 percent as a standard level of convenience," establishing the .05 cutoff that has become gospel throughout the sciences.^[Actually, right after establishing .05 as a cutoff, Fisher then writes that "in the statistical sense, we thereby admit that no isolated experiment, however significant in itself, can suffice for the experimental demonstration of any natural phenomenon... in order to assert that a natural phenomenon is experimentally demonstrable we need, not an isolated record, but a reliable method of procedure. In relation to the test of significance, we may say that a phenomenon is experimentally demonstrable when we know how to conduct an experiment which will rarely fail to give us a statistically significant result." In other words, Fisher was all for replication!]

With apologies to Fisher, for our example, we'll design a slightly more modern version of his experiment using a forced-choice judgmeent measure. We present participants with a series of cups of tea (in random order, of course). Half are prepared milk-first, half tea-first. Then the participant must give an independent judgement about each. The null model is based on the idea that, if the participant knows nothing then they should choose at random between the two possibilities. Hence, we can use a simple binomial distribution to show expected chance performance for different sets of trials.^[We quantify our measure as the proportion of correct trials, since trials here are interchangeable.] 

These null distributions are shown in Figure \@ref(fig:inference-null-model). We show cases with 6, 12, 24, and 48 trials, and highlight the .05 significance threshold -- the threshold above which you would fall only 5% of the time if you were guessing by chance.^[Note that we are effectively doing a "one-tailed" test here, since we're attending only to the right hand side of each plot and ignoring cases where the participant is exactly flipped in their judgments.] Conventionally, if you cross this threshold, you can say you "reject the null."

```{r inference-null-model, fig.cap="Probability of observing a particular number of trials correct under the null model of no sensitivity to tea preparation order. Each facet gives a different number of trials. The red line indicates a significance threshold above which data have less than a 5\\% chance of appearing."}
n_sims <- 10000
null_model <- expand_grid(n = c(6,12,24,48),
                          sim = 1:n_sims) %>%
  group_by(n) %>%
  mutate(correct = rbinom(n = n_sims, size = n, prob = .5)/n, 
         cutoff = quantile(correct, .95)) %>%
  group_by(n, correct) %>%
  summarise(n_correct = n(), 
            cutoff = cutoff[1]) %>%
  group_by(n) %>%
  mutate(prop_correct = n_correct / sum(n_correct))

ggplot(null_model, aes(x = correct, y = prop_correct)) + 
  geom_bar(stat = "identity") + 
  facet_wrap(~n) + 
  geom_vline(aes(xintercept = cutoff + .01), col = "red", lty = 2) + 
  ylab("Probability") + xlab("Proportion correct responses")
# dbinom(x = 8, size = 8, prob = .5)
```

The more trials a participant completes, the lower the accuracy threshold for significance becomes. With 6 trials, you  need to get all `r qbinom(.95, size = 6, prob = .5, lower.tail = TRUE)` correct for that result to have a probability of less than 5% probable under the null model. With 12, you need to observe `r qbinom(.95, size = 12, prob = .5)+1` (`r round(qbinom(.95, size = 12, prob = .5)+1 / 12)`%) or more. With 24 and 48, you need  `r qbinom(.95, size = 24, prob = .5)+1` (`r round(qbinom(.95, size = 24, prob = .5)+1 / 24)`%) and  `r qbinom(.95, size = 48, prob = .5)+1` (`r round(qbinom(.95, size = 48, prob = .5)+1 / 48)`%) respectively.^[You can calculate these numbers by looking at what's called the **quantile function** of the binomial binomial: the function that tells you how many correct trials out of the total number corresponds to a particular probability.] With greater numbers of trials, the expectation based on chance becomes more precise and so we are able to reject then null based on smaller deviations. 

Even here in the original case of null hypothesis significance testing, it is clear that our goals matter. If we're merely testing a hypothesis informally at a garden party, perhaps we don't want to subject our friend to dozens of trials. But if the stakes were higher and a more precise estimate was necessary, maybe we'd want to do a more extensive experiment!

Something very important is missing from this analytic strategy. If we think back to our goals as experimentalists, we are hoping to use the logic of random assignment to measure causal effects of interest. Here, the effect we're measuring is something like the effect of tea preparation on taste perception (maybe not the most consequential question, but...). Yet our statistical paradigm is misaligned with this goal! All of this talk of significance and rejecting the null doesn't tell us about either how big the effect is, or how precisely we have measured it! That's why we are going to be advocating throughout for estimation as the key goal for our statistical analysis, with the varieties of inference (significance testing included) as a secondary goal. 
<!-- ::: -->

## A probabilistic framework 

An alternative way to frame our statistical practice is to start from the idea of estimation.

Let's say we want to estimate some quantity, we'll call it $h$ -- our belief about the participant's accuracy.^[You can't get away from measurement and psychometrics! We said we were really interested in the effect of tea ordering on tea perception. But this number that we're estimating is something more like this particular participant's accuracy, and that's not the same thing that we actually wanted. To get from what we're estimating to our effect of interest, we'd need to establish some **linking hypotheses** about how the participant's accuracy can be derived from the participant's perception and the properties of the stimulus. That's perhaps not worth doing in this toy example, but more generally it's a critical part of getting from your measure to your construct of interest!] We observe some data $d$, consisting of the set of correct and incorrect responses in the experiment. Now we can use **Bayes' rule**, a tool from basic probability theory, to estimate this number. 

Bayes' rule says:

$$
\color{purple}{p(h | d)} = \frac{\color{red}{p(d | h)} \color{blue}{p(h)}}{\color{black}{p(d)}}.
$$
\noindent Each part of this equation has a name, and it's worth becoming familiar with them. The thing we want to compute ($p(h|d)$) is called the **posterior probability** -- it tell us what we should believe about the participant's ability given the data we observed. We then break that down into two terms in the numerator.^[We're making the posterior `r colorize("purple","purple")` to indicate the combination of likelihood (`r colorize("red","red")`) and prior (`r colorize("blue","blue")`).]

The first part of the numerator is $p(d|h)$, the probability of the data we observed given our hypothesis about the participant's ability. This part is called the **likelihood**.^[Speaking informally, "likelihood" is just a synonym for probability, but this is a technical meaning for the term, which can get a bit confusing.] This term tells us about the relationship between our hypothesis and the data we observed -- so if we think the participant has high ability (say $h = .9$) then the probability of a bunch of low accuracy observations will be fairly low. 

The second term in the numerator, $p(h)$, is called the **prior**. This term encodes our beliefs about how likely our participant is to have different levels of ability. Intuitively, if we think that they are very unlikely to have high tea discrimination ability, we should require more evidence to convince us of a particular level of discrimination. In contrast, if we think they are likely to have this ability, we should be easier to convince. 

```{r inference-bayes-demo, fig.cap="Examples of Bayesian inference about tea discrimination ability under three different priors (facets). Blue lines give the prior probability distribution, red lines give the likelihood of the data, and purple lines give the posterior distribution from combining likelihood and prior."}
bayes <- expand_grid(x = seq(0,1,.01), a = c(1, 3, 6)) %>%
  mutate(b = case_when(a == 1 ~ 1, 
                       a == 3 ~ 3, 
                       a == 6 ~ 2)) %>%
  rowwise %>%
  mutate(h = dbeta(x = x, shape1 = a, shape2 = b),
         d_given_h = dbeta(x = x, shape1 = 9, shape2 = 3),
         h_given_d = dbeta(x = x, shape1 = a + 9, shape2 = b + 3)) %>%
  group_by(a,b) %>%
  mutate(h_norm = h / sum(h), 
         h_given_d_norm = h_given_d / sum(h_given_d), 
         d_given_h_norm = d_given_h / sum(d_given_h), 
         map = x[h_given_d == max(h_given_d)],
         label = case_when(a == 1 ~ "Flat",
                           a == 3 ~ "Prior: no discrimination",
                           a == 6 ~ "Prior: discrimination"), 
         label = fct_relevel(label, "Flat", "prior: no discrimination"))

ggplot(bayes, aes(x = x, y = h_norm)) + 
  geom_line(col = "blue") +
  geom_line(aes(y = h_given_d_norm), col = "purple") +
  geom_line(aes(y = d_given_h_norm), col = "red") +
  geom_vline(xintercept = .5, lty = 2) + 
  geom_vline(aes(xintercept = map), col = "purple", lty = 3) + 
  facet_wrap(~label) + 
  ylab("Normalized probability") +
  xlab("Discrimination level")
```

Figure \@ref(fig:inference-bayes-demo) gives an example of the combination of prior and data.^[The model we use for this example is called a **Beta-Binomial conjugate model** and is a very convenient model for working with count data representing successes and failures.] For the sake of this example, we assume that we have run 12 tea discrimination trials and observed 9 successes and 3 failures. The evidence alone -- with no prior -- suggests a discrimination estimate of $9/12 = .75$.^[Technically this is known as the **maximum a posteriori** estimate, or the MAP. We won't use that term though.] When we use a flat prior, we get the same estimate of `r bayes$map[bayes$label == "Flat"][1]`. In contrast, if we go in assuming that discrimination is likely to be absent or weak, we are biased downward in our eventual estimate of `r bayes$map[bayes$label == "Prior: no discrimination"][1]`; if we go in assuming good discrimination, we end up biased upwards to `r bayes$map[bayes$label == "Prior: discrimination"][1]`.

Fisher's original framwork for significance testing focused only on the **null hypothesis** of no discrimination. In contrast, the Bayesian estimation method here focuses on the magnitude of accuracy.^[If you're reading carefully, you might have noticed that we *could* have discovered that the estimate of accuracy was very similar to chance -- more about this below.]  The intuition we'd like you to get is that, if you are an experimentalist who cares about the magnitude of causal relationships (and we hope you are), then Fisher's statistical approach isn't ideally suited to your goals. 

<!-- TODO: HERE WOULD BE A GREAT PLACE FOR AN INTERACTIVE -->

## Inference

We just finished arguing that an estimation strategy is more aligned with the default goal of an experimentalist. But that's not always the case. Sometimes you do want to make an inference, in the sense of making some kind of rule-based decision about your data. The clearest case of this is in the context of an experiment that evaluates an intervention -- say a curriuclum manipulation to improve an educational outcome. In this kind of case, we might primarily want a clear answer: does this intervention "work" -- meaning, improve our outcome relative to control -- or not. In this kind of context, perhaps the magnitude of the effect is going to be so dependent on the specifics of implementation that we don't really have a lot of predictions about the magnitude. 

So, how do we decide whether our data support a positive conclusion? There are at least two different common ways to set up this kind of inference. One of these is Fisher's way: null hypothesis significance testing. The second is via a technique called the **Bayes Factor**. Since we've already talked about NHST a bit, we'll introduce the Bayes Factor first. Then we can see how NHST relates. 

### Bayes Factors 

Bayes Factors are a method for quantifying the support for one hypothesis over another, based on an observed dataset. Informally, we've now talked about two different distinct hypotheses about the tea situation: our participant could have *no* tea discrimination ability -- leading to chance performance. We call this $H_0$. Or they could have some non-zero ability -- leading to greater than chance perfomance. We call this $H_1$. The Bayes Factor is simply the likelihood of the data (in the technical sense used above) under $H_1$ vs. under $H_0$:

$$
BF = \frac{\color{red}{p(d | H_1)}}{\color{red}{p(d | H_0)}}.
$$
The Bayes Factor is a ratio, so if it is greater than 1, the data are more likely under $H_1$ than they are under $H_0$ -- and vice versa for values between 1 and 0. A BF of 3 means there is three times as much evidence for $H_1$ than $H_01$, or equivalently 1/3 as much evidence for $H_0$ as $H_1$.^[Sometimes people refer to the BF in favor of $H_1$ as the $BF_{10}$ and the BF in favor of $H_0$ as the $BF_{01}$. This notation strikes us as a bit confusing because a reader might wonder what the 10 in the subscript means.] There are no hard and fast rules for Bayes Factor interpretation, but many people follow guidelines from @jeffreys1998, shown in the margin. 

```{r jeffreys, fig.margin=TRUE}
jeffreys <- tribble(~`BF range`, ~`Interpretation`,
                    "< 1", "Negative evidence (supports H0)",
                    "1 -- 3", "Barely worth mentioning",
                    "3 -- 10", "Substantial",
                    "10 -- 30", "Strong",
                    "30 -- 100", "Very strong",
                    "> 100", "Decisive")

kable(jeffreys, booktabs = TRUE, 
      caption = "Jeffreys' (1961/1998) interpretation guidelines for Bayes Factors.")
```
There are a couple of things to notice about the Bayes Factor. The first is that it is a continous measure. You can dichotomize decisions based on the Bayes Factor by declaring a cutoff (say, BF > 3 or BF > 10), there is no intrinsic threshold at which you would say the evidence is "significant."^[Of course this is true for $p$-values too, but the $p < .05$ criterion is so culturally ingrained in most scientists that it's hard to get away from.] Second, it doesn't depend on our prior probability of $H_1$ vs. $H_0$. We might think of $H_1$ as very implausible. But the BF is independent of that prior belief. So that means it's a measure of how much the evidence should shift our beliefs away from our prior. One nice way to think about this is that the Bayes Factor computes how much our beliefs -- whatever they are -- should be changed by the data [@morey2011]. 

In practice, the thing that is both tricky and good about Bayes Factors is that you need to define an actual model of what $H_0$ and $H_1$ are. That process involves making some assumptions explicit. We won't go into how to make these models here -- this is a big topic that is covered extensively in books on Bayesian data analysis.^[Two good ones are @gelman1995 (a bit more statistical) and @kruschke2014 (a bit more focused on psychological data analysis). An [in-prep web-book by Nicenboim et al.](https://vasishth.github.io/bayescogsci/book/) also looks great.] Below and in Chapter \@ref(models) we will provide some guidance for how to compute Bayes Factors for simple experimental designs. The goal here is just to give a sense of how they work. 

To continue our tea-tasting example, let's continue to assume our participant gets 9 of 12 trials right. Now to compute a Bayes Factor, we will have to define our hypotheses. Defining the null is pretty simple -- it should just be that choices are random coin flips. But to define our $H_1$ things get a bit more complex. We could just assume that the participant should get 75% percent of trials correct, but this could be a problematic choice! For example, what if our participant got 550 / 1000 trials correct? That would give us a strong sense that they had some tea ability, but would actually be more consistent with the $H_0$ of 50% accuracy than the $H_1$ of 75% accuracy! In practice, what we need to do is define a distribution over possible accuracies and then average across these -- so we assign some probability to 75% accuracy but also some probability to 55% accuracy and even 50% accuracy.^[You might wonder how we can compare a $H_0$ that assumes 50% accuracy and a $H_1$ that *includes* 50% accuracy in it as well. The key here is that $H_1$ is more complex because it includes a range of values.]. The details don't matter here, but just for simplicity we adopt the discrimination prior shown in Figure \@ref(fig:inference-bayes-demo). 


```{r inference-tea-tasting-bf}
compute_tea_bf <- function(h, n, a = 6, b = 2) {
  theta <- seq(.1,.99,.01)
  
  h0_likelihood <- dbinom(h, n, rep(.5, length(theta)))
  h1_likelihood <- dbinom(h, n, theta)
  
  likelihood_ratio <- h1_likelihood / h0_likelihood
  
  h1_prior_weights <- dbeta(theta, shape1 = a, shape2 = b)
  h1_prior_weights <- h1_prior_weights / sum(h1_prior_weights)
  
  BF <- sum(h1_prior_weights * likelihood_ratio)
}

BF_12trials <- compute_tea_bf(9, 12) 
BF_24trials <- compute_tea_bf(18, 24)
```

Once we set up our models, we can compute the relative likelihood of the data under each. The resulting BF is `r round(BF_12trials,2)`, somewhere between anecdotal and substantial evidence. On the other hand, if we doubled the size of the experiment and found the same pattern -- 18/24 trials correct -- the BF goes up to `r round(BF_24trials,2)`, which we could classify as strong evidence.

### *p*-values

We already have a working definition of what a $p$-value is from our discussion above: it's the **probability of the data (or any data that would be more extreme) under the null hypothesis**. How is this quantity related to either our Bayesian estimate or the BF? Well, the first thing to notice is that the $p$-value is very close (but not indentical to) a Bayesian likelihood, which is the probability of the data but doesn't include any more extreme data. So for example, the probability of getting 9 out of 12 tea-tasting trials correct under the null hypothesis is `r round(dbinom(9,12,.5),2)`, meaning there's a `r round(dbinom(9,12,.5),2)*100`% chance of observing exactly this many successes. But the p-value is given by the summed probability of 9, 10, 11, *and* 12 successes, which is `r round(sum(dbinom(9:12,12,.5)),2)`. In Table \@ref(tab:p-bf-comparison), you can see how $p$-values (from a one-tailed exact binomial test) compare to our Bayes Factors. In general BFs tend to be a bit more conservative than $p$-values, such that $p<.05$ generally translates to a BF of 3 or less [@benjamin2018]. 

```{r p-bf-comparison, fig.margin=TRUE }
tibble(`Successes` = c(3,6,12,9,18,36,55,550),
       `Total trials` = c(3,6,12,12,24,48,100,1000)) %>%
  rowwise %>%
  mutate(`% accuracy` = `Successes`/`Total trials`,
         `p value` = binom.test(`Successes`,`Total trials`,alternative = "greater")$p.value,
         BF = compute_tea_bf(`Successes`, `Total trials`)) %>%
  kable(booktabs = TRUE, digits = 3, caption = "Comparison of p-value and BF for several different tea-tasting scenarios. p-values are one-tailed. Note that BF is heavily dependent on the specific prior on tasting successes that we assign.")
```

The critical thing about $p$-values is not just that they are a specific kind of data likelihoods, it is that they are used in a specific inferential procedure The logic of NHST is that we make a binary decision about the presence of an effect. If $p < .05$, the null hypothesis is rejected; otherwise not. As @fisher1949 wrote, 

> It should be noted that the null hypothesis is never proved or established, but is possibly disproved, in the course of experimentation. Every experiment may be said to exist only in order to give the facts a chance of disproving the null hypothesis. (p. 19)

This logic relates to the falsificationist logic of Popper that we touched on briefly in \@ref(intro). Under NHST, null hypotheses can be rejected (essentially, falsified) but positive hypotheses cannot be supported. As we said then, this logic doesn't appear either optimal from a normative statistical -- and as we'll discuss below, it doesn't really fit with a descriptive view of how scientists (and people more generally) reason about their hypotheses. 

```{r inference-power-alpha, fig.cap="Standard decision matrix for NHST.", fig.margin=TRUE}
knitr::include_graphics("images/inference/power-alpha.png")
## TODO - RIGHTS ISSUE, NEEDS REMAKING
```

One way to "patch" NHST is to introduce a decision-theoretic view, shown in Figure \@ref(fig:inference-power-alpha).^[A little bit of useful history here is given in @cohen1990.] On this view, there is a real $H_1$, albeit one that is not specified. Then the true state of the world could be that $H_0$ is true or $H_1$ is true. The $p<.05$ criterion is the threshold at which we are willing to reject the null, and so this constitutes our **false positive** rate $\alpha$. But we also need to define a **false negative** rate, which is conventionally (and unhelpfully) called $\beta$. Setting these rates is a decision problem: If you are too conservative in your criteria for the intervention having an effect, however, then you risk a false negative, where you incorrectly conclude that it doesn't work. And if you're too liberal in your assessment of the evidence, then you riks a false positive.^[To make really rational decisions, you could couple this chart to some kind of utility function that assessed the costs of different outcomes. For example, you might think it's worse to proceed with an intervention that doesn't work than to stay with business as usual. In that case, you'd assign a higher cost to a false positive and accordingly try to adopt a more conservative criterion. We won't cover this kind of decision analysis here, but @pratt1995 is a classic textbook on statistical decision theory if you're interested.] In practice, however, mostly people leave $\alpha$ at .05 and try to control their false negative rate by increasing their sample size. As we saw in Figure \@ref(fig:inference-null-model), the greater the sample, the better your chance of rejecting the null for any given non-null effect. This model gives rise to the idea of classical power analysis, which we cover in Chapter \@ref(sampling). 

<!-- TODO: check this book rec -->


<!-- https://commons.wikimedia.org/wiki/File:ConfusionMatrixRedBlue.png -- CC -->


## Inference fallacies

$p$-values are basically likelihoods. The likelihood of the data under the null hypothesis is a critical number to know -- for computing the Bayes Factor among other reasoms. But it doesn't tell us a lot of things that we might like to know! For example, it doesn't tell us the probability of the data under any positive hypothesis that we might be interested in -- that's the posterior probability $p(H_1 | d)$. Lots of folks mistake these two for one another, e.g. that the probability of the data given the null is the probability of the null given the data. 

Another $p$-value problem that comes up frequently is what to conclude when $p>.05$. According to the classical logic of NHST, the answer is "nothing"! That is, a failure to reject the null does not mean that you can *accept* the null. Even if the probability of the data (or some more extreme data) under $H_0$ is high, their probability might be just as high or higher under $H_1$.^[Of course, weighing these two against one another brings you back to the Bayes Factor.] But many practicing researchers make this mistake. @aczel2018 coded a sample of articles from 2015 and found that 72% of negative statements were inconsistent with the logic of their statistical paradigm of choice -- most were cases where researchers said that an effect was not present when they had simply failed to reject the null. 

These are not the only issues with $p$-values. In fact, people have so much trouble understanding what $p$-values *do* say that there are whole articles written about these misconceptions. Table \@ref(tab:dirty-dozen) shows a set of misconceptions documented and refuted by @goodman2008. 

```{r dirty-dozen}
dozen <- tribble(~` `, ~Misconception,
                 1,	"If *p* = .05, the null hypothesis has only a 5% chance of being true.",
                 2,	"A nonsignificant difference (eg, *p* ‚â•.05) means there is no difference between groups.",
                 3,	"A statistically significant finding is clinically important.",
                 4,	"Studies with *p* values on opposite sides of .05 are conflicting.",
                 5,	"Studies with the same *p* value provide the same evidence against the null hypothesis.",
                 6,	"*p* = .05 means that we have observed data that would occur only 5% of the time under the null hypothesis.",
                 7,	"*p* = .05 and *p* ‚â§.05 mean the same thing.",
                 8,	"*p* values are properly written as inequalities (eg, \\'*p* ‚â§.02\\' when *p* = .015)",
                 9,	"*p* = .05 means that if you reject the null hypothesis, the probability of a false positive error is only 5%.",
                 10,	"With a *p* = .05 threshold for significance, the chance of a false positive error will be 5%.",
                 11,	"You should use a one-sided *p* value when you don't care about a result in one direction, or a difference in that direction is impossible.",
                 12,	"A scientific conclusion or treatment policy should be based on whether or not the *p* value is significant.")

knitr::kable(dozen, 
             booktabs = TRUE, 
             caption = "A \"dirty dozen\" *p* value misconceptions. Adapted from Goodman (2008).")
```

Further, the $p$-value is a probability of a certain set of events happening (corresponding to the observed data or any "more extreme" data, that is to say, data further from the null). Since $p$-values are probabilities, we can combine them together across different events. If we run a null experiment -- an experiment where we know the expected effect is zero -- the probability of a dataset with $p < .05$ is of course .05. But if we run two such experiments, we can get $p < .05$ with probability `r round(2*.05 - .05*.05,2)`. By the time we run 20 experiments, we have an `r round(1 - .95^20,2)` chance of getting a positive result. 

It would obviously be a major mistake to run 20 null experiments and then report only the positive ones (which, by design, are false positives) as though these still were "statistically significant." The same thing applies to doing 20 different statistical tests within a single experiment. There are many statistical corrections that can be made to adjust for this problem.^[The simplest one, the Bonferroni correction, just divides .05 (or technically, whatever your threshold is) by the number of comparisons you are making. Using that correction, if you do 20 null experiments, you would have a `r round((1 - .95^20)/20)` chance of a false positive, which is actually a little conservative.] But the the broader issue is one of transparency: unless you *know* what the appropriate set of experiments or tests is, it's not possible to implement one of these corrections!^[This issue is especially problematic with $p$-values because they are so often presented as an independent set of tests, but the problem of multiple comparisons comes up when you compute a lot of independent Bayes Factors as well. "Posterior hacking" via selective reporting of Bayes Factors is perfectly possible [@simonsohn2014].]

<!-- P goes to 0 as data goes to infinity. -->

::: {.accident-report}
‚ö†Ô∏è Accident report: Do extraordinary claims require extraordinary evidence? 

In a blockbuster paper that inadvertently kicked off the replication crisis, @bem2011 presented nine experiments showing evidence for precognition -- that is, sensing the future. In the first of these experiments, Bem showed each of a group of 100 undergraduates 36 two-alternative forced choice trials in which they had to guess which of two locations on a screen would reveal a picture immediately before the picture was revealed. By chance, participants should choose the correct side 50% of the time of course. Bem found that, specifically for erotic pictures, participants' guesses were 53.1% correct. This rate of guessing was unexpected under the null hypothesis of chance guessing ($p = .01$). Eight other studies with a total of more than 1,000 participants yielded apparently supportive evidence, with particpants appearing to show a variety of psychological effects even before the stimuli were shown! On this basis, should we conclude that precognition exists? 

Probably not. @wagenmakers2011 gave an influential critique of Bem's findings, arguing that 1) Bem's experiments were exploratory in nature, 2) that Bem's conclusions were a priori unlikely, and 3) that the level of statistical evidence from his experiments was quite low. We find each of these arguments alone compelling; together they present a knockdown case. 

First, we've already discussed the need to be skeptical about situations where experimenters have the opportunity for analytic flexibility in their choice of measures, manipulations, samples, and analyses. Flexibility leads to the possibility of cherry-picking those set of decisions from the "garden of forking paths" that lead to a positive outcome for the researcher's favored hypothesis. And there is plenty of flexibility on display even in Experiment 1 of Bem's paper. Although there were 100 participants in the study, they may have been combined post hoc from two distinct samples of 40 and 60, each of which saw different conditions. The 40 made guesses about the location of erotic, negative, and neutral pictures; the 60 saw erotic, positive non-romantic, and positive romantic pictures. The means of each of these conditions was presumably tested against chance (at least 6 comparisons, for a false positive rate of `r round(1 - .95^6,2)`), and had positive romantic pictures been found significant, an interpretation would have been available about this condition. 

Second, as we discussed, a $p$-value close to .05 does not necessarily provide strong evidence against the null hypothesis. Wagenmakers et al. computed the Bayes Factor for each of experiments in Bem's paper and found that, in many cases, the amount of evidence for $H_1$ was quite modest under a default Bayesian $t$-test. Experiment 1 was no exception: the BF was `r round(1/.61,2) # they reported bf_01`, giving "anecotal" support for the hypothesis of some non-zero effect, even before the multiple-comparisons problem mentioned above. 

Finally, since precognition is not attested by any compelling prior scientific evidence -- and many researchers have tried to provide this evidence -- perhaps we should assign a low prior probability to Bem's $H_1$, a non-zero precognition effect. Taking a strong Bayesian position, Wagenmakers et al. suggest that we might do well to adopt a prior reflecting how unlikely precognition is, say $p(H_1) = 10^{-20}$. And if we adopt this prior, even a very well-designed, highly informative experiment (with a Bayes factor conveying substantial or even decisive evidence) would still lead to a very low posterior probability of precognition. 

Wagenmakers et al. concluded that, rather than suppporting precognition, the conclusion from Bem's paper should be psychologists should revise how they think about analyzing their data!^[If you are intrigued by this set of issues, you might enjoy Slate Star Codex's post ["The Control Group is Out of Control"](https://slatestarcodex.com/2014/04/28/the-control-group-is-out-of-control/) -- we don't agree with everything in it, but it's definitely thought-provoking and contains many interesting links.]
:::

### Philosophical (and empirical) views of probability

Up until now we've presented Bayesian and frequentist tools as two different sets of computations. But in fact, these different tools derive from fundamentally different interpretations of what a probability even is. Very roughly, frequentist approaches tend to believe that probabilities quantify the long-run frequencies of certain events. So, if we say that some outcome of an event has probability .5, we're saying that if that event happened thousands of times, the long run frequency of the outcome would be 50% of the total events. In contrast, the Bayesian viewpoint doesn't depend on this sense that events could be exactly repeated. Instead, the **subjective Bayesian** interpretation of probability is that it quantifies the person's degree of belief in a particular outcome.^[This is really a very rough description. If you're interested in learning more about this philosophical background, we recommend the Stanford Encyclopedia of Philosophy entry on "interpretations of probability": https://plato.stanford.edu/entries/probability-interpret/.]

You don't have to take sides in this deep philosophical debate about what probability is. But it's helpful to know that people actually seem to reason about the world in ways that are well described by the subjective Bayesian view of probability. Recent cognitive science research has made a lot of headway in describing reasoning as a process of Bayesian inference (for review, see @probmods2) where probabilities describe degrees of belief in different hypotheses. These hypotheses in turn are a lot like the theories we described in Chapter \@ref(intro): they describe ways that different abstract entities connect with one another [@tenenbaum2011]. You might think that scientists are different from lay-people in this regard, but one of the striking findings from the literature on probabilistic reasoning and judgment is that expertise doesn't matter that much. Statistically-trained scientists make many of the same reasoning mistakes as their un-trained students [@kahneman1979]. Even children seem to reason intuitively in a way that looks a bit like creating probabilistic models [@gopnik2012]. 

These cognitive science findings help to explain some of the problems that people (scientists included) have reasoning about $p$-values. If you are an intuitively Bayesian reasoner, the quantity that you're probably tracking is how much you believe in your hypothesis (its posterior probability). So, many people treat the $p$-value as the posterior probability of the null hypothesis.^[@cohen1994 is a great treatment of this issue.] That's exactly what fallacy #1 -- "If *p* = .05, the null hypothesis has only a 5% chance of being true." -- states. It's not. Written in math, $p(d | H_0)$ (the likelihood that lets us compute the p-value) is not the same thing as $p(H_0 | d)$ (the posterior that we want). Pulling from our accident report above, even if the probability of the data given the null hypothesis of ESP is low, that doesn't mean that the probability of ESP is high.


## Summary: When inference is and isn't appropriate

Inferences like the ones we've discussed here are important for projects where there is a key binary hypothesis being tested. Did the intervention produce an effect? Were the two groups different from one another? These are questions that inference methods help us answer. But they are also only a subset of the interesting questions to be asked about an experiment.

More generally, the problem with inference as a method for thinking about experimental effects is that it creates a fragile scientific ecosystem. By the logic of statistical significance, either an experiment "worked" or it didn't. Because everyone would usually rather have an experiment that worked than one that didn't, inference criteria like p-values often become a target for selection, as we discussed in Chapter \@ref(replication).^[More generally, this is probably an example of Goodhart's law, which states that when a measure becomes a target, it ceases to be a good measure [@strathern1997]. Once the outcomes of statistical inference procedures become targets for publication, they are subject to selection biases that make them less meaningful.]

<!-- So that means we get phenomena like $p$-hacking, where  -->

<!-- Inference strategies -- especially $p$-values but Bayes Factors as well to a lesser extent --  -->
<!-- : because they are the single (often binary -->

<!-- First discussion of overfitting. See Chapter \@ref(prereg).  -->

We made the case here that, if you want to make inferences, Bayes Factors tend to make a bit more sense than $p$-values. In practice, $p$-values are hard to understand and many people misuse them. Despite the reasons to be worried about p-values, for many practicing scientists (at least at time of writing) there is no one right answer as to whether to reject $p$-values. Even if we'd like to be Bayesian all the time, there are a number of obstacles. First, though new computational tools make fitting Bayesian models and extracting Bayes Factors much easier than before (more about this below and in the next chapter), it's still on average quite a bit harder to fit a Bayesian model than it is a frequentist one. Second, because Bayesian analyses are less familiar, it may be an uphill battle to convince advisors, reviewers, and funders to use them. So as a group, we are still mostly Bayesian when we can be -- and frequentist when it's not practical. But one reason we don't feel so bad about this stance is that, a lot of them time we're not so worried about making binary inferences, whether they are at $p < .05$ or $BF > 3$ or whatever the threshold is. 

More generally, we hope we've convinced you that inference -- in the sense of trying to derive a particular decision on the basis of your data -- isn't the only strategy. We think that the estimation strategy is often much more helpful as a building block for theory. Towards this goal, the next chapter provides some tools for how to estimate causal effects from simple experimental manipulations. 

::: {.exercise}
Exercises:

1. Step away from the computer. Can you write the definition of a $p$-value and a Bayes Factor? 
2. Take three of Goodman's (2008) "dirty dozen" in Table \@ref(tab:dirty-dozen) and write a description of why each is a misconception. (These can be checked against the original article, which gives a nice discussion). 
::: 




<!-- Philosophy -->
<!-- Bayes: Data are the data, what can we infer? (subjective) -->
<!-- Frequentist: Do the same thing again and again, what happens (objective truth) -->
<!-- Prior information -->
<!-- Bayes theorem -->
<!-- ‚ÄúData fixed‚Äù vs. ‚Äúmodel fixed‚Äù -->

<!-- Bayesians don‚Äôt have to care about optional stopping -->

<!-- Differences in philosophy -->
<!-- Bayesian stats are good for: -->
<!-- Cases where you have a lot of prior knowledge -->
<!-- E.g., statistics for voter prediction in elections -->
<!-- Cases where you want to deal with uncertainty in a principled way -->
<!-- E.g., integrating knowledge about measurement error into your model -->
<!-- Null models -->
<!-- Cases where you want to decide in favor of null -->
<!-- (though you can do this in frequentist perspectives) -->
<!-- Small N data -->
<!-- Many frequentist tests are based on guarantees in the limit. Guarantees don‚Äôt hold at N=3 or 5 or even 10.  -->
<!-- Frequentist good for: -->
<!-- Decision theory (cf. bayesian decision theory) -->
<!-- Absence of assumptions (‚Äúmore objective‚Äù) -->
<!-- Often easier to state and solve models -->

<!--chapter:end:006-inference.Rmd-->


# Statistical models {#models} 

Placeholder


## Inference and estimation for two-group designs
### Simple models of between-group differences
## Effect size

<!--chapter:end:007-models.Rmd-->


# Sampling

Placeholder



<!--chapter:end:008-sampling.Rmd-->


# Preregistration

Placeholder



<!--chapter:end:009-prereg.Rmd-->


# (PART) Execution {-}
# Replicating or extending an existing study {#selection}

Placeholder



<!--chapter:end:010-selection.Rmd-->


# Data collection {#collection}

Placeholder


## The critical importance of piloting
## Data collection online
## Manipulation checks
## Experimental practices, beliefs, and superstitions
## Documenting your data collection

<!--chapter:end:011-collection.Rmd-->


# Data management {#management}

Placeholder



<!--chapter:end:012-manangement.Rmd-->


# (PART) Analysis and Reporting {-}
# Visualization {#viz}

Placeholder


## Basic principles of visualization
### Show the design
### Facilitate comparison
### Maximize information, minimize ink
### ``Fix the axis labels''
## Exploratory visualization

<!--chapter:end:013-viz.Rmd-->

```{r include=FALSE, cache=FALSE}
library(tidyverse)
library(glue)
library(knitr)
library(shiny)

opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  error = FALSE,
  comment = "#>",
  collapse = TRUE,
  cache = TRUE,
  echo = FALSE,
  cache.lazy = FALSE,
  dev = "png",
  dpi = 300,
  out.width = "\\linewidth"
)
kable <- function(...) knitr::kable(..., booktabs = TRUE, linesep = "")

set.seed(42)

.font <- "Source Sans Pro"
theme_set(theme_bw(base_size = 14, base_family = .font))
theme_update(panel.grid = ggplot2::element_blank(),
             strip.background = ggplot2::element_blank(),
             legend.key = ggplot2::element_blank(),
             panel.border = ggplot2::element_blank(),
             axis.line = ggplot2::element_line(),
             strip.text = ggplot2::element_text(face = "bold"))
.grey <- "grey70"
.refline <- "dotted"
.coef_line <- element_line(colour = .grey, size = 0.1)

.pal <- ggthemes::ptol_pal
.scale_colour_discrete <- ggthemes::scale_colour_ptol
.scale_color_discrete <- .scale_colour_discrete
.scale_fill_discrete <- ggthemes::scale_fill_ptol

.scale_colour_continuous <- viridis::scale_colour_viridis
.scale_color_continuous <- .scale_colour_continuous
.scale_fill_continuous <- viridis::scale_fill_viridis

.scale_colour_numerous <- scale_colour_discrete
.scale_color_numerous <- .scale_colour_numerous
.scale_fill_numerous <- scale_fill_discrete

# from https://bookdown.org/yihui/rmarkdown-cookbook/font-color.html
colorize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
            x)
  } else x
}
```
# Exploratory data analysis {#eda}

::: {.learning-goals}
üçé Learning goals: Explain how to evaluate the sensitivity of main analyses to modeling assumptions, describe how modeling and data visualization support data analysis, compare individual and item differences. 
:::

::: {.case-study}
üî¨ Case study: Frank, Vul, & Johnson (2009) report a small subset of the many many analyses they ran on a very rich dataset. Luckily, the simplest analysis was the most important, and that one was replicable (Frank, Amso, and Johnson 2014).
:::

Exploring the sensitivity of your main analyses: covariates and exclusions

- Sensitivity analysis for parameters

::: {.interactive}
‚å®Ô∏è Interactive box: multi-verse analysis (effects of alternative choices)
:::

Exploratory modeling and model+data visualization: a continuous loop

- How to constrain and document exploratory analysis
- Devil and the deep blue sea post (Danielle Navarro)

Fancy stuff and when it is and isn‚Äôt appropriate

- Dimensionality reduction: factor analysis and PCA (when these techniques are useful, and roughly what they do; no mathematical exploration). 
- Using cognitive models as predictors: an exciting way of making quantitative links between theory and data.
- Exploiting variation: individual differences and item differences. (Some discussion of psychometric models like item response theory as a way of exploring this variation). 

<!--chapter:end:014-eda.Rmd-->


# Reproducible writing {#writing}

Placeholder



<!--chapter:end:015-writing.Rmd-->


# Meta-analysis {#meta}

Placeholder


## Meta-notes
### Undefined notation and terminology
### Knowledge prerequisites
### Topics that should probably be covered
## Introduction?
## Running example: The contact hypothesis
## Intuitive, but problematic, approaches to evidence synthesis
## Fixed-effects meta-analysis
### Applied example
### Limitations of fixed-effects meta-analysis
## Random-effects meta-analysis
### Reporting on heterogeneity
### Applied example
## Bias in meta-analysis
### Within-study biases
### Across-study biases
## Summary

<!--chapter:end:016-meta.Rmd-->

```{r include=FALSE, cache=FALSE}
library(tidyverse)
library(glue)
library(knitr)
library(shiny)

opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  error = FALSE,
  comment = "#>",
  collapse = TRUE,
  cache = TRUE,
  echo = FALSE,
  cache.lazy = FALSE,
  dev = "png",
  dpi = 300,
  out.width = "\\linewidth"
)
kable <- function(...) knitr::kable(..., booktabs = TRUE, linesep = "")

set.seed(42)

.font <- "Source Sans Pro"
theme_set(theme_bw(base_size = 14, base_family = .font))
theme_update(panel.grid = ggplot2::element_blank(),
             strip.background = ggplot2::element_blank(),
             legend.key = ggplot2::element_blank(),
             panel.border = ggplot2::element_blank(),
             axis.line = ggplot2::element_line(),
             strip.text = ggplot2::element_text(face = "bold"))
.grey <- "grey70"
.refline <- "dotted"
.coef_line <- element_line(colour = .grey, size = 0.1)

.pal <- ggthemes::ptol_pal
.scale_colour_discrete <- ggthemes::scale_colour_ptol
.scale_color_discrete <- .scale_colour_discrete
.scale_fill_discrete <- ggthemes::scale_fill_ptol

.scale_colour_continuous <- viridis::scale_colour_viridis
.scale_color_continuous <- .scale_colour_continuous
.scale_fill_continuous <- viridis::scale_fill_viridis

.scale_colour_numerous <- scale_colour_discrete
.scale_color_numerous <- .scale_colour_numerous
.scale_fill_numerous <- scale_fill_discrete

# from https://bookdown.org/yihui/rmarkdown-cookbook/font-color.html
colorize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
            x)
  } else x
}
```
# Conclusions {#conclusions}

::: {.learning-goals}
üçé Learning goals: Synthesize and extend lessons from previous chapters.
:::

::: {.case-study}
üî¨ Case study: How to run a study that doesn‚Äôt replicate, and how to design a study that doesn‚Äôt contribute to our knowledge. We report new empirical analysis of aggregate data from 10 years of Psych 251 (>100 replication projects), discussing empirical predictors of (non-)replicability
:::

Revisiting themes of precision, transparency, generalizability, bias-reduction

- Discuss the ‚Äúcrisis‚Äù narrative and reframe as one way into seeing the set of interlocking issues that keep behavioral research from contributing to cumulative theory.
- Collaboration as a way forward. Spotlight the Psychological Science Accelerator (Moshontz et al. 2018).

Concrete suggestions for building a cumulative research program

- This was a big book, and incorporating all of this advice into your research program may seem difficult or overwhelming.
- Start simple, and repeat: for building an empirical research program, you want to measure one thing well and then build on that measurement. 
- Internal replication as a critical way to check your own work. 
- Cumulativity as a key principle that you can model in your own work. 

<!--chapter:end:017-conclusions.Rmd-->


# (PART) Appendices {-}
# Github Tutorial {#github}

Placeholder


## Introduction
## Git ‚â† GitHub
## Terminology
## Step 1: Review basic terminal commands
## Step 2: Install git
### Did you successfully install?
### Other versions
### Set your name and email address
## Step 3: Make a repo on GitHub, clone it to your computer
## Step 4: Make some commits
### Update your README file
### Commit changes to git
### Add another file to the repo + commit it 
## Step 5: Push your changes to GitHub
## Step 6: Make more changes to the repo
## Step 7: Rolling back to previous versions
## Step 8: What not to put on git
## Further Resources

<!--chapter:end:100-github.Rmd-->


# R Markdown Tutorial {#rmarkdown}

Placeholder


## Introduction
### Who is this aimed at?
### Why write reproducible papers?
### Learning goals
## Getting Started
## Structure of an RMarkdown file
### Header
### Body text
### Code chunks
## Markdown syntax
### Exercises
##  Headers, Tables, and Graphs
### Headers 
### Graphs
### Tables
### Statistics
### Exercises
## Collaboration 
## Writing APA-format papers
### Software requirements
### Installing `papaja`
### Creating a document
### Bibiographic management
### Exercise

<!--chapter:end:101-rmarkdown.Rmd-->


# Tidyverse Tutorial {#tidyverse}

Placeholder


## Goals and Introduction
### Data frames
### Tidy data
### Functions and Pipes
### `ggplot2` and tidy data
## Tidy Data Analysis with `dplyr`
### Exploring and characterizing the dataset
### Filtering & Mutating
### Standard psychological descriptives
## Getting to Tidy with `tidyr`
### Tidy verbs
## A bigger worked example: Wordbank data
## Exciting stuff you can do with this workflow
### Reading bigger files, faster
### Interactive visualization
### Function application 
## Exercise solutions

<!--chapter:end:102-tidyverse.Rmd-->


# ggplot Tutorial {#ggplot}

Placeholder


## Exploring ggplot2 using `qplot`
## More complex with `ggplot`
## Facets
## Geoms
## Themes and plot cleanup
## Plot inheritance example
## Advanced plot exploration exercise
### Variable exploration
### Hypothesis-related exploration

<!--chapter:end:103-ggplot.Rmd-->

```{r include=FALSE, cache=FALSE}
library(tidyverse)
library(glue)
library(knitr)
library(shiny)

opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  error = FALSE,
  comment = "#>",
  collapse = TRUE,
  cache = TRUE,
  echo = FALSE,
  cache.lazy = FALSE,
  dev = "png",
  dpi = 300,
  out.width = "\\linewidth"
)
kable <- function(...) knitr::kable(..., booktabs = TRUE, linesep = "")

set.seed(42)

.font <- "Source Sans Pro"
theme_set(theme_bw(base_size = 14, base_family = .font))
theme_update(panel.grid = ggplot2::element_blank(),
             strip.background = ggplot2::element_blank(),
             legend.key = ggplot2::element_blank(),
             panel.border = ggplot2::element_blank(),
             axis.line = ggplot2::element_line(),
             strip.text = ggplot2::element_text(face = "bold"))
.grey <- "grey70"
.refline <- "dotted"
.coef_line <- element_line(colour = .grey, size = 0.1)

.pal <- ggthemes::ptol_pal
.scale_colour_discrete <- ggthemes::scale_colour_ptol
.scale_color_discrete <- .scale_colour_discrete
.scale_fill_discrete <- ggthemes::scale_fill_ptol

.scale_colour_continuous <- viridis::scale_colour_viridis
.scale_color_continuous <- .scale_colour_continuous
.scale_fill_continuous <- viridis::scale_fill_viridis

.scale_colour_numerous <- scale_colour_discrete
.scale_color_numerous <- .scale_colour_numerous
.scale_fill_numerous <- scale_fill_discrete

# from https://bookdown.org/yihui/rmarkdown-cookbook/font-color.html
colorize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
            x)
  } else x
}
```
# Instructor's Guide {#instructors}


## Introduction 

This is an instructor‚Äôs guide to conducting replication projects in courses. 

Possible motivations for structuring a course around replication projects include: 

* **Student interest**: Allows application of course methods and lessons to a topic that is interesting to each student.
* **Usefulness**: If this course is taught in the first year of the program (as recommended), students may use their project as a way to establish. robustness of a phenomenon before building studies on top of it
* **Realism**: Real data is often messier than practice datasets typically provided for course exercises.
* **Intuition**: Final presentation of students‚Äô replication outcomes trains students to read the literature with a better ‚Äúsense‚Äù of which studies may be more likely to replicate than others. Frustrating experiences can motivate students to adopt best practices for their own future studies.
* **Logistics**: we provide some discussion of course budgets as well as sample text for course-related Institutional Review Board applications and a sample syllabus for a 10-week and 16-week version of the course. 


## Student Assignments

### Problem Set 0: Git and Github

* Points: 5 points
* Instructions: "Submit a link to the Github repository you created. In order to receive credit, the repository must have an appropriately-formatted README file and another file with one successfully-pushed change in its history. This shows you have pushed a change in a second file to GitHub" 

<!--chapter:end:104-instructors.Rmd-->

