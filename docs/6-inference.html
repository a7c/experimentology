<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 6 Statistical inference | Experimentology" />
<meta property="og:type" content="book" />





<meta name="author" content="Michael C. Frank, Mika Braginsky, Julie Cachia, Nicholas Coles, Tom Hardwicke, Robert Hawkins, Maya Mathur, and Rondeline Williams" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Chapter 6 Statistical inference | Experimentology">

<title>Chapter 6 Statistical inference | Experimentology</title>

<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />




<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="toc/toc.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="part"><span><b>I Preliminaries</b></span></li>
<li><a href="1-intro.html#intro"><span class="toc-section-number">1</span> Introduction: theory and experiments</a></li>
<li><a href="2-replication.html#replication"><span class="toc-section-number">2</span> Replication, reproducibility and transparency</a></li>
<li><a href="3-ethics.html#ethics"><span class="toc-section-number">3</span> Ethics</a></li>
<li class="part"><span><b>II Design and Planning</b></span></li>
<li><a href="4-measurement.html#measurement"><span class="toc-section-number">4</span> Measurement</a></li>
<li><a href="5-design.html#design"><span class="toc-section-number">5</span> Design of experiments</a></li>
<li><a href="6-inference.html#inference"><span class="toc-section-number">6</span> Statistical inference</a></li>
<li><a href="7-models.html#models"><span class="toc-section-number">7</span> Statistical models for more complex designs</a></li>
<li><a href="8-sampling.html#sampling"><span class="toc-section-number">8</span> Sampling</a></li>
<li><a href="9-preregistration.html#preregistration"><span class="toc-section-number">9</span> Preregistration</a></li>
<li class="part"><span><b>III Execution</b></span></li>
<li><a href="10-selection.html#selection"><span class="toc-section-number">10</span> Replicating or extending an existing study</a></li>
<li><a href="11-collection.html#collection"><span class="toc-section-number">11</span> Data collection</a></li>
<li><a href="12-management.html#management"><span class="toc-section-number">12</span> Data management</a></li>
<li class="part"><span><b>IV Analysis and Reporting</b></span></li>
<li><a href="13-viz.html#viz"><span class="toc-section-number">13</span> Visualization</a></li>
<li><a href="14-eda.html#eda"><span class="toc-section-number">14</span> Exploratory data analysis</a></li>
<li><a href="15-writing.html#writing"><span class="toc-section-number">15</span> Reproducible writing</a></li>
<li><a href="16-meta.html#meta"><span class="toc-section-number">16</span> Meta-analysis</a></li>
<li><a href="17-conclusions.html#conclusions"><span class="toc-section-number">17</span> Conclusions</a></li>
<li class="part"><span><b>V Appendices</b></span></li>
<li><a href="18-github.html#github"><span class="toc-section-number">18</span> Github Tutorial</a></li>
<li><a href="19-rmarkdown.html#rmarkdown"><span class="toc-section-number">19</span> R Markdown Tutorial</a></li>
<li><a href="20-tidyverse.html#tidyverse"><span class="toc-section-number">20</span> Tidyverse Tutorial</a></li>
<li><a href="21-ggplot.html#ggplot"><span class="toc-section-number">21</span> ggplot Tutorial</a></li>
<li><a href="22-instructors.html#instructors"><span class="toc-section-number">22</span> Instructor‚Äôs Guide</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="inference" class="section level1" number="6">
<h1><span class="header-section-number">Chapter 6</span> Statistical inference</h1>
<div class="learning-goals">
<p>üçé Learning goals:</p>
<ul>
<li>Discuss differences between frequentist and Bayesian perspectives</li>
<li>Reconceptualize statistical ‚Äútests‚Äù as models of data</li>
<li>Build intuitions about how specific ‚Äútests‚Äù (e.g., t-tests) relate to more general frameworks (e.g., regression, mixed effects models)</li>
<li>Identify which models are best suited for which research questions</li>
<li>Reason about effect effect size as estimated by statistical models</li>
</ul>
</div>
<p>We‚Äôve been arguing that experiments are about measuring effects. You might ask then, why does this book even need a chapter about statistical inference? Why can‚Äôt we just report our measurements and be done?</p>
<p>Statistical inference is critical to experimental methods because experiments ‚Äì especially experiments with human participants ‚Äì tend to yield noisy and variable data. Statistical methods allow us to make sense of the data and ask critical questions of it, like:</p>
<ol style="list-style-type: decimal">
<li>How likely is it that this pattern of measurements was produced by chance variation?</li>
<li>Do these data provide more support for one hypothesis or another?</li>
<li>How big is the effect of a manipulation, and how precise is our estimate of that effect?</li>
<li>What portion of the variation in the data is due to a particular manipulation (as opposed to variation between participants, stimulus items, or other manipulations)?</li>
</ol>
<p>Question (1) is associated with one particular type of statistical testing ‚Äì <strong>null hypothesis significance testing</strong> (NHST) in the <strong>frequentist</strong> statistical tradition. NHST has become synonymous with data analysis. This equivalence has been to the detriment of both data analysis and statistics more generally. The instinct to ‚Äúgo test your data for significance‚Äù before visualizing your data and trying to understand how it relates to the various sources of variation in your design (participants, items, manipulations, etc.) is in our view one of the most unhelpful things an experimenter can do. <span class="math inline">\(p &lt; .05\)</span> or not, a test of this sort gives you literally <em>one bit</em><label for="tufte-sn-1" class="margin-toggle sidenote-number">1</label><input type="checkbox" id="tufte-sn-1" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">1</span> In the information theoretic sense, as well as the common sense!</span> of information about your data. The kinds of visualizations we advocate in Chapter <a href="13-viz.html#viz">13</a> give you a much richer sense of what happened in your experiment!</p>
<p>In this chapter, we will describe NHST, the conventional method that many students still learn (and many scientists still use) as their primary method for engaging with data. All practicing experimentalists need to understand NHST both to read the literature and also to apply this method in appropriate situations ‚Äì for example, tests of interventions where the key question for informing practice is whether there is a significant difference between a treatment condition and an appropriate control. But we will also try to contextualize NHST as a very special case of a broader set of strategies around modeling and inference. Further, we will note how some of the pathologies of NHST have almost certainly 62been a driver of the replication crisis.</p>
<p><label for="tufte-mn-1" class="margin-toggle">‚äï</label><input type="checkbox" id="tufte-mn-1" class="margin-toggle"><span class="marginnote"><span style="display: block;"><img src="images/krushke.png"/> Clarifying the distinctions between Bayesian and Frequentist paradigms and the ways that they approach inference and estimation. For many settings, we think the estimation mindset is more useful. From Kruschke and Liddell (2018).</span></span></p>
<p>What should replace NHST? There has been a recent move towards the use of Bayes Factors to quantify the evidence in support of a hypothesis. Bayes Factors can help answer questions like (2). We introduce these tools, and believe that they have broader applicability than the NHST framework and should be known by students. Both NHST and Bayes Factors are examples of what are called <strong>inference</strong> strategies, which are centered around drawing conclusions from data: significant or not, hypothesis one or hypothesis two.<label for="tufte-sn-2" class="margin-toggle sidenote-number">2</label><input type="checkbox" id="tufte-sn-2" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">2</span> An important but subtle point is that ‚Äúinference‚Äù is an ambiguous term. We are using it in the sense of drawing conclusions from data. But the term is used more broadly as well in the phrase ‚Äústatisical inference,‚Äù which is often contrasted with purely descriptive approaches. In this usage, ‚Äúinference‚Äù means making generalizations from data that extend beyond the current set of observations. We do want to do that!</span></p>
<p>We contrast inference strategies with <strong>estimation</strong> and <strong>modeling</strong> stratgies, which are more suited towards questions (3) and (4) <span class="citation">(<label for="tufte-mn-2" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-2" class="margin-toggle">J. K. Kruschke and Liddell 2018<span class="marginnote">Kruschke, John K, and Torrin M Liddell. 2018. <span>‚ÄúThe Bayesian New Statistics: Hypothesis Testing, Estimation, Meta-Analysis, and Power Analysis from a Bayesian Perspective.‚Äù</span> <em>Psychon. Bull. Rev.</em> 25 (1): 178‚Äì206.</span>)</span>. The goal of these strategies is to yield a precise estimate of the relationships underlying observed variation in the data. Critically, one of these estimates is the causal effect of the experimental manipulation(s). That explains our affection for these approaches: if a good theory predicts these kinds of causal effects, it makes sense that we‚Äôd want to estimate them precisely! Estimating one quantity in isolation is not maximally effective, though, since often there will be variation in the estimate that has to do with other known sources. If it‚Äôs helpful to have an example, imagine the Stroop effect, which has a fairly consistent effect on both fast and slow readers <span class="citation">(<label for="tufte-mn-3" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-3" class="margin-toggle">Haaf and Rouder 2017<span class="marginnote">Haaf, Julia M, and Jeffrey N Rouder. 2017. <span>‚ÄúDeveloping Constraint in Bayesian Mixed Models.‚Äù</span> <em>Psychological Methods</em> 22 (4): 779.</span>)</span>. But estimates of this effect will be more precise if we take into account that some readers are slower or faster, rather than just averaging across all this variation.]</p>
<p>This isn‚Äôt a statistics book and we won‚Äôt attempt to teach the full array of important statistical concepts that will allow students to build good models of a broad array of datasets. (Sorry!).<label for="tufte-sn-3" class="margin-toggle sidenote-number">3</label><input type="checkbox" id="tufte-sn-3" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">3</span> If you‚Äôre interested in going deeper, here are two books that have been really influential for us. The first is <span class="citation"><label for="tufte-mn-4" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-4" class="margin-toggle">Gelman and Hill (2006)<span class="marginnote">Gelman, Andrew, and Jennifer Hill. 2006. <em>Data Analysis Using Regression and Multilevel/Hierarchical Models</em>. Cambridge university press.</span></span>, which teaches regression and multi-level modeling from the persepective of data description and modeling. The second is <span class="citation"><label for="tufte-mn-5" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-5" class="margin-toggle">McElreath (2018)<span class="marginnote">McElreath, Richard. 2018. <em>Statistical Rethinking: A Bayesian Course with Examples in r and Stan</em>. Chapman; Hall/CRC.</span></span>. a course on building Bayesian models of the causal structure of your data. Honestly, neither is an easy book to sit down and read (unless you are the kind of person who reads stats books on the subway for fun) but both really reward detailed study. We encourage you to get together a reading group and go through the exercises together. It‚Äôll be well worth while in its impact on your statistical and scientific thinking.</span> Instead, we‚Äôre going to focus on the case wherethere is a single binary manipulation (experimental treatment vs.¬†control) and we want to estimate its effect. In Chapter <a href="7-models.html#models">7</a>, we‚Äôll generalize this approach to model other sources of variability. We‚Äôll start by looking at a case study that established modern statistical inference.</p>
<!-- ::: {.case-study} -->
<!-- üî¨ Case study: The lady tasting tea -->
<div id="the-lady-tasting-tea" class="section level2" number="6.1">
<h2><span class="header-section-number">6.1</span> The lady tasting tea</h2>
<p>The birth of modern statistical inference came from a single, epochal act of mansplaining.<label for="tufte-sn-4" class="margin-toggle sidenote-number">4</label><input type="checkbox" id="tufte-sn-4" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">4</span> An important piece of context for the work of Ronald Fisher, Karl Pearson, and other early pioneers of statistical inference is that they were all strong proponents of eugenics. Fisher was the founding Chairman of the Cambridge Eugenics Society. Pearson was perhaps even worse, an avowed social darwinist who believed fervently in Eugenic legislation. These views are repugnant.</span> Sir Ronald Fisher was apparently at a party when a lady declared that she could tell the difference between cups when the tea was added to the milk vs.¬†the milk to the tea. Rather than taking her at her word, Fisher devised an experimental and data analysis procedure to test her claim.<label for="tufte-sn-5" class="margin-toggle sidenote-number">5</label><input type="checkbox" id="tufte-sn-5" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">5</span> If you‚Äôre interested in this history, we recommend <span class="citation"><label for="tufte-mn-6" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-6" class="margin-toggle">Salsburg (2001)<span class="marginnote">Salsburg, David. 2001. <em>The Lady Tasting Tea: How Statistics Revolutionized Science in the Twentieth Century</em>. Macmillan.</span></span>‚Äôs delightful book, ‚ÄúThe Lady Tasting Tea,‚Äù about the origins of modern statistics.</span></p>
<p>The basic schema of the experiment was that the lady would have to judge a set of new cups of tea (a two-alternative forced choice measure). Her data would then be analyzed to determine whether her level of correct choice exceeded that expected by chance. While this process now sounds like a quotidian experiment that might be done on a cooking reality show, in fact this is one of those touchstones that feels unremarkable because it literally established the way science was done for the next century.</p>
<p>The first element of the experiment that was unusual was its treatment of design confounds such as pouring order or cup material. Prior experimental practice would have been to assume that the best practice would have been to try to equate all of the cups as closely as possible, decreasing the influence of confounders. Fisher recognized that this strategy was insufficient and that random assignment was critical for making strong causal inferences about the treatment (milk then tea vs.¬†tea then milk).<label for="tufte-sn-6" class="margin-toggle sidenote-number">6</label><input type="checkbox" id="tufte-sn-6" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">6</span> We just asserted the causal power of random assignment in Chapter <a href="1-intro.html#intro">1</a> but here‚Äôs where it originates!</span></p>
<p>The second innovation was the idea of creating a model of what might happen during the experiment: specifically, a <strong>null model</strong> in which the nameless lady chose cups by chance rather than because of some tea sensitivity. The goal of the analysis was then to compute the probability of the lady‚Äôs choices under this null model. Fisher then declared that it is ‚Äúusual and convenient for experimenters to take 5 percent as a standard level of convenience,‚Äù establishing the .05 cutoff that has become gospel throughout the sciences.<label for="tufte-sn-7" class="margin-toggle sidenote-number">7</label><input type="checkbox" id="tufte-sn-7" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">7</span> Actually, right after establishing .05 as a cutoff, Fisher then writes that ‚Äúin the statistical sense, we thereby admit that no isolated experiment, however significant in itself, can suffice for the experimental demonstration of any natural phenomenon‚Ä¶ in order to assert that a natural phenomenon is experimentally demonstrable we need, not an isolated record, but a reliable method of procedure. In relation to the test of significance, we may say that a phenomenon is experimentally demonstrable when we know how to conduct an experiment which will rarely fail to give us a statistically significant result.‚Äù In other words, Fisher was all for replication!</span></p>
<p>With apologies to Fisher, we‚Äôll design a slightly more modern version of his experiment using a forced-choice judgmeent measure. We present participants with a series of cups of tea (in random order, of course). Half are prepared with milk first, half tea first. Then the participant must give a judgement. The null model is based on the idea that, if the participant knows nothing then they should choose at random between the two possibilities. Hence, we can use a simple binomial distribution to show expected chance performance for different sets of trials.<label for="tufte-sn-8" class="margin-toggle sidenote-number">8</label><input type="checkbox" id="tufte-sn-8" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">8</span> We quantify our measure as the proportion of correct trials, since trials here are interchangeable.</span></p>
<p>These null distributions are shown in Figure @ref(fig:null_model). We show cases with 6, 12, 24, and 48 trials, and highlight a significance threshold ‚Äì the threshold above which you would fall only 5% of the time if you were guessing by chance.<label for="tufte-sn-9" class="margin-toggle sidenote-number">9</label><input type="checkbox" id="tufte-sn-9" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">9</span> Note that we are effectively doing a ‚Äúone-tailed‚Äù test here, since we‚Äôre attending only to the right hand side of each plot and ignoring cases where the participant is exactly flipped in their judgments.</span> Conventionally, if you cross this threshold, you can say you ‚Äúreject the null.‚Äù</p>
<div class="figure"><span id="fig:null-model"></span>
<p class="caption marginnote shownote">
Figure 6.1: Probability of observing a particular number of trials correct under the null model of no sensitivity to tea preparation order. Each facet gives a different number of trials. The red line indicates a significance threshold above which data have less than a 5% chance of appearing.
</p>
<img src="experimentology_files/figure-html/null-model-1.png" alt="Probability of observing a particular number of trials correct under the null model of no sensitivity to tea preparation order. Each facet gives a different number of trials. The red line indicates a significance threshold above which data have less than a 5\% chance of appearing." width="\linewidth"  />
</div>
<p>Examining these null distributions reveals an intuitive regularity. The more trials a participant completes, the lower the threshold for significance becomes. With 6 trials, you need to get all 5 correct for that result to have a probability of less than 5% probable under the null model. With 12, you need to observe 10 (9.08%) or more. With 24 and 48, you need 17 (16.04%) and 31 (30.02%) respectively.<label for="tufte-sn-10" class="margin-toggle sidenote-number">10</label><input type="checkbox" id="tufte-sn-10" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">10</span> You can calculate these numbers by looking at what‚Äôs called the <strong>quantile function</strong> of the binomial binomial: the function that tells you how many correct trials out of the total number corresponds to a particular probability.</span> With greater numbers of trials, the expectation based on chance becomes more precise and so we are able to reject then null based on smaller deviations from chance. Even here in the original case of null hypothesis significance testing, it is clear that our goals matter. If we‚Äôre merely testing a hypothesis informally at a garden party, perhaps we don‚Äôt want to subject our friend to dozens of trials. But if the stakes were higher and a more precise estimate was necessary, maybe we‚Äôd want to do a more extensive experiment!</p>
<p>Something very important is missing from this way of evaluating tea tasting. If we think back to our goals as experimentalists, we are hoping to use the logic of random assignment to measure causal effects of interest. Here, the effect we‚Äôre measuring is something like the effect of tea preparation on taste perception (maybe not the most consequential question, but‚Ä¶). Yet our statistical paradigm is misaligned with our goal! All of this talk of significance and rejecting the null doesn‚Äôt tell us about either how big the effect is, or how precisely we have measured it! That‚Äôs why we are going to be advocating throughout for estimation as the key goal for our statistical analysis, with the varieties of inference (significance testing included) as a secondary goal.
<!-- ::: --></p>
</div>
<div id="a-probabilistic-framework" class="section level2" number="6.2">
<h2><span class="header-section-number">6.2</span> A probabilistic framework</h2>
<p>An alternative way to frame our statistical practice is to start from the idea of estimation.</p>
<p>Let‚Äôs say we want to estimate some quantity, we‚Äôll call it <span class="math inline">\(h\)</span> ‚Äì our belief about the participant‚Äôs accuracy.<label for="tufte-sn-11" class="margin-toggle sidenote-number">11</label><input type="checkbox" id="tufte-sn-11" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">11</span> You can‚Äôt get away from measurement and psychometrics! We said we were really interested in the effect of tea ordering on tea perception. But this number that we‚Äôre estimating is something more like this particular participant‚Äôs accuracy, and that‚Äôs not the same thing that we actually wanted. To get from what we‚Äôre estimating to our effect of interest, we‚Äôd need to establish some <strong>linking hypotheses</strong> about how the participant‚Äôs accuracy can be derived from the participant‚Äôs perception and the properties of the stimulus. That‚Äôs perhaps not worth doing in this toy example, but more generally it‚Äôs a critical part of getting from your measure to your construct of interest!</span> We observe some data <span class="math inline">\(d\)</span>, consisting of the set of correct and incorrect responses in the experiment. Now we can use <strong>Bayes‚Äô rule</strong>, a tool from basic probability theory, to estimate this number.</p>
<p>Bayes‚Äô rule says:</p>
<p><span class="math display">\[
\color{purple}{p(h | d)} = \frac{\color{red}{p(d | h)} \color{blue}{p(h)}}{p(d)}.
\]</span>
Each part of this equation has a name, and it‚Äôs worth becoming familiar with them. The thing we want to compute (<span class="math inline">\(p(h|d)\)</span>) is called the <strong>posterior probability</strong> ‚Äì it tell us what we should believe about the participant‚Äôs ability given the data we observed. We then break that down into two terms in the numerator.<label for="tufte-sn-12" class="margin-toggle sidenote-number">12</label><input type="checkbox" id="tufte-sn-12" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">12</span> We‚Äôre making the posterior  to indicate the combination of likelihood () and prior ().</span></p>
<p>The first part of the numerator is <span class="math inline">\(p(d|h)\)</span>, the probability of the data we observed given our hypothesis about the participant‚Äôs ability. This part is called the <strong>likelihood</strong>.<label for="tufte-sn-13" class="margin-toggle sidenote-number">13</label><input type="checkbox" id="tufte-sn-13" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">13</span> Speaking informally, ‚Äúlikelihood‚Äù is just a synonym for probability, but this is a technical meaning for the term, which can get a bit confusing.</span> This term tells us about the relationship between our hypothesis and the data we observed ‚Äì so if we think the participant has high ability (say <span class="math inline">\(h = .9\)</span>) then the probability of a bunch of low accuracy observations will be fairly low.</p>
<p>The second term in the numerator, <span class="math inline">\(p(h)\)</span>, is called the <strong>prior</strong>. This term encodes our beliefs about how likely our participant is to have different levels of ability. Intuitively, if we think that they are very unlikely to have high tea discrimination ability, we should require more evidence to convince us of a particular level of discrimination. In contrast, if we think they are likely to have this ability, we should be easier to convince.</p>
<div class="figure"><span id="fig:bayes-demo"></span>
<p class="caption marginnote shownote">
Figure 6.2: Examples of Bayesian inference about tea discrimination ability under three different priors (facets). Blue lines give the prior probability distribution, red lines give the likelihood of the data, and purple lines give the posterior distribution from combining likelihood and prior.
</p>
<img src="experimentology_files/figure-html/bayes-demo-1.png" alt="Examples of Bayesian inference about tea discrimination ability under three different priors (facets). Blue lines give the prior probability distribution, red lines give the likelihood of the data, and purple lines give the posterior distribution from combining likelihood and prior." width="\linewidth"  />
</div>
<p>Figure <a href="6-inference.html#fig:bayes-demo">6.2</a> gives an example of the combination of prior and data.<label for="tufte-sn-14" class="margin-toggle sidenote-number">14</label><input type="checkbox" id="tufte-sn-14" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">14</span> The model we use for this example is called a <strong>Beta-Binomial conjugate model</strong> and is a very convenient model for working with count data representing successes and failures.</span> For the sake of this example, we assume that we have run 12 tea discrimination trials and observed 9 successes and 3 failures. The evidence alone ‚Äì with no prior ‚Äì suggests a discrimination estimate of <span class="math inline">\(9/12 = .75\)</span>.<label for="tufte-sn-15" class="margin-toggle sidenote-number">15</label><input type="checkbox" id="tufte-sn-15" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">15</span> Technically this is known as the <strong>maximum a posteriori</strong> estimate, or the MAP. We won‚Äôt use that term though.</span> When we use a flat prior, we get the same estimate of 0.75. In contrast, if we go in assuming that discrimination is likely to be absent or weak, we are biased downward in our eventual estimate of 0.69; if we go in assuming good discrimination, we end up biased upwards to 0.78.</p>
<p>Fisher‚Äôs original framwork for significance testing focused only on the <strong>null hypothesis</strong> of no discrimination. In contrast, the Bayesian estimation method here focuses on the magnitude of accuracy.<label for="tufte-sn-16" class="margin-toggle sidenote-number">16</label><input type="checkbox" id="tufte-sn-16" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">16</span> If you‚Äôre reading carefully, you might have noticed that we <em>could</em> have discovered that the estimate of accuracy was very similar to chance ‚Äì more about this below.</span> The intuition we‚Äôd like you to get is that, if you are an experimentalist who cares about the magnitude of causal relationships (and we hope you are), then Fisher‚Äôs statistical approach isn‚Äôt ideally suited to your goals.</p>
<!-- TODO: HERE WOULD BE A GREAT PLACE FOR AN INTERACTIVE -->
</div>
<div id="inference-1" class="section level2" number="6.3">
<h2><span class="header-section-number">6.3</span> Inference</h2>
<p>We just finished arguing that an estimation strategy is more aligned with the default goal of an experimentalist. But that‚Äôs not always the case. Sometimes you do want to make an inference, in the sense of making some kind of rule-based decision about your data. The clearest case of this is in the context of an experiment that evaluates an intervention ‚Äì say a curriuclum manipulation to improve an educational outcome. In this kind of case, we might primarily want a clear answer: does this intervention ‚Äúwork‚Äù ‚Äì meaning, improve our outcome relative to control ‚Äì or not. In this kind of context, perhaps the magnitude of the effect is going to be so dependent on the specifics of implementation that we don‚Äôt really have a lot of predictions about the magnitude.</p>
<p>This kind of decision problem is shown in the figure in the margin. You want to correctly classify the intervention effect ‚Äì if it works, then you want to detect that (true positive) and same if it doesn‚Äôt (true negative). If you are too conservative in your criteria for the intervention having an effect, however, then you risk a false negative, where you incorrectly conclude that it doesn‚Äôt work. And if you‚Äôre too liberal in your assessment of the evidence, then you riks a false positive.<label for="tufte-sn-17" class="margin-toggle sidenote-number">17</label><input type="checkbox" id="tufte-sn-17" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">17</span> To make really rational decisions, you could couple this chart to some kind of utility function that assesed the costs of different outcomes. For example, you might think it‚Äôs worse to proceed with an intervention that doesn‚Äôt work than to stay with business as usual. In that case, you‚Äôd assign a higher cost to a false positive and accordingly try to adopt a more conservative criterion. We won‚Äôt cover this kind of decision analysis here, but <span class="citation"><label for="tufte-mn-7" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-7" class="margin-toggle">Pratt et al. (1995)<span class="marginnote">Pratt, John Winsor, Howard Raiffa, Robert Schlaifer, and others. 1995. <em>Introduction to Statistical Decision Theory</em>. MIT press.</span></span> is a classic textbook on statistical decision theory if you‚Äôre interested.</span> This kind of consideration of different outcomes comes up again when we consider classical power analysis in Chapter <a href="8-sampling.html#sampling">8</a>.</p>
<!-- TODO: check this book rec -->
<p><label for="tufte-mn-8" class="margin-toggle">‚äï</label><input type="checkbox" id="tufte-mn-8" class="margin-toggle"><span class="marginnote"><span style="display: block;"><img src="images/confusion.png"/> Standard confusion matrix.</span></span>
<!-- https://commons.wikimedia.org/wiki/File:ConfusionMatrixRedBlue.png -- CC --></p>
<p>So now that we know our decision problem, we need to set up a decision rule ‚Äì how do we decide whether our data support a positive conclusion? There are at least two different common ways to set up this kind of inference. One of these is Fisher‚Äôs way: null hypothesis significance testing. The second is via a technique called the <strong>Bayes Factor</strong>. Since we‚Äôve already talked about NHST a bit, we‚Äôll introduce the Bayes Factor first. Then we can see how NHST relates.</p>
<div id="what-is-a-bayes-factor" class="section level3" number="6.3.1">
<h3><span class="header-section-number">6.3.1</span> What is a Bayes factor?</h3>
<p>Bayes Factors are a method for quantifying the support for one hypothesis over another, based on an observed dataset. Informally, we‚Äôve now talked about two different distinct hypotheses about the tea situation: our participant could have <em>no</em> tea discrimination ability ‚Äì leading to chance performance. We call this <span class="math inline">\(H_0\)</span>. Or they could have some non-zero ability ‚Äì leading to greater than chance perfomance. We call this <span class="math inline">\(H_1\)</span>. The Bayes Factor is simply the likelihood of the data (in the technical sense used above) under <span class="math inline">\(H_1\)</span> vs.¬†under <span class="math inline">\(H_0\)</span>:</p>
<p><span class="math display">\[
BF = \frac{\color{red}{p(d | H_1)}}{\color{red}{p(d | H_0)}}.
\]</span>
The Bayes Factor is a ratio, so if it is greater than 1, the data are more likely under <span class="math inline">\(H_1\)</span> than they are under <span class="math inline">\(H_0\)</span> ‚Äì and vice versa for values between 1 and 0. A BF of 3 means there is three times as much evidence for <span class="math inline">\(H_1\)</span> than <span class="math inline">\(H_01\)</span>, or equivalently 1/3 as much evidence for <span class="math inline">\(H_0\)</span> as <span class="math inline">\(H_1\)</span>.<label for="tufte-sn-18" class="margin-toggle sidenote-number">18</label><input type="checkbox" id="tufte-sn-18" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">18</span> Sometimes people refer to the BF in favor of <span class="math inline">\(H_1\)</span> as the <span class="math inline">\(BF_{10}\)</span> and the BF in favor of <span class="math inline">\(H_0\)</span> as the <span class="math inline">\(BF_{01}\)</span>. This notation strikes us as a bit confusing because a reader might wonder what the 10 in the subscript means.</span> There are no hard and fast rules for Bayes Factor interpretation, but many people follow guidelines from <span class="citation"><label for="tufte-mn-9" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-9" class="margin-toggle">Jeffreys (1998)<span class="marginnote">Jeffreys, Harold. 1998. <em>The Theory of Probability</em>. OUP Oxford.</span></span>, shown in the margin.</p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:jeffreys">Table 6.1: </span>Jeffreys‚Äô (1961/1998) interpretation guidelines for Bayes Factors.</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">BF range</th>
<th align="left">Interpretation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">&lt; 1</td>
<td align="left">Negative evidence (supports H0)</td>
</tr>
<tr class="even">
<td align="left">1 ‚Äì 3</td>
<td align="left">Barely worth mentioning</td>
</tr>
<tr class="odd">
<td align="left">3 ‚Äì 10</td>
<td align="left">Substantial</td>
</tr>
<tr class="even">
<td align="left">10 ‚Äì 30</td>
<td align="left">Strong</td>
</tr>
<tr class="odd">
<td align="left">30 ‚Äì 100</td>
<td align="left">Very strong</td>
</tr>
<tr class="even">
<td align="left">&gt; 100</td>
<td align="left">Decisive</td>
</tr>
</tbody>
</table>
<p>There are a couple of things to notice about the Bayes Factor. The first is that it is a continous measure. You can dichotomize decisions based on the Bayes Factor by declaring a cutoff (say, BF &gt; 3 or BF &gt; 10), there is no intrinsic threshold at which you would say the evidence is ‚Äúsignificant.‚Äù<label for="tufte-sn-19" class="margin-toggle sidenote-number">19</label><input type="checkbox" id="tufte-sn-19" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">19</span> Of course this is true for <span class="math inline">\(p\)</span>-values too, but the <span class="math inline">\(p &lt; .05\)</span> criterion is so culturally ingrained in most scientists that it‚Äôs hard to get away from.</span> Second, it doesn‚Äôt depend on our prior probability of <span class="math inline">\(H_1\)</span> vs.¬†<span class="math inline">\(H_0\)</span>. We might think of <span class="math inline">\(H_1\)</span> as very implausible. But the BF is independent of that prior belief. So that means it‚Äôs a measure of how much the evidence should shift our beliefs away from our prior. One nice way to think about this is that the Bayes Factor computes how much our beliefs ‚Äì whatever they are ‚Äì should be changed by the data <span class="citation">(<label for="tufte-mn-10" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-10" class="margin-toggle">Morey and Rouder 2011<span class="marginnote">Morey, Richard D, and Jeffrey N Rouder. 2011. <span>‚ÄúBayes Factor Approaches for Testing Interval Null Hypotheses.‚Äù</span> <em>Psychological Methods</em> 16 (4): 406.</span>)</span>.</p>
<p>In practice, the thing that is both tricky and good about Bayes Factors is that you need to define an actual model of what <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_1\)</span> are. That process involves making some assumptions explicit. We won‚Äôt go into how to make these models here ‚Äì this is a big topic that is covered extensively in books on Bayesian data analysis.<label for="tufte-sn-20" class="margin-toggle sidenote-number">20</label><input type="checkbox" id="tufte-sn-20" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">20</span> Two good ones are <span class="citation"><label for="tufte-mn-11" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-11" class="margin-toggle">Gelman et al. (1995)<span class="marginnote">Gelman, Andrew, John B Carlin, Hal S Stern, and Donald B Rubin. 1995. <em>Bayesian Data Analysis</em>. Chapman; Hall/CRC.</span></span> (a bit more statistical) and <span class="citation"><label for="tufte-mn-12" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-12" class="margin-toggle">J. Kruschke (2014)<span class="marginnote">Kruschke, John. 2014. <em>Doing Bayesian Data Analysis: A Tutorial with r, JAGS, and Stan</em>. Academic Press.</span></span> (a bit more focused on psychological data analysis). An in-prep web-book by Nicenboim et al.¬†also looks great: <a href="">https://vasishth.github.io/bayescogsci/book/</a>.</span> Below and in Chapter <a href="7-models.html#models">7</a> we will provide some guidance for how to compute Bayes Factors for simple experimental designs. The goal here is just to give a sense of how they work.</p>
<p>To continue our tea-tasting example, let‚Äôs continue to assume our participant gets 9 of 12 trials right. Now to compute a Bayes Factor, we will have to define our hypotheses. Defining the null is pretty simple ‚Äì it should just be that choices are random coin flips. But to define our <span class="math inline">\(H_1\)</span> things get a bit more complex. We could just assume that the participant should get 75% percent of trials correct, but this could be a problematic choice! For example, what if our participant got 550 / 1000 trials correct? That would give us a strong sense that they had some tea ability, but would actually be more consistent with the <span class="math inline">\(H_0\)</span> of 50% accuracy than the <span class="math inline">\(H_1\)</span> of 75% accuracy! In practice, what we need to do is define a distribution over possible accuracies and then average across these ‚Äì so we assign some probability to 75% accuracy but also some probability to 55% accuracy and even 50% accuracy.<label for="tufte-sn-21" class="margin-toggle sidenote-number">21</label><input type="checkbox" id="tufte-sn-21" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">21</span> You might wonder how we can compare a <span class="math inline">\(H_0\)</span> that assumes 50% accuracy and a <span class="math inline">\(H_1\)</span> that <em>includes</em> 50% accuracy in it as well. The key here is that <span class="math inline">\(H_1\)</span> is more complex because it includes a range of values.</span>. The details don‚Äôt matter here, but just for simplicity we adopt the discrimination prior shown in Figure <a href="6-inference.html#fig:bayes-demo">6.2</a>.</p>
<p>Once we set up our models, we can compute the relative likelihood of the data under each. The resulting BF is 2.96, somewhere between anecdotal and substantial evidence. On the other hand, if we doubled the size of the experiment and found the same pattern ‚Äì 18/24 trials correct ‚Äì the BF goes up to 11.17, which we could classify as strong evidence.</p>
</div>
<div id="what-a-p-value-is-and-isnt" class="section level3" number="6.3.2">
<h3><span class="header-section-number">6.3.2</span> What a p-value is (and isn‚Äôt)</h3>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:p-bf-comparison">Table 6.2: </span>Comparison of p-value and BF for several different tea-tasting scenarios. p-values are one-tailed. Note that BF is heavily dependent on the specific prior on tasting successes that we assign.</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="right">Successes</th>
<th align="right">Total trials</th>
<th align="right">% accuracy</th>
<th align="right">p value</th>
<th align="right">BF</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">3</td>
<td align="right">3</td>
<td align="right">1.00</td>
<td align="right">0.125</td>
<td align="right">3.732</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="right">6</td>
<td align="right">1.00</td>
<td align="right">0.016</td>
<td align="right">17.214</td>
</tr>
<tr class="odd">
<td align="right">12</td>
<td align="right">12</td>
<td align="right">1.00</td>
<td align="right">0.000</td>
<td align="right">501.764</td>
</tr>
<tr class="even">
<td align="right">9</td>
<td align="right">12</td>
<td align="right">0.75</td>
<td align="right">0.073</td>
<td align="right">2.960</td>
</tr>
<tr class="odd">
<td align="right">18</td>
<td align="right">24</td>
<td align="right">0.75</td>
<td align="right">0.011</td>
<td align="right">11.169</td>
</tr>
<tr class="even">
<td align="right">36</td>
<td align="right">48</td>
<td align="right">0.75</td>
<td align="right">0.000</td>
<td align="right">194.031</td>
</tr>
<tr class="odd">
<td align="right">55</td>
<td align="right">100</td>
<td align="right">0.55</td>
<td align="right">0.184</td>
<td align="right">0.199</td>
</tr>
<tr class="even">
<td align="right">550</td>
<td align="right">1000</td>
<td align="right">0.55</td>
<td align="right">0.001</td>
<td align="right">5.626</td>
</tr>
</tbody>
</table>
<p>We already have a working definition of what a <span class="math inline">\(p\)</span>-value is from our discussion above: it‚Äôs the <strong>probability of the data (or any data that would be more extreme) under the null hypothesis</strong>. How is this quantity related to either our Bayesian estimate or the BF? Well, the first thing to notice is that the <span class="math inline">\(p\)</span>-value is very close (but not indentical to) a Bayesian likelihood, which is the probability of the data but doesn‚Äôt include any more extreme data. So for example, the probability of getting 9 out of 12 tea-tasting trials correct under the null hypothesis is 0.05, meaning there‚Äôs a 5% chance of observing exactly this many successes. But the p-value is given by the summed probability of 9, 10, 11, <em>and</em> 12 successes, which is 0.07. You can see how <span class="math inline">\(p\)</span>-values (from a one-tailed exact binomial test) compare to our Bayes Factors in Table <a href="6-inference.html#tab:p-bf-comparison">6.2</a>. Although in general BFs tend to be a bit more conservative, you‚Äôll see that the relationship isn‚Äôt obvious for this example. That‚Äôs because, especially for small samples, BFs are going to be quite dependent on the specific assumptions used in their computation.</p>
<p>The likelihood of the data under the null hypothesis is a critical number to know for computing the Bayes factor. But it doesn‚Äôt tell us a lot of things that we might like to know! For example, it doesn‚Äôt tell us the probability of the data under any positive hypothesis that we might be interested in ‚Äì that‚Äôs the posterior probability <span class="math inline">\(p(H_1 | d)\)</span>. In fact, people have so much troule understanding what <span class="math inline">\(p\)</span>-values <em>do</em> say that there are whole articles written about these misconceptions. Table <a href="6-inference.html#tab:dirty-dozen">6.3</a> shows a set of misconceptions documented and refuted by <span class="citation"><label for="tufte-mn-13" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-13" class="margin-toggle">Goodman (2008)<span class="marginnote">Goodman, Steven. 2008. <span>‚ÄúA Dirty Dozen: Twelve p-Value Misconceptions.‚Äù</span> In <em>Seminars in Hematology</em>, 45:135‚Äì40. 3. Elsevier.</span></span>.</p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:dirty-dozen">Table 6.3: </span>A ‚Äúdirty dozen‚Äù <em>p</em> value misconceptions. Adapted from Goodman (2008).</span><!--</caption>--></p>
<table>
<colgroup>
<col width="2%" />
<col width="97%" />
</colgroup>
<thead>
<tr class="header">
<th align="right"></th>
<th align="left">Misconception</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="left">If <em>p</em> = .05, the null hypothesis has only a 5% chance of being true.</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="left">A nonsignificant difference (eg, <em>p</em> ‚â•.05) means there is no difference between groups.</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="left">A statistically significant finding is clinically important.</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="left">Studies with <em>p</em> values on opposite sides of .05 are conflicting.</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="left">Studies with the same <em>p</em> value provide the same evidence against the null hypothesis.</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="left"><em>p</em> = .05 means that we have observed data that would occur only 5% of the time under the null hypothesis.</td>
</tr>
<tr class="odd">
<td align="right">7</td>
<td align="left"><em>p</em> = .05 and <em>p</em> ‚â§.05 mean the same thing.</td>
</tr>
<tr class="even">
<td align="right">8</td>
<td align="left"><em>p</em> values are properly written as inequalities (eg, '<em>p</em> ‚â§.02' when <em>p</em> = .015)</td>
</tr>
<tr class="odd">
<td align="right">9</td>
<td align="left"><em>p</em> = .05 means that if you reject the null hypothesis, the probability of a false positive error is only 5%.</td>
</tr>
<tr class="even">
<td align="right">10</td>
<td align="left">With a <em>p</em> = .05 threshold for significance, the chance of a false positive error will be 5%.</td>
</tr>
<tr class="odd">
<td align="right">11</td>
<td align="left">You should use a one-sided <em>p</em> value when you don‚Äôt care about a result in one direction, or a difference in that direction is impossible.</td>
</tr>
<tr class="even">
<td align="right">12</td>
<td align="left">A scientific conclusion or treatment policy should be based on whether or not the <em>p</em> value is significant.</td>
</tr>
</tbody>
</table>
</div>
<div id="philosophical-and-empirical-views-of-probability" class="section level3" number="6.3.3">
<h3><span class="header-section-number">6.3.3</span> Philosophical (and empirical) views of probability</h3>
<p>Up until now we‚Äôve presented Bayesian and frequentist tools as two different sets of computations. But in fact, these different tools derive from fundamentally different interpretations of what a probability even is. Very roughly, frequentist approaches tend to believe that probabilities quantify the long-run frequencies of certain events. So, if we say that some outcome of an event has probability .5, we‚Äôre saying that if that event happened thousands of times, the long run frequency of the outcome would be 50% of the total events. In contrast, the Bayesian viewpoint doesn‚Äôt depend on this sense that events could be exactly repeated. Instead, the <strong>subjective Bayesian</strong> interpretation of probability is that it quantifies the person‚Äôs degree of belief in a particular outcome.<label for="tufte-sn-22" class="margin-toggle sidenote-number">22</label><input type="checkbox" id="tufte-sn-22" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">22</span> This is really a very rough description. If you‚Äôre interested in learning more about this philosophical background, we recommend the Stanford Encyclopedia of Philosophy entry on ‚Äúinterpretations of probability‚Äù: <a href="https://plato.stanford.edu/entries/probability-interpret/" class="uri">https://plato.stanford.edu/entries/probability-interpret/</a>.</span></p>
<p>Take our tea-tasting example (again). On a frequentist interpretation, the participant has a true accuracy that we can approximate via repeated measurement.</p>
<p>You don‚Äôt have to take sides in this deep philosophical debate about what probability is. But it‚Äôs helpful to know that people ‚Äì and scientists are no exception ‚Äì actually seem to reason about the world in ways that are well described by the subjective Bayesian view of probability. Recent cognitive science research has made a lot of headway in describing reasoning as a process of Bayesian probability estimation</p>
<!-- Philosophy -->
<!-- Bayes: Data are the data, what can we infer? (subjective) -->
<!-- Frequentist: Do the same thing again and again, what happens (objective truth) -->
<!-- Prior information -->
<!-- Bayes theorem -->
<!-- ‚ÄúData fixed‚Äù vs. ‚Äúmodel fixed‚Äù -->
<!-- Bayesians don‚Äôt have to care about optional stopping -->
<!-- Differences in philosophy -->
<!-- Bayesian stats are good for: -->
<!-- Cases where you have a lot of prior knowledge -->
<!-- E.g., statistics for voter prediction in elections -->
<!-- Cases where you want to deal with uncertainty in a principled way -->
<!-- E.g., integrating knowledge about measurement error into your model -->
<!-- Null models -->
<!-- Cases where you want to decide in favor of null -->
<!-- (though you can do this in frequentist perspectives) -->
<!-- Small N data -->
<!-- Many frequentist tests are based on guarantees in the limit. Guarantees don‚Äôt hold at N=3 or 5 or even 10.  -->
<!-- Frequentist good for: -->
<!-- Decision theory (cf. bayesian decision theory) -->
<!-- Absence of assumptions (‚Äúmore objective‚Äù) -->
<!-- Often easier to state and solve models -->
</div>
<div id="bayes-factors-and-p-values-practical-guidance" class="section level3" number="6.3.4">
<h3><span class="header-section-number">6.3.4</span> Bayes Factors and p-values: Practical guidance</h3>
<div class="accident-report">
<p>‚ö†Ô∏è Accident report: Do extraordinary claims require extraordinary evidence?</p>
<p>Feeling the future?</p>
<p>ESP example (Wagenmakers et al.¬†2011):
Assume p(h1) is 10x-20</p>
</div>
</div>
</div>
<div id="when-inference-is-and-isnt-appropriate" class="section level2" number="6.4">
<h2><span class="header-section-number">6.4</span> When inference is and isn‚Äôt appropriate</h2>
<p>First discussion of overfitting. See Chapter <a href="#prereg"><strong>??</strong></a>.</p>
</div>
<div id="inference-and-estimation-for-two-group-designs" class="section level2" number="6.5">
<h2><span class="header-section-number">6.5</span> Inference and estimation for two-group designs</h2>
<p>Throughout this book we‚Äôve taken the position that the goal of experiments is to estimate a causal effect of interest, ideally as part of some theory of how different constructs relate to one another. All this talk of hypotheses and inferences above is only indirectly related to that goal.</p>
<ul>
<li>Intuition builder: For very large n, or flat prior, Bayes and frequentist coincide.</li>
</ul>
<div id="simple-models-of-between-group-differences" class="section level3" number="6.5.1">
<h3><span class="header-section-number">6.5.1</span> Simple models of between-group differences</h3>
<p>Introducing simple inference models:</p>
<ul>
<li>The chi-squared test for inferring whether two samples come from the same distribution</li>
<li>The t-test for inferring whether a single group‚Äôs effect differs from 0</li>
<li>The t-test for inferring whether two groups differ from one another</li>
<li>The paired t-test as a first glimpse at how we might account for participant-level random effects (see Chapter 7).</li>
</ul>
<div class="accident-report">
<p>‚ö†Ô∏è Accident report: Once you have the basic t-test under your belt, it might feel natural to compare each group to 0 and conclude that one group is different from 0 and the other one isn‚Äôt. But ‚Äúthe difference between significant and not significant is not necessarily itself statistically significant‚Äù (Nieuwenhuis, Forstmann, and Wagenmakers 2011).</p>
</div>
<p>How to go from theory to hypotheses to statistical model</p>
</div>
</div>
<div id="effect-size" class="section level2" number="6.6">
<h2><span class="header-section-number">6.6</span> Effect size</h2>
<p>With all our talk about estimation above, we didn‚Äôt say nuch about what precisely was being estimated.</p>
<p>Effect size: a common language for describing group differences. Pros and cons of this approach. Pro: comparability across studies. Con: loss of information about measures and real-world predictions; dependence on baseline variability.</p>
<p>Effect sizes (ES):
Cohen‚Äôs = m1 - m2 / SD
Units of ES easy to compare across studies
Good for comparison
Example: school interventions to improve achievement
Necessary for meta-analysis
Example: MA of infant consonant discrimination (cross method comparison)
Necessary for power analysis
Negative: not related to any real units
Many measures of ES, e.g.¬†r2, n2 (eta), log odds</p>
<p>Precision</p>
<p>Error bar:
- standard deviation (why is this bad)?
- SEM
- CI</p>
<p>CIs for inference</p>
<p>Confidence intervals:
95% of these regions will contain the TRUE parameter
Remember frequentists - there is a TRUE parameter</p>
<p><a href="https://istats.shinyapps.io/ExploreCoverage/" class="uri">https://istats.shinyapps.io/ExploreCoverage/</a></p>
<p>But this is not our typical interpretation, which is that 95% chance parameter is in this interval
That‚Äôs the BAYESIAN interpretation</p>
<p>Bayesian Estimation</p>
<p>Find the posterior distribution of the parameter of interest
You can take its mean
Its HPD (highest posterior density)</p>
<p>Confidence in confidence intervals:
<a href="https://link.springer.com/article/10.3758/s13423-015-0947-8" class="uri">https://link.springer.com/article/10.3758/s13423-015-0947-8</a></p>
<div class="interactive">
<p>‚å®Ô∏è Interactive box: non-parametric simulations where you can shuffle data across groups a bunch of times and see what kind of distribution it produces by chance</p>
</div>
<p>Despite all these reasons to be worried about p-values, for many practicing scientists (at least at time of writing) there is no one right answer as to whether to reject <span class="math inline">\(p\)</span>-values. Even if we‚Äôd like to be Bayesian all the time because the paradigm makes philosophical and practical sense, there are a number of obstacles. First, though tools like <code>Stan</code>, <code>BayesFactor</code>, and <code>brms</code> make fitting Bayesian models easier (more about this below and in the next chapter), it‚Äôs still on average quite a bit harder to fit a Bayesian model than it is a frequentist one. Second, because Bayesian analyses are less familiar, it may be an uphill battle to convince advisors, reviewers, and funders to use them. So as a group, we are still mostly Bayesian when we can be ‚Äì and frequentist when it‚Äôs not practical. One reason we don‚Äôt feel so bad about this stance is that, a lot of them time we‚Äôre not so worried about making binary inferences, whether they are at <span class="math inline">\(p &lt; .05\)</span> or <span class="math inline">\(BF &gt; 3\)</span> or whatever the threshold is.</p>
<div class="exercise">
<ol style="list-style-type: decimal">
<li></li>
<li>Split up Goodman‚Äôs (2008) ‚Äúdirty dozen‚Äù in Table <a href="6-inference.html#tab:dirty-dozen">6.3</a> and write a description of why each is a misconception. These can be checked against the original article.</li>
</ol>
</div>

</div>
</div>
<p style="text-align: center;">
<a href="5-design.html"><button class="btn btn-default">Previous</button></a>
<a href="7-models.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="toc/toc.js"></script>


</body>
</html>
