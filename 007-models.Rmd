# Statistical models for more complex designs {#models}

::: {.learning-goals}
üçé Learning goals: Describe what it means to ‚Äòcontrol for‚Äô something; explore what kinds of ‚Äòclustered variance‚Äô are present in our designs (e.g. selecting appropriate fixed vs. random effect structures); select models appropriate for different kinds of dependent variables.
:::

::: {.case-study}
üî¨ Case study: Sometimes effects are driven by specific stimuli. The ‚Äúlanguage as fixed effects fallacy‚Äù (Clark, 1973) means that many early psycholinguistic effects didn‚Äôt take into account stimulus variability. This critique extends to a wide variety of other domains, e.g. task fMRI (Westfall, Nichols, and Yarkoni 2016).
:::

Re-casting the t-test as a regression model 

::: {.interactive}
‚å®Ô∏è Interactive box: Visualizing how different tests are variants of linear models.
:::

- Discrete data and logistic regression. Same thing, different linking function. (lead-in to GLM: probit, Poisson, beta, etc.).

Multilevel regression models of a difference between two groups, controlling for experimental items and subject

  - Sidebar: what should you control for? Different subcultures in psychology either post-hoc control for or look for moderation by demographic factors. We discuss the consequences of these decisions for both precision and causal inference. 

Causality revisited: what can and can‚Äôt be concluded from an experiment

::: {.accident-report}
‚ö†Ô∏è Accident report: Mediation going wrong: even when you have a randomized experiment, you can still mess up your causal inference (Montgomery, Nyhan, and Torres 2018). 
:::

- Dropping subjects who fail a manipulation check can be problematic (Aronow, Baron, and Pinson 2019).
- Mediation requires more confounding assumptions than causal inference about total effects, and these assumptions may be violated even in randomized experiments. 
