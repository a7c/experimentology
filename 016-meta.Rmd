# Meta-analysis {#meta}


::: {.learning-goals}
üçé Learning goals: practice being a critical reader of the literature; extract effect sizes; conduct a simple random-effects meta analysis; monitor within- and across-study biases that can decrease precision and increase bias. 
:::

::: {.case-study}
üî¨ Case study: Hotel towel reuse (Scheibehenne, Jamil, and Wagenmakers 2016). A simple example of aggregating noisy evidence from replications. 



<!-- Comment character is like this -->

## Meta-notes

### Undefined notation and terminology

- "Narrative review"
- Need clear terminology to distinguish estimates from population effects; keep synced with earlier chapters
- Assumes knowledge of standardized mean differences


## Introduction?

In a widely-cited study on the power of describing social norms, Goldstein, Cialdini, and Griskevicius (2008) gave one group of hotel guests a message that encouraged them to reuse their towels and another group of hotel guests a message that indicated that most other guests reused their towels. Across two studies, they found that guests who received the social norm message were more likely to reuse their towels than guests who received the control message (Study 1 *p* = .05, Study 2 *p* = .03). Five later replications, though, all failed to find significant evidence of increases in towel reuse amongst guests receiving the social norm message (all *p*s \> .05).

At first glance, you may be enticed to conclude that the effect of social norm messages does not reliably impact towel reuse. You may even go a step further and try to think of explanations for why one team found the effect, but the other did not. (Maybe the messages change behavior when guest receive nice fluffy towels, but doesn't work when guests receive sets of crappy towels.) Or, if you're a practitioner of experimentology, you may ask yourselves whether these findings are really all that different. You may even be so bold to do a meta-analysis!

If you did the latter, you would find that the results across these studies are, for the most part, quite consistent. Guests in four out of five of the studies were more likely to reuse their towels after receiving social norm messages, but the difference was not large enough in any one study to confidently conclude that this difference was larger than zero. When the results were statistically combined in a meta-analysis, Scheibehenne, Jamil, and Wagenmakers (2016) found that social norm messages *on average* do increase hotel towel reuse.

It's not magic--it's just meta-analysis! In this chapter, you'll learn more about this useful technique.

Notes to Maya: Scheibehenne et al. did not estimate heterogeneity, so I do not know how consistent these results actually are. One of the studies did actually yield a sizeable oppostite-direction effect, so I'm not really sure if their fixed-effect assumption is reasonable. (Note, this was a commentary and tutorial about Bayesian meta-analysis, not an actual meta-analysis. So some of the methodological details are somewhat weak.) We may want to look into a scenario where (a) a fixed effect meta-analysis is better justified, (b) the direction of the effects is consistent across studies, but (c) the statistical significance of the effects is not consistent across studies.
:::

What distinguishes meta-analysis from other types of reviews? Literature review typology, organized by your search, appraisal, synthesis, and analysis strategy (Grant and Booth 2009).

When many students hear the term "review," they probably have flashbacks to a time where they threw some search term into Google Scholar, downloaded a bunch of articles that looked interesting, spent a few hours reading those articles, and then wrote a summary of what has been learned from these studies. There's not necessarily anything wrong with this approach, but there are a few things that can be additionally learned from doing a meta-analysis.

Meta-analysis is a statistical technique for combining results from multiple studies.

In addition to reading articles, a researcher performing a meta-analysis would also extract information about the *effect size*.

After this information has been extracted, the researcher would perform an analysis where these effect sizes are combined in a single set of analyses. By combining information from multiple studies, the researcher can make a more precise estimate of the size of some effect. The researcher can also look at the extent to which this effect varies across studies. They may even them perform analyses indicating whether studies with certain characteristics systematically produce different results.

Think about how meta-analysis might give you a different impression of an area of study than a literature review. If you would have performed a literature review on studies examining the effect of social norm messaging on hotel towel re-use, you would have likely come away with the impression that there is an unreliable effect at best and no effect at worst. However, when statistically combining the results of these studies, a meta-analyst is able to see that the findings are more consistent than they appear at first glance--and that, when considering all the available evidence, it actually appears that social norm messaging does decrease hotel towel reuse. Considering that, for example, there were 1.7 billion overnight hotel bookings in the European Union alone in 2013 (Eurostat, 2015), the difference in the types of the conclusions formed by a literature review and a meta-analysis matter quite a bit!

Accident report: Money priming and vote counting.

- Reading the literature revisited: how to see the ‚Äúbones‚Äù of the study ‚Äì what is the manipulation, what‚Äôs the measure, how reliable and valid is it, how precise and generalizable is it, what are the inferences?
- How not to do a meta-analysis (approaches that are intuitive but wrong)
    - Don‚Äôt just count the significant positive studies (vote counting); don‚Äôt just average the estimates.
    
::: {.accident-report}
‚ö†Ô∏è Accident report: Money priming and vote counting. Vadillo et al. (2016) show a case where vote counting indicates that there is overwhelming support for money priming, but a meta-analysis reveals that the support is limited. 
:::

## Running example: The contact hypothesis

> [Introduce the applied example]


## Intuitive, but problematic, approaches to evidence synthesis

We have seen why it is important to synthesize evidence across studies on the same topic rather than just focus on one study at a time. How should we actually do this in practice? When considering evidence across multiple studies, what most of us do intuitively is simply to count how many studies supported the hypothesis under investigation, versus how many did not support the hypothesis. Such a count usually amounts to counting the number of studies with "significant" $p$-values, since (for better or for worse) "significance" is largely what drives studies' take-home conclusions (@mcshane2017statistical; @nelson1986interpretation). This is called "**vote-counting**" (@borenstein2021introduction). For example, in Paluck et al.'s meta-analysis on the contact hypothesis, 11 of 27 (EYEBALLED) studies were significant, all with positive point estimates. So, based on this vote-count, we would have the impression that most studies do not support the contact hypothesis. Indeed, many narrative reviews take essentially this vote-counting approach as well, albeit often not explicitly.

Despite its intuitive appeal, vote-counting can be very misleading because this approach characterizes evidence solely in terms of dichotomized $p$-values, while entirely ignoring effect sizes. In Chapter \@ref(replication) on replication, we saw how this fetishism of statistical "significance" can mislead in the context of individual studies, and these problems propagate to vote-counting statistical "significance" across studies as well. For example, it could be the case that relatively few studies are "significant" (for example, in a literature of typically small studies), yet if the studies typically have large point estimates, then the consensus of evidence could actually be quite strong. Or inversely, it could be that nearly all studies are "significant", but if they typically have small point estimates, then the consensus of evidence might still be rather weak. In these cases, vote-counting could lead us badly astray (@borenstein2021introduction).

To avoid these pitfalls, a principled evidence synthesis needs to account for effect sizes themselves. How about if we just average the studies' estimates? For example, in Paluck et al.'s meta-analysis, the mean of the studies' estimates is XXX. This approach is perhaps a step in the right direction, but still has some important limitations. In particular, this approach gives equal weight to each study, such that a small study (like Hull et al., with a sample size of XXX [note: this had the second-smallest RE weight]) contributes as much to the mean effect size as a large study (like Boisjoly et al., with a sample size of XXX [note: this had the highest RE weight]). That doesn't make sense: larger studies should clearly carry more weight in the analysis. This brings us to our first principled approach to meta-analysis, namely fixed-effects meta-analysis.

## Fixed-effects meta-analysis

Fixed-effects meta-analysis is a simple way to average studies' estimates while giving more weight to larger studies. Specifically, fixed-effects meta-analysis is a weighted average in which each study's estimate is weighted by its inverse-variance (i.e., the inverse of its squared standard error). This makes sense because larger, more precise studies have smaller variances, so they should get more weight in the analysis. The fixed-effects pooled estimate is: 

$$\widehat{\mu} = \frac{ \sum_{i=1}^k w_i \widehat{\theta}_i}{\sum_{i=1}^k w_i}$$
where $k$ is the number of studies, $\widehat{\theta}_i$ is the point estimate of the $i^{th}$ study, and $w_i = 1/\widehat{\sigma}^2_i$ is study $i$'s weight in the analysis (i.e., the inverse of its variance). 

> [Do we want to show the SE for $\widehat{\mu}$?]

### Applied example

In Paluck et al.'s meta-analysis, we would calculate the fixed-effects estimate, $\widehat{\mu}$, as:

>[SHOW ARITHMETIC FOR THE FIRST FEW STUDIES]. 

We thus estimate that the effect size in this studies is a standardized mean difference of $\widehat{\mu} = XXX$; 95\% confidence interval: [XXX]; $p=$XXX. That is, we estimated that intergroup contact was associated with a decrease in prejudice of XXX standard deviations.

### Limitations of fixed-effects meta-analysis

Fixed-effects meta-analysis does have important limitations. To see the problem intuitively, imagine we actually had the full dataset of individual observations from each study. (This is what meta-analysts dream of, but it's rarely feasible in practice.) Fixed-effects meta-analysis is like simply concatenating all the studies' datasets and then calculating the effect size, ignoring that observations came from different studies. 

But what if the studies differ from one another? For example, in Paluck et al.'s meta-analysis, many design elements differed in ways that could make the interventions more or less effective in different studies: some studies recruited adult participants while others recruited children, some studies considered contact between participants of different races while others considered contact between participants of different ages, and so on. This means that the contact effect could actually differ across studies: the effects could be **heterogeneous**.

Does this presence of heterogeneity remind you of anything from when we analyzed repeated-measures data in Chapter \@ref(models) on models? Recall that, with repeated-measures data, we had dealt with the possibility of heterogeneity across participants by introducing random intercepts by participant to our regression model. We can do much the same thing in meta-analysis; let's turn to this now.


## Random-effects meta-analysis

Whereas fixed-effects meta-analysis essentially postulates that all studies in the meta-analysis have the same population effect size, $\mu$, random-effects meta-analysis instead postulates that studies' population effects come from a normal distribution^[Technically, other specifications of random-effects meta-analysis are possible. For example, robust variance estimation does not require making assumptions about the distribution of effects across studies (@hedges2010robust).] with mean $\mu$ and standard deviation $\tau$. The larger the standard deviation $\tau$, the more heterogeneous the effects are across studies. A random-effects model then tries to estimate both $\mu$ and $\tau$, for example by maximum likelihood (@dersimonian1986meta; @brockwell2001comparison).  

In practice, the estimate $\widehat{\mu}$ is still a weighted average of studies' estimates, as for fixed-effects meta-analysis:

$$\widehat{\mu} = \frac{ \sum_{i=1}^k w_i \widehat{\theta}_i}{\sum_{i=1}^k w_i}$$
but the inverse-variance weights now incorporate the heterogeneity as well: $w_i = 1/\left(\widehat{\tau}^2 + \widehat{\sigma}^2_i \right)$. These weights represent the inverse of studies' *marginal* variances, comprising not only statistical error due to their finite sample sizes ($\widehat{\sigma}^2_i$) but also genuine effect heterogeneity ($\widehat{\tau}^2$).^[The estimate of $\widehat{\tau}^2$ is a bit more complicated, but is essentially a weighted average of studies' residuals, $\widehat{\theta_i} - \widehat{\mu}$, while subtracting away variation due to statistical error, $\widehat{\sigma}^2_i$ (@dersimonian1986meta; @brockwell2001comparison).]

The upshot is that small studies contribute more strongly to a random-effects meta-analysis (i.e., they have larger weights) than they do to a fixed-effects meta-analysis. Additionally, confidence intervals are typically wider in random-effects meta-analysis than in fixed-effects meta-analysis; heuristically, this is because random-effects meta-analysis has to draw inference to a distribution of heterogeneous effects rather than to only one population effect that is shared by all studies.


### Reporting on heterogeneity

Remember that in random-effects meta-analysis, the estimate $\widehat{\mu}$ represents only the *mean* population effect across studies. It tells us nothing about the heterogeneity. In practice, we would recommend always reporting the heterogeneity estimate $\widehat{\tau}$ as well, perhaps supplemented by other related metrics (@riley2011interpretation, @wang2019simple, @mathur_mam). Reporting on the heterogeneity can help indicate how consistent or inconsistent the effects are across studies, which may point to the need to investigate moderators of the effect.^[For example, one approach to investigate moderators in meta-analysis is meta-regression, in which moderators (e.g., type of intergroup contact) are included as covariates in a random-effects meta-analysis model. As in standard regression, coefficients can then be estimated for each moderator, representing the mean difference in population effect between studies with versus without the moderator.]  


### Applied example

Conducting a random-effects meta-analysis of Paluck et al.'s dataset yields $\widehat{\mu} = XXX$; 95\% confidence interval: [XXX]; $p=$XXX. That is, we estimated that, *on average across studies*, intergroup contact was associated with a decrease in prejudice of XXX standard deviations. Additionally, we estimated that the standard deviation of population effects was $\widehat{\tau}=XXX$; 95\% confidence interval: [XXX].

>[COULD INCLUDE A NORMAL DENSITY TO ILLUSTRATE THE FITTED DISTRIBUTION]


## Bias in meta-analysis

Meta-analysis is an invaluable tool to synthesize evidence across studies. However, meta-analyses can be compromised by two categories of bias: **within-study biases** and **across-study biases**. Either type of bias can lead to meta-analysis estimates that are too large, too small, or in the wrong direction. We will now discuss examples of each type of bias. We will also discuss ways to address these biases when conducting a meta-analysis, including mitigating the biases at the outset through sound meta-analysis design and also assessing the robustness of the ultimate conclusions to possible remaining bias.

### Within-study biases

Within-study biases affect the internal validity of the meta-analyzed studies' internal validity and include, for example, XXX (Chapter XXX; [use examples of types of bias that were discussed earlier]). For example, in Paluck et al.'s meta-analysis, XXX. 

These within-study biases can propagate to the results of the meta-analysis. To mitigate this problem, meta-analysts should try to define study eligibility criteria that reduce within-study biases and, after data have been collected, should qualitatively assess studies' risks of bias using established rating tools (@art). Additionally, it is often informative to conduct quantitative quantitatively assess how sensitive meta-analysis results may be to residual bias that cannot be eliminated by limiting study eligibility (@art).


### Across-study biases

Across-study biases affect the meta-analyzed literature holistically and include, for example, publication bias and selective reporting, as discussed in Chapter XXX. Often, these across-study biases favor statistically significant positive results; in this case, results that survive to publication and inclusion in the meta-analysis will typically have inflated estimates (they are too large compared to the truth) and so result in an inflated meta-analytic estimate. As was the case for within-study biases, meta-analysts should try to mitigate across-study biases both through prevention and . That is, meta-analysis inclusion criteria should be designed to cast a broad net, capturing not only high-profile, published studies on the topic, but also studies published in low-profile journals, and "gray literature" such as unpublished dissertations and theses. A broad search will also help capture not only studies in which the analysis that is relevant to the meta-analysis was primary "headline" finding, but also studies that reported the relevant analysis only as a secondary or supplemental result (perhaps because it wasn't "significant"!) (@sapbe). 

There are also statistical methods to help assess how robust the results may be to across-study biases. Among the most popular tools to assess and correct for publication bias is the **funnel plot** (@duval2000trim; @egger1997bias), a graph relating the meta-analyzed studies‚Äô point estimates to a measure of their precision, such as sample size or standard error. Funnel plots are usually interpreted as indicating publication bias when they are asymmetric, that is, when smaller studies tend to have larger point estimates. Several popular statistical methods, such as Trim-and-Fill (@duval2000trim) and Egger‚Äôs regression (@egger1997bias) are designed to quantify this type of asymmetry. Funnel plot methods can be useful to assess whether the effect sizes differ systematically between small and large studies. These **"small-study effects"** could arise not only from publication bias but also from genuine substantive differences between small and large studies (@egger1997bias; @lau2006case). For example, in a meta-analysis of intervention studies, if the most effective interventions are also the most expensive or difficult to implement, these highly effective interventions might be used primarily in the smallest studies. Funnel plot methods detect these types of small-study effects as well as those arising from publication bias.

[Now briefly talk about funnel plots, selection models, etc.]


::: {.exercise}
Exercises:


::: 

::: {.exercise}
Challenge exercises:

\begin{itemize}
\item Why do you think small studies receive more weight in random-effects meta-analysis than in fixed-effects meta-analysis? Can you see why this is true mathematically based on the equations given above, and can you also explain the intuition in English? 
\end{itemize}
::: 


::: {.case-study}
üî¨ Case study: Publication bias in the social sciences: Unlocking the file drawer (Franco, Malhotra, and Simonovits 2014)
:::

- Characterizing publication bias after the fact: funnel plot asymmetry, precision effects tests, weight-function modeling, sensitivity analyses


## FROM OUTLINE

Basic meta-analysis approaches

- Extracting effect sizes. Effect sizes for continuous data, binary data, and correlations (needs to come before applied example because it refers to standized mean differeneces)





::: {.accident-report}
‚ö†Ô∏è Accident report: Garbage in, garbage out? Meta-analyzing bad research (Coles et al. 2019)
:::
