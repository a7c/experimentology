<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Experimentology" />
<meta property="og:type" content="book" />





<meta name="author" content="Michael C. Frank, Mika Braginsky, Julie Cachia, Nicholas Coles, Tom Hardwick, Robert Hawkins, Maya Mathur, and Rondeline Williams" />



<meta name="description" content="Experimentology">

<title>Experimentology</title>

<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />




<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="toc/toc.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="part"><span><b>I Before you begin your experiment</b></span></li>
<li><a href="1-intro.html#intro"><span class="toc-section-number">1</span> Introduction: theory and experiments</a></li>
<li><a href="2-replication.html#replication"><span class="toc-section-number">2</span> Replication, reproducibility and transparency</a></li>
<li><a href="3-ethics.html#ethics"><span class="toc-section-number">3</span> Ethics</a></li>
<li class="part"><span><b>II Experimental design and planning</b></span></li>
<li><a href="4-measurement.html#measurement"><span class="toc-section-number">4</span> Measurement</a></li>
<li><a href="5-design.html#design"><span class="toc-section-number">5</span> Design of experiments</a></li>
<li><a href="6-inference.html#inference"><span class="toc-section-number">6</span> Statistical inference for comparing groups</a></li>
<li><a href="7-models.html#models"><span class="toc-section-number">7</span> Statistical models for more complex designs</a></li>
<li><a href="8-sampling.html#sampling"><span class="toc-section-number">8</span> Sampling</a></li>
<li><a href="9-preregistration.html#preregistration"><span class="toc-section-number">9</span> Preregistration</a></li>
<li class="part"><span><b>III Doing the experiment</b></span></li>
<li><a href="10-selection.html#selection"><span class="toc-section-number">10</span> Replicating or extending an existing study</a></li>
<li><a href="11-collection.html#collection"><span class="toc-section-number">11</span> Data collection</a></li>
<li><a href="12-management.html#management"><span class="toc-section-number">12</span> Data management</a></li>
<li class="part"><span><b>IV Analysis and reporting</b></span></li>
<li><a href="13-viz.html#viz"><span class="toc-section-number">13</span> Visualization</a></li>
<li><a href="14-eda.html#eda"><span class="toc-section-number">14</span> Exploratory data analysis</a></li>
<li><a href="15-writing.html#writing"><span class="toc-section-number">15</span> Reproducible writing</a></li>
<li class="part"><span><b>V Contextualizing your study</b></span></li>
<li><a href="16-meta.html#meta"><span class="toc-section-number">16</span> Meta-analysis</a></li>
<li><a href="17-conclusions.html#conclusions"><span class="toc-section-number">17</span> Conclusions</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="header">
<h1 class="title">Experimentology</h1>
<h3 class="subtitle"><em>An Open Science Approach to Experimental Psychology Methods</em></h3>
<h4 class="author"><em>Michael C. Frank, Mika Braginsky, Julie Cachia, Nicholas Coles, Tom Hardwick, Robert Hawkins, Maya Mathur, and Rondeline Williams</em></h4>
</div>
<div id="section" class="section level1 unlisted unnumbered">
<h1 class="unlisted unnumbered"></h1>
<div id="summary" class="section level2 unnumbered">
<h2>Summary</h2>
<p>How do we extract generalizable insights about the vast complexity of human behavior? The goal of this book is to provide an introduction to the workflow of the experimental researcher in the psychological sciences. The organization is sequential, from the planning stages of the research process through design, data collection, analysis, and reporting. We introduce these concepts via narrative examples from a range of sub-disciplines, including cognitive, developmental, and social psychology. Throughout, we also illustrate the pitfalls that led to the “replication crisis” in psychology. Across chapters, the book will emphasize four themes of successful experimental research: transparency, precision, bias reduction, and generalizability. The book takes an open-science based approach, providing readers with tutorials and justifications of practices such as preregistration, data sharing, and reproducible workflows. The audience for the book is graduate students, advanced undergraduates, or highly-motivated self-learners with some domain knowledge in psychology and a basic grasp of linear regression. For relevant chapters, examples will be presented using code from R (the free statistical programming language), Git (a version control system), and the Open Science Framework (a data sharing/preregistration site), and appendices will provide tutorials on these tools. The author group has collaborated for years in both teaching and research contexts and has broad expertise in experimental methods pedagogy as well as meta-science, statistical analysis and meta-analysis, and ethics. Our hope is that this book provides a practical introduction to how to do robust and replicable work as an experimental psychologist.</p>
</div>
<div id="introduction" class="section level2 unnumbered">
<h2>Introduction</h2>
<p>Lab in Experimental Methods (Psych 251) is the foundational course for incoming graduate students in the Stanford psychology department. For the last ten years, one of us (Frank) has taught this course and most of us (Hawkins, Cachia, Hardwicke, Mathur, Williams) have TA’d, taken, or otherwise contributed to the course. The goal is to orient students to the nuts and bolts of doing behavioral experiments, including how to plan and design a solid experiment and how to avoid common pitfalls regarding design, measurement, and sampling. Almost all of students’ coursework both before and in graduate school deals with the content of their research, including theories and results in their areas of focus. In contrast, the course is sometimes the only one that deals with the process of research, from big questions about why we do experiments and what it means to make a causal inference all the way to the tiny details of organization, like what to name your directories and how to make sure you don’t lose your data in a computer crash. This observation leads to our book’s title. “Experimentology” is not psychology, cognitive science, or any other body of content knowledge – but rather the set of practices, findings, and approaches that help to enable the construction of robust, precise, and generalizable experiments.</p>
<p>The centerpiece of the Lab in Experimental Methods course is a replication project, a model first described in <span class="citation"><label for="tufte-mn-1" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-1" class="margin-toggle">Frank and Saxe (2012)<span class="marginnote">Frank, Michael C, and Rebecca Saxe. 2012. <span>“Teaching Replication.”</span> <em>Perspectives on Psychological Science</em> 7: 595–99.</span></span> and later expanded on in <span class="citation"><label for="tufte-mn-2" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-2" class="margin-toggle">Hawkins et al. (2018)<span class="marginnote">Hawkins, Robert X D, Eric N Smith, Carolyn Au, Juan Miguel Arias, Rhia Catapano, Eric Hermann, Martin Keil, et al. 2018. <span>“Improving the Replicability of Psychological Science Through Pedagogy.”</span> <em>Advances in Methods and Practices in Psychological Science</em> 1 (1): 7–18.</span></span>. Each student chooses a published experiment in the literature and collects new data on a pre-registered version of the same experimental paradigm, comparing their result to the original publication. Over the course of the quarter, we walk through how to set up a replication experiment, how to pre-register confirmatory analyses, and how to write a reproducible report on the findings. The project provides numerous object lessons for teaching concepts like reliability and validity, which allow students to analyze choices that the original experimenters made – often choices that could have been made differently in hindsight!</p>
<p>At the end of the course, we reap the harvest of projects. The project presentations are a wonderful demonstration of both how much the students can accomplish in a quarter and also how tricky it can be to reproduce (redo calculations in the original data) and replicate (recover similar results in new data) the published literature. Often our replication rate for the course hovers just above 50%, an outcome that can be disturbing or distressing for students who assume that the published literature reports the absolute truth!</p>
<p>This finding motivates many discussions in class about the “replication crisis” <span class="citation">(<a href="#ref-osc2015" role="doc-biblioref"><strong>osc2015?</strong></a>)</span>. In this book, we will tell the story of the major shifts in psychology that have come about in the last ten years, including both the crisis narrative and the positive methodological reforms that have resulted from it. Using this story as motivation, we will highlight the importance of transparency during all aspects of the experimental process from planning to dissemination of materials, data, and code.</p>
</div>
<div id="what-this-book-is-and-isnt-about" class="section level2 unnumbered">
<h2>What this book is (and isn’t) about</h2>
</div>
<div id="themes" class="section level2 unnumbered">
<h2>Themes</h2>
<p>Transparency will be one major cross-cutting theme of the book, intertwined with three others: precision of measurement, bias reduction, and generalizability. Throughout chapters on measurement, pre-registration, sampling, and statistical analysis – to name a few – we will return to the important relationships between these four concepts, and how the decisions made by the experimenter at every stage of design, data collection, and analysis bear on the inferences that can be made about the results. Importantly, discussions of reproducibility and replicability have often proceeded without consideration of issues like precision, bias, or generalizability, leading to a number of deep critiques of the methodological reform movement that we will cover in detail.</p>
<p>Transparency: For experiments to be reproducible, other researchers need to be able to determine exactly what you did. Thus, every stage of the research process should be guided by a primary concern for transparency. For example, preregistration creates transparency into the researcher’s evolving expectations and thought processes; releasing open materials and analysis scripts creates transparency into the details of the procedure.</p>
<p>Precision of measurement: We want researchers to start planning an experiment by thinking “what causal effect do I want to measure” and to make their planning, sampling, design, and analytic choices to maximize the precision of this measurement. A downstream consequence of this mindset is that we move away from a focus on dichotomized inference, like p-value significance, and towards analytic and meta-analytic models that focus on continuous effect sizes and confidence intervals (Cumming 2014).</p>
<p>Bias reduction: Precision is not enough if the estimate is biased. In our samples, analyses, experimental designs, and in the literature, we need to think carefully about sources of bias in the quantity being estimated. This kind of thinking also reveals key weaknesses of dichotomous, “significance”-based reasoning.</p>
<p>Generalizability: Complex behaviors are rarely universal across all settings and populations, and any given experiment can only hope to cover a small slice of the possible conditions where the behavior takes place (Yarkoni 2020). Behavioral scientists must therefore consider the generalizability of their findings at every stage of the process, from stimulus selection, sampling procedures, and analytic methods, to how findings are reported.</p>
</div>
<div id="how-to-read-this-book" class="section level2 unnumbered">
<h2>How to read this book</h2>
<p>Each chapter of the book will start with a narrative case study in which we describe an experiment or series of experiments, using these to illustrate and motivate the specific issues that the chapter deals with, often with a focus on pitfalls or challenges in previous research. These will draw from a broad set of subfields. To give one example, in the chapter on preregistration, we will relate a case study we call “the tale of two RCTs” (randomized controlled trials). We describe a published critique of a prominent RCT of an educational app, where the analyses reported in the paper appeared to be post-hoc and would not have been convincing unless they were pre-registered (Frank 2016). But we then go on to describe how this observation came from our own experiences running a randomized trial of an educational intervention where our own hard work was undermined by the lack of a preregistration (Barner et al. 2016)! Overall we will be taking a critical look at the psychological literature, but will try to do so through the lens of our own failures.</p>
<p>In addition to narrative examples that begin the chapters, we hope to include several other instructional elements with specific examples. Throughout we will include “accident reports,” short cases drawn from the published literature where a particular practice led to an error or faulty inference. We will also include “ethics boxes” where we discuss ethical issues arising from the content of a chapter. Finally, in order to facilitate the use of this book for a course with a replication project, we will include a section at the end of each chapter on how students can apply chapter content to their own projects. In addition, an appendix for instructors will provide more details on these projects (including sample syllabi).</p>
<!-- Below we provide a detailed outline of the chapters of the book, which are organized into five main sections, mirroring the timeline of an experiment: 1) Before You Begin, 2) Design and Planning, 3) Doing the Experiment, 4) Analysis and Reporting, and 5) Contextualizing. Throughout, the goal is not to give comprehensive coverage of all technical topics but rather to provide an approach and workflow. We want to give students what they need to plan and execute their own study. Instead of enumerating different approaches, we will try to provide a single coherent – and often quite opinionated – perspective, using boxes and references to give pointers to more advanced materials or alternative approaches.  -->
<p><label for="tufte-mn-3" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-3" class="margin-toggle"><span class="marginnote"><span style="display: block;">This book has fun stuff going on in the margins!</span></span></p>
<p><label for="tufte-mn-4" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-4" class="margin-toggle"><span class="marginnote"><span style="display: block;"><img src="images/dog.jpeg"/></span></span></p>
<p>How to read the book: This book can be read as an engaging story about lessons drawn from the “replication crisis” and reforms that have taken place in the behavioral sciences in the wake of it. The book’s linear structure also roughly follows the curriculum of a graduate-level experimental methods course, making it well-suited for teaching, or for use as a reference book for self-study.</p>
</div>
<div id="the-software-toolkit-of-the-behavioral-researcher-and-of-this-book" class="section level2 unnumbered">
<h2>The software toolkit of the behavioral researcher (and of this book)</h2>
<p>Version control, R and the tidyverse, the Open Science Framework.
Workflow diagram: what tools we use in the life-cycle of a project.</p>
</div>
<div id="integrating-this-book-into-an-experimental-methods-course" class="section level2 unnumbered">
<h2>Integrating this book into an experimental methods course</h2>
<p>The project-based approach (argument for doing replication/reproducibility study as part of learning methods)
Each chapter ends with a mixture of discussion questions, exercises, and project milestones that can be integrated into course assignments.
We include links to appendices, references, and recurring boxes with ethical content and ‘accident reports’ from documented problems in the literature.</p>

</div>
</div>



<p style="text-align: center;">
<a href="1-intro.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="toc/toc.js"></script>


</body>
</html>
