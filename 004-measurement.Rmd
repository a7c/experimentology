# (PART) Experimental design and planning {-}

::: {learning-goals}
üçé Learning goals: 
* Summarize differences between measurement reliability and validity
* Reason about appropriate cognitive psychology measures; 
* Identify well-constructed survey questions.
:::

# Measurement {#measurement}

The goal of an experiment is to make a (maximally precise and unbiased) measurement of a particular causal effect of interest. In this next section of the book, we're going to try to figure out how to do that. This chapter focuses on the topic of measurement.^[As a topic, measurement is actually much less well-discussed in experimental contexts compared with, say, observational studies. As far as we can tell, this is a sociological fact, not a scientific one. No matter whether you can manipulate the world directly (as in an experiment) or whether you are doing observational or quasi-experimental research, good measurement is the name of the game.] To paraphrase @steven1946, measurement is the practice of putting numbers to things. 

No matter where you are working in the sciences, you need to measure things. If you're doing physics or chemistry, you need to be able to measure physical quantities; if you're doing biology you might measure populations or lifespan as well as a host of physical quantities. Proper measurement instruments are incredibly important for this kind of work.^[A lot could be said, of course, about the transformative value of better measurement instruments in the sciences -- PHILOSOPHY OF SCIENCE MEASUREMENT REFERENCES?] Psychology and the behavioral sciences are no different -- we need proper measurement instruments. The difference is that in psychology, we are typically trying to measure something that's inside the heads of our participants, which we call a **latent construct**. (See Chapter \@ref(theory)). 

Not all measurements are created equal. This point is obvious when you think about physical measurement instruments: a caliper will give you a much more precise estimate of the thickness of a small object than a ruler. One way to see that the measurement is more precise is by repeating it a bunch of times. The measurements from the caliper will likely be more similar to one another, reflecting the fact that the amount of error in each individual measurement is smaller. We can do the same thing with a psychological measurement -- repeat and assess variation -- though as we'll see below it's a little trickier. Measurement instruments that have less error are called more **reliable** instruments.^[Is **reliability** the same as **precision**? Yes, more or less. Confusingly, different fields call these concepts different things [there's a helpful table of these names in @brandmeier2018]. Here we'll talk about reliability as a property of instruments specifically while using the term precision to talk about the measurements themselves.]

When we have a physical quantity of interest, we can assess how well an instrument measures that quantity. But, as we saw in Chapter \@ref(theory), things are much trickier when the construct we are trying to measure can't be assessed directly. We have to measure something observable -- our **operationalization** of the construct -- and then make an argument about how it relates to the construct of interest. We call this argument an argument for the **validity** of the measure. 

These two concepts, reliability and validity, provide a conceptual toolkit for assessing how good a psychological measurement instrument is. Let's start by taking a look at an example of the challenge of measuring a particular latent construct, children's early language ability. We can use this example to understand the concepts of reliability and validity. 

::: {.case-study}
Anyone who has worked with little children or had children of their own can attest to how variable their early language is. Some children speak clearly and produce long sentences from an early age. Others struggle to produce words but clearly show evidence of understanding. And yet others show deficits in both producing and understanding language. Further, this variation appears to be linked to later outcomes -- children whose very early language processing is slower and whose vocabularies are smaller tend to do worse in school years later [@fernald2008]. Thus, there are many reasons why you'd want to make precise measurements of children's early language ability as a latent construct of interest.^[Of course, you can also ask if early language is a single construct, or whether it is multi-dimensional! For example, does grammar develop separately from vocabulary? It turns out the two are very closely coupled [@frank2021]. This point illustrates the general idea that, especially in psychology, measurement and theory building are intimately related -- you need data to inform your theory, but the measurement instruments you use to collect your data in turn presuppose some theory!]

As with all developmental research, there are many constraints on measurement that are imposed by the age of the children you want to work with. You can't give toddlers a multiple-choice test! So to measure early language ability (to be concrete, let's say for children under two and a half years old), you have roughly three options open. First, you can do some kind of observation of them and transcribe their language production -- this could be a play session in the lab or at home, with an experimenter or with a parent or other caregiver. Second, you could do some kind of direct assessment, e.g. by asking them to point or look at the referent of a word (e.g. "look at the kitty") and record their responses using video, a tablet, or even eye-tracking technology [@frank2016]. Or, you could ask their parents about their language, for example sending a questionnaire like the MacArthur Bates Communicative Development Inventory (CDI for short), which asks parents to mark words that their child says or understands. CDI forms are basically long checklists of words (and other items, which we'll ignore for now); the first page of an English form is shown in the margin.


```{marginfigure, echo=TRUE}
<img src="images/cdi.jpg"/>
The first page of the MacArthur-Bates Communicative Development Inventory, a parent report instrument for measuring children's early language.
```

To decide which of these methods to use for a specific study, we need to think through the properties of these measurement instruments -- both in terms of reliability and validity and also in terms of their practicality for a specific research situation. Practicalities matter! For example, observational measurement can be extremely costly in both time and money because it not only requires a visit of some sort (to the home or the lab) but also transcription of speech, which is quite time-consuming -- often taking 5-10 minutes of work to transcribe a single minute of speech. Direct assessment still requires a lab or home visit, but scoring is typically more straightforward. Finally, parent report -- (e)mailing a questionnaire to the parent -- is extremely time- and cost-effective. 

On the other hand, we wouldn't want to use CDI questionnaires as a tool if they were a bad measurement instrument. How can we tell? This is where assessment of reliability and validity are critical. In practice, the task of selecting and justifying a measurement instrument comes down to an argument about reliability and validity.


```{r measurement-reliability}
library(wordbankr)
ws <- wordbankr::get_instrument_data(language = "English (American)", 
                                     form = "WS", administrations = TRUE, 
                                     iteminfo = TRUE)

ws_mat <- ws |>
  filter(!longitudinal, type == "word") |>
  mutate(value = value == "produces", 
         even_item = ifelse(num_item_id %% 2 == 0, "even","odd")) |>
  group_by(data_id, even_item) |>
  summarise(prop = mean(value)) |>
  pivot_wider(names_from = even_item, values_from = prop) 

n <- length(unique(ws_mat$data_id))
test_retest <- cor.test(ws_mat$even, ws_mat$odd)$estimate
```

How reliable is the CDI? As we'll discuss more below, there's no single answer to this question. Not only are there multiple ways to compute reliability, but also reliability in practice is going to depend on the population being measured, the fidelity with which the instrument is administered, and other factors. That said, a quick and dirty thing we can do is called a **split half correlation**. We can take a dataset of CDI data [from Wordbank; @frank2017] and split each test in half. Since there are 680 words on the CDI, that means we pretend that each of our `r n` participants took two versions of the CDI, each with 340 items -- one test consists of only the even-numbered items and the other is the odd numbered items. Then we just compute the correlation between each participant's score on the even CDI and the odd CDI. The resulting correlation is very high: $r = `r round(test_retest, digits = 3)`$. So that gives us a sense that CDIs are pretty reliable. 


```{marginfigure, echo=TRUE}
<img src="images/psycho-cors.png"/>
Longitudinal correlations between a child's score on one administration of the CDI and another one several months later. 
```

On the other hand, a stronger test of reliability is a **test-retest** correlation. Our split-half number only tells us about what was happening within a single administration session, but lots of things vary between administration sessions. For example, maybe the child was having a bad day and not producing as much complex language. Comparing correlations across different days removes this source of correlation -- at a cost. The longer you wait between observations the more the child has changed! The figure in the margin shows longitudinal test-retest correlations for two CDIs, showing how correlations start off high and drop off as the gap between observations increases [@frank2021]. Overall, this evidence is comforting. It looks like the CDI shows good reliability ($>.9$) across several methods. 

Given that CDI forms are relatively reliable instruments, are they valid? Well, as a starting point, they certainly have reasonable **face validity** -- they look like they are measuring the construct that they purport to measure. They also arguably have some **ecological validity** in that they measure the child's language (as observed by the parent) in their day-to-day experiences, rather than in a particular lab situation. But how well do they really measure the construct of interest, namely children's early language ability? 


```{marginfigure, echo=TRUE}
<img src="images/cdi-validity.png"/>
Relations between an early form of the CDI (the ELI) and several other measurements of children's early language from both transcripts and direct assessments. Arrows indicate loadings of different measures (boxes) onto a single construct (circle). Numbers give standardized weights [@bornstein1998].
```

A study by @bornstein1998 answered this question via a common measure validation strategy: concurrently administering a variety of measures that are hypothesized to relate to the same construct. The figure in the the margin shows the results of a structural equation model that measures the shared variance between a variety of different measures (critically including examples of the other two methods of assessment we discussed, direct assessment and observational, transcript-based assessments) and a single hypothesized central construct. The relation between the ELI (an early version of the CDI) and the central construct is quite strong: the ELI score correlated closely with the shared variance among all the different measures. Taken together with the reliability evidence, this kind of **concurrent validity** evidence suggests that, if you want to measure early language, the CDI is a pretty good way to do so. 

The story of the CDI is a success story -- it's a relatively inexpensive measure that has some evidence for both reliability and validity. We should celebrate (and also use it as a potential outcome measure in our studies). But there is also plenty more work to do! A critic could very reasonably point out that we haven't shown any evidence that reliability and validity extends across different populations or ages. For any measure, it's important to start by asking whether there is *any* evidence for reliability and validity. But once you have a specific target population in mind, you can also ask how likely it is that the measure will yield reliable and valid data with *that particular population*. Oftentimes you yourself will have to do this measurement work, "checking" that your instruments function appropriately in the particular situation you want to use them. This is sometimes a lot of hard work, but it's an important part of doing good experimental measurement!
:::


## Measurement Reliability


```{marginfigure, echo=TRUE}
<img src="images/elife-35718-fig1-v2.jpg"/>
Precision and bias visualized. The expected precision of measurements from an instrument is its reliabilty. The bias of measurements from an instrument also provide a metaphor for its validity. Figure from @brandmeier2018.
```

Having looked at the CDI as an example, we can now talk more generally about what it means for a measure to be reliable. Reliability is a way of describing the extent to which a measure yields signal relative to noise. Intuitively, if there's less noise, then there will be more similarity between different measurements of the same quantity.^[As the graphic above illustrates, a reliable measure that yields high-precision observations can still be highly biased. We'll talk about bias in a moment when we discuss validity.] Here's the trouble. How do we measure signal and noise? 

In the physical sciences, it's common to measure the precision of an instrument by quantifying its coefficient of variation [@brandmeier2018]: 

$$CV = \frac{\sigma}{\mu}$$
\noindent where $\sigma$ is the standard deviation of the measurements and $\mu$ is the mean of the measurements. Imagine we measure the height of a person five times, resulting in measurements of 171cm, 172cm, 171cm, 173cm, and 172cm. Now we can use these measurements to compute the coefficient of variation, which is `r round(sd(c(171,172,171,173,172))/mean(c(171,172,171,173,172)),digits =3)`. Why can't we just do that with psychological measurements? 

Thinking about this question takes us on a detour through the different kinds of measurement scales used in psychological research [@stevens1946]. The height measurements in our example are on what is known as a **ratio** scale: a scale in which numerical measurements are equally spaced and on which there is a true zero point. These scales are common for physical quantities but actually quite infrequent in psychology. More common are **interval** scales, in which there is no true zero point. For example, IQ (and other standardized scores) are intended to capture interval variation on some dimension but 0 is meaningless -- an IQ of 0 does not correspond to any particular interpretation. 

**Ordinal** scales are also commonly used. These are scales that are ordered but are not necessarily spaced equally. For example, levels of educational achievement ("Elementary","High school","Some college","College","Graduate school") are ordered, but there is no sense in which "High school" is as far from "Elementary" as "Graduate school" is from "College." The last type in @stevens1946's hierarchy is **nominal** scales, in which no ordering is possible either. For example, race is an unordered scale in which multiple categories are present but there is no inherent ordering of these categories. The full hierarchi

Critically, different summary measures are possible for each scale type, as shown in Table \@ref(tab:measurement-stevens-table).



```{r measurement-stevens-table}
stevens <- tribble(~Scale, ~Definition, ~Operations, ~Statistics, 
        "Nominal", "Unordered list","Equality","Mode",
        "Ordinal","Ordered list","Greater than or less than", "Median",
        "Interval", "Numerical","Equality of intervals", "Mean, SD",
        "Ratio", "Numerical with zero", "Equality of ratios","Coefficient of variation")

knitr::kable(stevens, caption = "Stevens (1946) table of scale types and their associated operations and statistics.", booktabs = TRUE)
```




This definition seems straightforward, but it conceals some complexity around two issues. First, what does it mean to measure the same quantity more than once? This question has an obvious answer for physical characteristics -- imagine using a ruler to measure your height twice -- but is much more subtle for psychological constructs. After all, you shouldn't give the same test to your students twice and expect to see the same scores the second time! Second, 



### Computing reliability

Test-retest. Variance related to the test-taker‚Äôs performance. 	
Inter-rater. Variance related to the measurement method.

### Reliability paradoxes

https://lucklab.ucdavis.edu/blog/2019/2/19/reliability-and-precision



## Measurement Validity

In Chapter \@ref(theory), we talked about the process of theory building. 

the "game of psychological science" as the enterprise of connecting operationalizations to constructs. 

Does the measure relate to the construct? Classic concurrent and predictive validation strategies.

Face and ecological validity.
The nomological network (Cronbach and Meehl 1955). In other words, the measure is valid if it fits into the theory and is supported by other aspects of the theory.


### Bias

## How to design a good measure?

We have discussed that ideally you want a measure that is reliable and valid. How do you get one? 


Use someone else's measure. Perhaps they have done the hard work of validating. 

If you don't use someone else's measure, you'll need to make one up yourself. 

What's the thing you're measuring?

### Measure types



Data types: Stevens (1946) framework.
Classic cognitive psychology measures: Forced choices and reaction times
Likert scales and asking good survey-style questions.

The promise and perils of open-ended measures. 

## Survey measures



## Conclusion

In olden times, all the psychologists went to the same conferences and worried about the same things. But then a split formed between different groups. Educational psychologists and psychometricians knew that different problems on tests had different measurement properties, and began exploring how to select good and bad items, and how to figure out people's ability abstracted away from specific items. Cognitive psychologists, on the other hand, spurned this item-level variation and embraced the dogma of exchangeable experimental items.

People did Lots Of Trials, all generated from the same basic template. The sumscore^[The sumscore is just what we normal psychologists call "percent correct" -‚Äì treating the sum of your correct answers on the test as your score, as opposed to inferring the latent trait (ability) from the performance on the observed variables.] reigned supreme, and yielded important insight into Memory, Attention, and Reasoning (irrespective of what was being remembered, attended to, or reasoned about). 

Psychophysicists diverged from the cognitivist hierarchy. They always knew that they needed to infer a latent relationship. As they got better at doing this, they fit models that included parameters of the decision process (for example, a "lapse" parameter to capture inattention) as well as the quantities of interest. And because they typically fit these curves within individual subjects, these parameters were participant-level estimates. But the models that fit these curves were often specific to particular metric relationships and not appropriate for increasingly complicated domains.

Now in modern cognitive science, we get work on sophisticated constructs ‚Äì for example, in moral psychology or psycholinguistics ‚Äì where experimenters break with the cognitivist dogma and use non-exchangeable items. Sometimes items are sentences or even whole vignettes. Yet for the most part these researchers have forgotten to model item variation (except occasionally using a random intercept for items in their linear mixed effects models). @clark1973 scolded them about the problematic statistical inferences that could result from forgetting to model items and this guidance has reappeared in recent exhortations to Keep It Maximal! [@barr2013]. But as far as I can tell, no one really talks about modeling items in more detail *in order to learn more about what is in people's heads*.
