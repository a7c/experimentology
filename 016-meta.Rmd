# (PART) Contextualizing your study {-}

# Meta-analysis {#meta}

::: {.learning-goals}
🍎 Learning goals: practice being a critical reader of the literature; extract effect sizes; conduct a simple random-effects meta analysis; monitor within- and across-study biases that can decrease precision and increase bias. 
:::

::: {.case-study}
🔬 Case study: Hotel towel reuse (Scheibehenne, Jamil, and Wagenmakers 2016). A simple example of aggregating noisy evidence from replications. 
:::

What distinguishes meta-analysis from other types of reviews? Literature review typology, organized by your search, appraisal, synthesis, and analysis strategy (Grant and Booth 2009).

- Reading the literature revisited: how to see the “bones” of the study – what is the manipulation, what’s the measure, how reliable and valid is it, how precise and generalizable is it, what are the inferences?
- How not to do a meta-analysis (approaches that are intuitive but wrong)
    - Don’t just count the significant positive studies (vote counting); don’t just average the estimates.
    
::: {.accident-report}
⚠️ Accident report: Money priming and vote counting. Vadillo et al. (2016) show a case where vote counting indicates that there is overwhelming support for money priming, but a meta-analysis reveals that the support is limited. 
:::

Basic meta-analysis approaches

- Extracting effect sizes. Effect sizes for continuous data, binary data, and correlations.
- Fixed and random effects meta-analysis. Distinction between fixed and random-effects meta-analysis, assumptions of both models, and the meaning of their summary effects.
- Heterogeneity as a feature rather than a bug; summarizing effect sizes in the presence of heterogeneity. 
- Meta-regression. Predicting heterogeneity by examining continuous and categorical moderators of effects.

Within-study biases 

- Selection bias, performance bias, detection bias, attrition bias, reporting bias (Mathur and VanderWeele, n.d.)
- Limiting within-study biases at the outset by carefully defining methodological inclusion criteria
- Characterizing within-study biases after the fact: the Cochrane Risk of Bias tool.

Across-study biases

::: {.case-study}
🔬 Case study: Publication bias in the social sciences: Unlocking the file drawer (Franco, Malhotra, and Simonovits 2014)
:::

- Limiting across-study biases at the outset by casting a broad net, including grey literature
- Characterizing publication bias after the fact: funnel plot asymmetry, precision effects tests, weight-function modeling, sensitivity analyses

::: {.accident-report}
⚠️ Accident report: Garbage in, garbage out? Meta-analyzing bad research (Coles et al. 2019)
:::
