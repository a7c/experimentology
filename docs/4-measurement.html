<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 4 Measurement | Experimental Methods in the Behavioral and Cognitive Sciences" />
<meta property="og:type" content="book" />





<meta name="author" content="Michael C. Frank et al" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Chapter 4 Measurement | Experimental Methods in the Behavioral and Cognitive Sciences">

<title>Chapter 4 Measurement | Experimental Methods in the Behavioral and Cognitive Sciences</title>

<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />





<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="toc/toc.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="part"><span><b>I Before you begin your experiment</b></span></li>
<li><a href="1-intro.html#intro"><span class="toc-section-number">1</span> Introduction: theory and experiments</a></li>
<li><a href="2-replication.html#replication"><span class="toc-section-number">2</span> Replication, reproducibility and transparency</a></li>
<li><a href="3-ethics.html#ethics"><span class="toc-section-number">3</span> Ethics</a></li>
<li class="part"><span><b>II Experimental design and planning</b></span></li>
<li><a href="4-measurement.html#measurement"><span class="toc-section-number">4</span> Measurement</a></li>
<li><a href="5-design.html#design"><span class="toc-section-number">5</span> Design of experiments</a></li>
<li><a href="6-inference.html#inference"><span class="toc-section-number">6</span> Statistical inference for comparing groups</a></li>
<li><a href="7-models.html#models"><span class="toc-section-number">7</span> Statistical models for more complex designs</a></li>
<li><a href="8-sampling.html#sampling"><span class="toc-section-number">8</span> Sampling</a></li>
<li><a href="9-preregistration.html#preregistration"><span class="toc-section-number">9</span> Preregistration</a></li>
<li class="part"><span><b>III Doing the experiment</b></span></li>
<li><a href="10-selection.html#selection"><span class="toc-section-number">10</span> Replicating or extending an existing study</a></li>
<li><a href="11-collection.html#collection"><span class="toc-section-number">11</span> Data collection</a></li>
<li><a href="12-management.html#management"><span class="toc-section-number">12</span> Data management</a></li>
<li class="part"><span><b>IV Analysis and reporting</b></span></li>
<li><a href="13-viz.html#viz"><span class="toc-section-number">13</span> Visualization</a></li>
<li><a href="14-eda.html#eda"><span class="toc-section-number">14</span> Exploratory data analysis</a></li>
<li><a href="15-writing.html#writing"><span class="toc-section-number">15</span> Reproducible writing</a></li>
<li class="part"><span><b>V Contextualizing your study</b></span></li>
<li><a href="16-meta.html#meta"><span class="toc-section-number">16</span> Meta-analysis</a></li>
<li><a href="17-conclusions.html#conclusions"><span class="toc-section-number">17</span> Conclusions</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="measurement" class="section level1" number="4">
<h1><span class="header-section-number">Chapter 4</span> Measurement</h1>
<p>The goal of an experiment is to make a (maximally precise and unbiased) measurement of a particular causal effect of interest. In this next section of the book, we’re going to try to figure out how to do that. This chapter focuses on the topic of measurement.<label for="tufte-sn-1" class="margin-toggle sidenote-number">1</label><input type="checkbox" id="tufte-sn-1" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">1</span> As a topic, measurement is actually much less well-discussed in experimental contexts compared with, say, observational studies. As far as we can tell, this is a sociological fact, not a scientific one. No matter whether you can manipulate the world directly (as in an experiment) or whether you are doing observational or quasi-experimental research, good measurement is the name of the game.</span></p>
<p>No matter where you are working in the sciences, you need to measure things. If you’re doing physics or chemistry, you need to be able to measure physical quantities; if you’re doing biology you might measure populations or lifespan as well as a host of physical quantities. Proper measurement instruments are incredibly important for this kind of work.<label for="tufte-sn-2" class="margin-toggle sidenote-number">2</label><input type="checkbox" id="tufte-sn-2" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">2</span> A lot could be said, of course, about the transformative value of better measurement instruments in the sciences – PHILOSOPHY OF SCIENCE MEASUREMENT REFERENCES?</span> Psychology and the behavioral sciences are no different – we need proper measurement instruments. The difference is that in psychology, we are typically trying to measure something that’s inside the heads of our participants, which we call a <strong>latent construct</strong>. (See Chapter <a href="#theory"><strong>??</strong></a>).</p>
<p>Not all measurements are created equal. This point is obvious when you think about physical measurement instruments: a caliper will give you a much more precise estimate of the thickness of a small object than a ruler. One way to see that the measurement is more precise is by repeating it a bunch of times. The measurements from the caliper will likely be more similar to one another, reflecting the fact that the amount of error in each individual measurement is smaller. We can do the same thing with a psychological measurement – repeat and assess variation – though as we’ll see below it’s a little trickier. Measurement instruments that have less error are called more <strong>reliable</strong> instruments.<label for="tufte-sn-3" class="margin-toggle sidenote-number">3</label><input type="checkbox" id="tufte-sn-3" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">3</span> Is <strong>reliability</strong> the same as <strong>precision</strong>? Yes, more or less. Reliability is a property of the instrument while precision is a property of a specific set of measurements using that instrument. In other words, to say that an instrument is reliable is just to say that it typically yields precise measurements.</span></p>
<p>When we have a physical quantity of interest, we can assess how well an instrument measures that quantity. But, as we saw in Chapter <a href="#theory"><strong>??</strong></a>, things are much trickier when the construct we are trying to measure can’t be assessed directly. We have to measure something observable – our <strong>operationalization</strong> of the construct – and then make an argument about how it relates to the construct of interest. We call this argument an argument for the <strong>validity</strong> of the measure.</p>
<p>These two concepts, reliability and validity, provide a conceptual toolkit for assessing how good a psychological measurement instrument is. Let’s start by taking a look at an example of the challenge of measuring a particular latent construct, children’s early language ability. We can use this example to understand the concepts of reliability and validity.</p>
<div class="case-study">
<p>Anyone who has worked with little children or had children of their own can attest to how variable their early language is. Some children speak clearly and produce long sentences from an early age. Others struggle to produce words but clearly show evidence of understanding. And yet others show deficits in both producing and understanding language. Further, this variation appears to be linked to later outcomes – children whose very early language processing is slower and whose vocabularies are smaller tend to do worse in school years later <span class="citation">(<a href="#ref-fernald2008" role="doc-biblioref"><strong>fernald2008?</strong></a>)</span>. Thus, there are many reasons why you’d want to make precise measurements of children’s early language ability as a latent construct of interest.<label for="tufte-sn-4" class="margin-toggle sidenote-number">4</label><input type="checkbox" id="tufte-sn-4" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">4</span> Of course, you can also ask if early language is a single construct, or whether it is multi-dimensional! For example, does grammar develop separately from vocabulary? It turns out the two are very closely coupled <span class="citation">(<a href="#ref-frank2021" role="doc-biblioref"><strong>frank2021?</strong></a>)</span>. This point illustrates the general idea that, especially in psychology, measurement and theory building are intimately related – you need data to inform your theory, but the measurement instruments you use to collect your data in turn presuppose some theory!</span></p>
<p>As with all developmental research, there are many constraints on measurement that are imposed by the age of the children you want to work with. You can’t give toddlers a multiple-choice test! So to measure early language ability (to be concrete, let’s say for children under two and a half years old), you have roughly three options open. First, you can do some kind of observation of them and transcribe their language production – this could be a play session in the lab or at home, with an experimenter or with a parent or other caregiver. Second, you could do some kind of direct assessment, e.g. by asking them to point or look at the referent of a word (e.g. “look at the kitty”) and record their responses using video, a tablet, or even eye-tracking technology <span class="citation">(<a href="#ref-frank2016" role="doc-biblioref"><strong>frank2016?</strong></a>)</span>. Or, you could ask their parents about their language, for example sending a questionnaire like the MacArthur Bates Communicative Development Inventory (CDI for short), which asks parents to mark words that their child says or understands. CDI forms are basically long checklists of words (and other items, which we’ll ignore for now); the first page of an English form is shown in the margin.</p>
<p><label for="tufte-mn-4" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-4" class="margin-toggle"><span class="marginnote"><span style="display: block;"><img src="images/cdi.jpg"/> The first page of the MacArthur-Bates Communicative Development Inventory, a parent report instrument for measuring children’s early language.</span></span></p>
<p>To decide which of these methods to use for a specific study, we need to think through the properties of these measurement instruments – both in terms of reliability and validity and also in terms of their practicality for a specific research situation. Practicalities matter! For example, observational measurement can be extremely costly in both time and money because it not only requires a visit of some sort (to the home or the lab) but also transcription of speech, which is quite time-consuming – often taking 5-10 minutes of work to transcribe a single minute of speech. Direct assessment still requires a lab or home visit, but scoring is typically more straightforward. Finally, parent report – (e)mailing a questionnaire to the parent – is extremely time- and cost-effective.</p>
<p>On the other hand, we wouldn’t want to use CDI questionnaires as a tool if they were a bad measurement instrument. How can we tell? This is where assessment of reliability and validity are critical. In practice, the task of selecting and justifying a measurement instrument comes down to an argument about reliability and validity.</p>
<p>How reliable is the CDI? As we’ll discuss more below, there’s no single answer to this question. Not only are there multiple ways to compute reliability, but also reliability in practice is going to depend on the population being measured, the fidelity with which the instrument is administered, and other factors. That said, a quick and dirty thing we can do is called a <strong>split half correlation</strong>. We can take a dataset of CDI data [from Wordbank; <span class="citation">(<a href="#ref-frank2017" role="doc-biblioref"><strong>frank2017?</strong></a>)</span>] and split each test in half. Since there are 680 words on the CDI, that means we pretend that each of our 4214 participants took two versions of the CDI, each with 340 items – one test consists of only the even-numbered items and the other is the odd numbered items. Then we just compute the correlation between each participant’s score on the even CDI and the odd CDI. The resulting correlation is very high: <span class="math inline">\(r = 0.997\)</span>. So that gives us a sense that CDIs are pretty reliable.</p>
<p><label for="tufte-mn-5" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-5" class="margin-toggle"><span class="marginnote"><span style="display: block;"><img src="images/psycho-cors.png"/> Longitudinal correlations between a child’s score on one administration of the CDI and another one several months later.</span></span></p>
<p>On the other hand, a stronger test of reliability is a <strong>test-retest</strong> correlation. Our split-half number only tells us about what was happening within a single administration session, but lots of things vary between administration sessions. For example, maybe the child was having a bad day and not producing as much complex language. Comparing correlations across different days removes this source of correlation – at a cost. The longer you wait between observations the more the child has changed! The figure in the margin shows longitudinal test-retest correlations for two CDIs, showing how correlations start off high and drop off as the gap between observations increases <span class="citation">(<a href="#ref-frank2021" role="doc-biblioref"><strong>frank2021?</strong></a>)</span>. Overall, this evidence is comforting. It looks like the CDI shows good reliability (<span class="math inline">\(&gt;.9\)</span>) across several methods.</p>
<p>Given that CDI forms are relatively reliable instruments, are they valid? Well, as a starting point, they certainly have reasonable <strong>face validity</strong> – they look like they are measuring the construct that they purport to measure. They also arguably have some <strong>ecological validity</strong> in that they measure the child’s language (as observed by the parent) in their day-to-day experiences, rather than in a particular lab situation. But how well do they really measure the construct of interest, namely children’s early language ability?</p>
<p><label for="tufte-mn-6" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-6" class="margin-toggle"><span class="marginnote"><span style="display: block;"><img src="images/cdi-validity.png"/> Relations between an early form of the CDI (the ELI) and several other measurements of children’s early language from both transcripts and direct assessments. Arrows indicate loadings of different measures (boxes) onto a single construct (circle). Numbers give standardized weights <span class="citation"><span class="citation">(<a href="#ref-bornstein1998" role="doc-biblioref"><strong>bornstein1998?</strong></a>)</span></span>.</span></span></p>
<p>A study by <span class="citation">(<a href="#ref-bornstein1998" role="doc-biblioref"><strong>bornstein1998?</strong></a>)</span> answered this question via a common measure validation strategy: concurrently administering a variety of measures that are hypothesized to relate to the same construct. The figure in the the margin shows the results of a structural equation model that measures the shared variance between a variety of different measures (critically including examples of the other two methods of assessment we discussed, direct assessment and observational, transcript-based assessments) and a single hypothesized central construct. The relation between the ELI (an early version of the CDI) and the central construct is quite strong: the ELI score correlated closely with the shared variance among all the different measures. Taken together with the reliability evidence, this kind of <strong>concurrent validity</strong> evidence suggests that, if you want to measure early language, the CDI is a pretty good way to do so.</p>
<p>The story of the CDI is a success story – it’s a relatively inexpensive measure that has some evidence for both reliability and validity. We should celebrate (and also use it as a potential outcome measure in our studies). But there is also plenty more work to do! A critic could very reasonably point out that we haven’t shown any evidence that reliability and validity extends across different populations or ages. For any measure, it’s important to start by asking whether there is <em>any</em> evidence for reliability and validity. But once you have a specific target population in mind, you can also ask how likely it is that the measure will yield reliable and valid data with <em>that particular population</em>. Oftentimes you yourself will have to do this measurement work, “checking” that your instruments function appropriately in the particular situation you want to use them. This is sometimes a lot of hard work, but it’s an important part of doing good experimental measurement!</p>
</div>
<div id="measurement-validity" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Measurement Validity</h2>
<p>In Chapter <a href="#theory"><strong>??</strong></a>, we talked about the “game of psychological science” as</p>
<p>Does the measure relate to the construct? Classic concurrent and predictive validation strategies.</p>
<p>Face and ecological validity.
The nomological network (Cronbach and Meehl 1955). In other words, the measure is valid if it fits into the theory and is supported by other aspects of the theory.</p>
</div>
<div id="measurement-reliability" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Measurement Reliability</h2>
<div id="computing-reliability" class="section level3" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Computing reliability</h3>
<p>Test-retest. Variance related to the test-taker’s performance.<br />
Inter-rater. Variance related to the measurement method.</p>
</div>
<div id="reliability-paradoxes" class="section level3" number="4.2.2">
<h3><span class="header-section-number">4.2.2</span> Reliability paradoxes</h3>
<p><a href="https://lucklab.ucdavis.edu/blog/2019/2/19/reliability-and-precision" class="uri">https://lucklab.ucdavis.edu/blog/2019/2/19/reliability-and-precision</a></p>
</div>
</div>
<div id="design-of-measures" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Design of measures</h2>
<p>Data types: Stevens (1946) framework.
Classic cognitive psychology measures: Forced choices and reaction times
Likert scales and asking good survey-style questions.</p>
<p>The promise and perils of open-ended measures.</p>
</div>
<div id="survey-measures" class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> Survey measures</h2>
</div>
<div id="conclusion" class="section level2" number="4.5">
<h2><span class="header-section-number">4.5</span> Conclusion</h2>
<p>In olden times, all the psychologists went to the same conferences and worried about the same things. But then a split formed between different groups. Educational psychologists and psychometricians knew that different problems on tests had different measurement properties, and began exploring how to select good and bad items, and how to figure out people’s ability abstracted away from specific items. Cognitive psychologists, on the other hand, spurned this item-level variation and embraced the dogma of exchangeable experimental items.</p>
<p>People did Lots Of Trials, all generated from the same basic template. The sumscore<label for="tufte-sn-5" class="margin-toggle sidenote-number">5</label><input type="checkbox" id="tufte-sn-5" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">5</span> The sumscore is just what we normal psychologists call “percent correct” -– treating the sum of your correct answers on the test as your score, as opposed to inferring the latent trait (ability) from the performance on the observed variables.</span> reigned supreme, and yielded important insight into Memory, Attention, and Reasoning (irrespective of what was being remembered, attended to, or reasoned about).</p>
<p>Psychophysicists diverged from the cognitivist hierarchy. They always knew that they needed to infer a latent relationship. As they got better at doing this, they fit models that included parameters of the decision process (for example, a “lapse” parameter to capture inattention) as well as the quantities of interest. And because they typically fit these curves within individual subjects, these parameters were participant-level estimates. But the models that fit these curves were often specific to particular metric relationships and not appropriate for increasingly complicated domains.</p>
<p>Now in modern cognitive science, we get work on sophisticated constructs – for example, in moral psychology or psycholinguistics – where experimenters break with the cognitivist dogma and use non-exchangeable items. Sometimes items are sentences or even whole vignettes. Yet for the most part these researchers have forgotten to model item variation (except occasionally using a random intercept for items in their linear mixed effects models). <span class="citation">(<a href="#ref-clark1973" role="doc-biblioref"><strong>clark1973?</strong></a>)</span> scolded them about the problematic statistical inferences that could result from forgetting to model items and this guidance has reappeared in recent exhortations to Keep It Maximal! <span class="citation">(<a href="#ref-barr2013" role="doc-biblioref"><strong>barr2013?</strong></a>)</span>. But as far as I can tell, no one really talks about modeling items in more detail <em>in order to learn more about what is in people’s heads</em>.</p>

</div>
</div>
<p style="text-align: center;">
<a href="3-ethics.html"><button class="btn btn-default">Previous</button></a>
<a href="5-design.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="toc/toc.js"></script>


</body>
</html>
