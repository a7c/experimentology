# Replication, reproducibility, and transparency  {#replication}

::: {.learning-goals}
üçé Learning goals: 

* Understand the "crisis" narrative in psychology and the empirical evidence supporting it
* Define the distinction between reproducibility and replicability
* Consider types of replication 
* Reason about the relation of replication to theory building
:::

In the previous chapter, we gave a sober and considered introduction to the topic of experiments, their connection with causal inference, and their role in building psychological theory. In this chapter we're going to change gears a little bit and tell the story of the period from 2011 -- 2021 and how it has given rise to a number of "crisis" narratives in psychology. 

In order to set the terms of discussion, we need to describe some of the outcomes we are interested in. Figure \@ref(fig:replication-terms) gives us a basic starting point for our definitions.^[These terms have been a bit of a problem for the field, at least at first, but it seems like people have been agreeing on them recently.] For some claim in a paper, if we can take the same data that were analyzed in that paper, do the same analysis, and get the same result, we call that result **reproducible** (sometimes, **analytically* or **computationally reproducible**). If we can collect new data in the same experiment, do the same analysis, and get the same result, we call that a **replication** and say that the experiment is **replicable**. If we can do a different analysis with the original dataset, we call this a **robustness check** and so if a claim passes it is **robust**.^[These are less common in experimental psychology, but are very common in fields that work with large, complex observational datasets like sociology.] And if the same finding obtains in a different population, perhaps with a different analysis, the result is likely to be more **generalizable** beyond the initial conditions in which it was observed.^[You might have observed that a lot of work is being done here by the word "same." How do we operationalize same-ness for experimental procedures, statistical analyses, or samples? These are difficult questions that we'll address in part below, but there's no single answer and so these terms are always going to helpful guides rather than exact labels.]


```{r replication-terms, fig.cap="A terminological framework for meta-science discussions. Based on [https://figshare.com/articles/Publishing_a_reproducible_paper/5440621]().", fig.margin=TRUE}
knitr::include_graphics("images/replication/terms.png")
```

We're also going to abandon the sober tone of the introduction and try to get you a bit worked up. From an empirical perspective, things have been far from ideal in the psychology literature. Many classic findings may be wrong, or at least overstated. Their statistical tests are probably not trustworthy. The actual numbers are even wrong in many papers! And even when experimental findings are "real" they may not reflect deep psychological generalizations. And even if they do, they likely don't reflect generalizations that are true about people in general, only some very specific groups of people. Your hair should be on fire, at least a little bit. If you by the end of this chapter, you don't feel a little bit of despair about the published psychological literature, then we haven't done our job.

On the other hand, you might be thinking, how do you know that all this bad stuff is true? Claims about a literature or field as a whole go beyond the kind of standard paradigmatic science that we were talking about in the previous chapter -- instead they are part of a new field called **meta-science**. Meta-science research is research *about research*, for example investigating the rate of replication in a literature, or trying to figure out how widespread some negative practice is within a subfield. Meta-science allows us to go beyond one-off anecdotes about particular results or rumors about bad practices. Critically, we can also evaluate the strength of claims about the field using the same tools we use to evaluate standard research -- that is, we can critique research designs and inferences systematically, rather than just accepting (or failing to accept) assertions.

In this chapter, we'll discuss reproducibility and replicability primarily -- discussions of robustness and generalizability will be taken up in Chapters \@ref(models) and \@ref(sampling) respectively. We'll start out by reviewing some of the key concepts around reproducibility and replicability as well as the key meta-science findings. We'll then discuss some of the proposed sources of problems in replicability, focusing especially on how these might guide our study design and planning. We end by taking up the issue of how reproducibility and replicability relate to theory building in psychology. 

::: {.case-study}
üî¨ Case study: The Open Science Collaboration

Around 2011, we were teaching our Experimental Methods course for the first time, based on a course model that we had worked on with Rebeccca Saxe [@frank2012]. The idea was to have a replication-based course that introduced students to the nuts and bolts of research.^[One of the current author team was a student in the course that year!] A guy named Brian Nosek was on sabbatical nearby, and over coffee we learned that he was starting up an ambitious project to replicate a large sample of studies from top psychology journals in 2008.

Students in Experimental Methods that year chose replication projects from the sample that Nosek had told us about. Four of these projects were done at a high standard and were nominated by the course TAs for inclusion in the broader project on the basis of strong implementations of the paradigm and good sample sizes. A few years later, when the final group of 100 replication studies was completed, we got a look at the results, shown in Figure \@ref(fig:replication-osc-2015). 

```{r replication-osc-2015, fig.cap="Results from the Open Science Collaboration (2015). Each point represents one of the studies in the sample, with the horizontal position giving the original effect size and the vertical position giving the replication effect size. Dot size shows estimated statistical power. The dotted line represents a perfect replication."}
knitr::include_graphics("images/replication/osc-2015.png")
```
The resulting meta-science paper made a substantial impression on both psychologists and the broader community, defining both a field of psychology meta-science studies and providing a template for many-author  collaborative projects [@osc2015]. But the most striking thing was the result: disappointingly, by several different criteria, only around a third of studies showed the same finding as the original. The others yielded smaller effects that were no significant in the replication sample. This study provided the first large-scale evidence that  methodological issues in the psychological literature might lead to a bad outcome in the literature -- namely a literature with many findings that didn't replicate.

The study's results -- and their interpretation -- were controversial, however, and much ink was spilled on what these data showed. In particular, critics pointed to different degrees of fidelity between the original studies and the replications; insufficient levels of statistical power in the replications; non-representative sampling of the literature; and difficulties identifying specific statistical outcomes for replication success [@gilbert2016;@anderson2016]. In our view, many of these critiques have merit, and you can't simply interpret the results of this stufy as an unbiased estimate of the replicability of results in the literature, contra the title.^[Confusingly, the title of the paper is "Estimating the reproducibility of psychological science", not "the replicability of psychological science". This caused terminological confusion for several years; it seems like at this point people have decided it's just a mistake.]

And yet, the results are still important and compelling, and they undeniably changed the direction of the field of psychology. Many good studies are like this -- they have critical flaws but they lead the way towards other studies that pursue this new direction, often with greater technical precision and fewer issues. For several of us personally, working on this project was also transformative in that it showed us the power of collaborative work -- together we could do a study that no one of us had any hope of completing on our own, and potentially make a difference in our field.
:::

## Reproducibility

As one of their primary purposes, scientific papers report measurements, statistical results, and more complex analytic findings and visualizations. For these results to be subject to scrutiny, readers and reviewers need to be able to access some aspects of the set of steps from the original raw measures all the way to the final products. For much of the history of the scientific paper, complete verification of the **provenance** of a particular reported number in a paper was impossible -- at best, a reader was presented with a verbal or mathematical description of the computations that were performed on the raw data, and the raw data themselves were not available.^[In practice, for many years data have been available "on request," and professional societies like the American Psychological Association have [mandated data sharing](https://www.apa.org/ethics/code) for purposes of verification. But in practice data are rarely made available [@wicherts2006]. We believe this is untenable, and we provide a longer argument justifying data sharing in Chapter \@ref(ethics) and discuss some of the practicalities of sharing in Chapter \@ref(management).]

Data sharing (and the sharing of analytic code) is increasing, and we believe this is a very good thing for science as a whole.^[Psychology still likely lags behind other fields, however [@tedersoo2021].] But because sharing has been relatively limited in the past, the reproducibility of numbers in nearly all published papers cannot be checked, leaving open a number of negative possibilities. Most prominently, errors in calculation or reporting could lead to disparities between the reported result and the actual result. Mis-specification of analytic computations could mislead readers about the computations that were actually performed. And the robustness of analyses cannot be checked.^[We're focusing on data sharing here, because much experimental reseach uses relatively straightforward analyses. But the same points apply to code sharing as well! In computational research, the relevant position is nicely summed up by a prescient quote from @buckheit1995: "An article about computational science in a scientific publication is not the scholarship itself, it is merely advertising of the scholarship. The actual scholarship is the complete software development environment and the complete set of instructions which generated the figures."]

Are errors common? There are plenty of individual instances of errors that are corrected in the published literature [e.g., @cesana-arlotti2018], and we ourselves have made significant analytic errors [@frank2013]. But these kinds of experiences don't tell us about the frequency of error (or the consequences of error for the conclusions that researchers draw).^[There is a very interesting discussion of the pernicious role of scientific error on theory building in @gould1996's "The Mismeasure of Man." Gould examines research on racial differences in intelligence and documents how scientific errors that supported racial differences were often overlooked. Errors are often caught asymmetrically; we are more motivated to double-check a result that contradicts our biases.] This question about frequency is a meta-scientific question that a variety of researchers have attempted to answer over the years. If errors are frequent, that would suggest a need for changes in our policies and practices to reduce their frequency! 

```{r replication-fake-ttest}
t <- t.test(1:10, 2:11)
```

Unfortunately, the lack of data availability creates a problem: it's hard to figure out if calculations are wrong if you can't reproduce them in the first place. One meta-scientific research program has taken a clever approach to this issue. In standard American Psychological Association (APA) reporting format, inferential statistics must be reported with three pieces of information: the test statistic, the degrees of freedom for the test, and the $p$-value (e.g., $t(`r t$parameter`) = `r round(t$statistic, 2)`$, $p = `r round(t$p.value, 3)`$). Yet these pieces of information are redundant with one another. Thus, reported statistics can be checked for consistency simply by evaluating whether they line up with one another -- that is, whether the $p$-value recomputed from the $t$ and degrees of freedom matches the reported value. 

@bakker2011 performed precisely this analysis on a sample of 281 papers, and found that around 18% of statistical results were incorrectly reported. Even more worrisome, around 15% of articles contained at least decision error -- that is, a case where the error changed the direction of the inference that was made (e.g., from significant to insignificant).^[Confirming Gould's speculation, most of the reporting errors that led to decision errors were in line with the researchers' own hypotheses.] @nuijten2016 used an automated method called "statcheck"^[Statcheck is now available as a [web app](http://statcheck.io) and an R package so that you can check your own manuscripts!] to confirm and extend this analysis. They checked $p$-values for more than 250,000 psychology papers in the period 1985--2013 and found that around half of all papers contained at least one incorrect $p$-value! 

```{r replication-hardwicke, fig.cap="Analytic reproducibility of results from open-data articles in *Cognition* and *Psychological Science*. From Hardwicke et al. (2021)."}
knitr::include_graphics("images/replication/hardwicke2021.png")
```
There may be many more errors than are detected via this method: incorrect computation can be accompanied by correct reporting. While there is probably no general way to check reproducibility across the literature, a group of us conducted some more targeted studies of two journals with open-data policies. @hardwicke2018b and @hardwicke2021a identified datasets with reusable data (not all datasets were complete and comprehensible) and then downloaded the data and attempted to reproduce the main statistical results from 60 of these articles. This process was incredibly labor-intensive, with articles requiring 5--10 hours of work each. Only about a third of articles were completely reproducible without help from the original authors; around 62% were successfully reproduced after -- sometimes extensive -- correspondence (Figure \@ref(fig:replication-hardwicke)). A good number of the remaining papers appeared to have some irreproducible results due to errors of one type or another (and some reproducible numbers were reproducible despite errors identified by the team). Although none of the errors we identified invalidated the conclusions of the original studies, several authors were still motivated to submit corrections to their articles!

This body of evidence suggests to us that reporting and computation errors are frequent in the published literature, and the identification of these errors depends on the findings being reproducible. If data are not available, then errors simply cannot be found. This body of evidence leads us to believe that transparency is a critical imperative for decreasing the frequency of errors in the published literature.^[Does the meta-scientific literature provide strong support for transparency as a policy intervention leading to better reproducibility outcomes? To our knowledge, this experiment hasn't been run yet, perhaps because it would be quite difficult to execute. On the other hand, we can't help but mention that there is an *association* between data sharing and reporting errors, such that author teams who were less likely to provide data on request also had more reporting errors in their papers [@wicherts2011].] 

## Replication

As we discussed in Chapter \@ref(intro), organized skepticism and independent verifiability of observations are key features of scientific inquiry. Reproducibility is thus a key desideratum for scientific work. But beyond verifying the analyses reported in a paper, we are often interested in understanding whether the measurements can be replicated. To quote from @popper2005, "the scientifically significant... effect may be defined as that which can be regularly reproduced by anyone who carries out the appropriate experiment in the way prescribed."

### Conceptual frameworks for replication

The key challenge of replication is **invariance** -- Popper's "in the way prescribed". That is, what are the features of the world over which a particular observation should be relatively constant, and what are those that are specified as the key ingredients for the effect? Replication is relatively straightforward in the physical and biological sciences, in part because of presupposed theoretical background that allows us to make strong inferences about invariance. If a biologist reports an observation about a particular cell type from an organism, the color of the microscope is presumed not to matter to the observation. 

These invariances are far harder to state in psychology, for both the procedure of an experiment and its sample. Procedurally, how much should the color of the experimental stimulus matter to the measured effect? In some cases yes, in some cases no.^[A fascinating study by @baribault2018 proposes a method for empirically understanding psychological invariances. Treating a subliminal priming effect as their model system, they sampled thousands of "micro-experiments" in which small parameters of their experimental procedure were randomly sampled. These parameters allowed for measurement of their effect of interest, averaging across this irrelevant variation. Turned out in their case, color did not in fact matter.] If color does not matter, how about the context of presentation for an experiment -- should presentation to a participant at home on a web browser produce the same effect as presentation in a laboratory setting using custom software [@crump2013]? Without a theory of human responses to guide our decision-making, these invariances are quite hard to state. Yet problems of invariances in procedure pale in comparison to the problem of stating invariances across samples from human populations! In some sense, the research program of some branches of the social sciences amounts to an understanding of invariances across human cognition [@chomsky1967]. 

A lot is at stake in this discussion. If Frog publishes a finding with US undergraduates and Toad then "replicates" the procedure in Germany, to what extent should we be perturbed if the effect is different in magnitude or absent?^[Presumably not very much if Toad gave the original instructions in English instead of in German -- that's another one of these pesky invariances that we are always worrying about!] People have amde a number of replication taxonomies to try and quantify the "distance" between two experiments. One influential one is the distinction between **direct replications** and **conceptual replications** [@zwaan2018]. 





- We describe the framework for replication in Zwaan et al. (2018), highlighting the idea of multiple dimensions on which a replication can differ from the original study and the ways that this complicates inferences about replication ‚Äúsuccess‚Äù (Mathur and VanderWeele 2019, 2020).
- We can get different epistemic value from doing direct replications (attempt to copy original study with as much fidelity as possible) vs. conceptual replications (attempt to somewhat perturb operationalization of original study).

::: {.accident-report}
‚ö†Ô∏è Accident report: "Small Telescopes" 

We've been discussing the question of invariance with respect to procedure and sample, but we haven't really discussed invariance with respect to *result*. To what extent can we consider two measurements to be "the same"? @simonsohn2015 gives Several obvious definitions 

case study of Simonsohn et al. (2015): what if weather really does affect mood, but the effect is too small for the original study to ever detect?

^[Scott Alexander has a [graphic take](https://astralcodexten.substack.com/p/on-hreha-on-behavioral-economics) on this problem: "Imagine I claimed our next-door neighbor was a billionaire oil sheik who kept thousands of boxes of gold and diamonds hidden in his basement. Later we meet the neighbor, and he is the manager of a small bookstore and has a salary 10% above the US average... Should we describe this as ‚Äúwe have confirmed the Wealthy Neighbor Hypothesis, though the effect size was smaller than expected‚Äù? Or as ‚ÄúI made up a completely crazy story, and in unrelated news there was an irrelevant deviation from literally-zero in the same space‚Äù?"]
:::


### The meta-science of replication

So, if we can't really take the Open Science Collaboration (2015) number of 33% seriously, how replicable is psychological research? Based on the discussion above, we hope we've made you skeptical that this is a well-posed question without a lot of additional details. Any answer is going to have to provide details about the scope of this claim, the definition of replication being used, and the metric for replication success. On the other hand, versions of this question have led to a number of empirical studies that help us better understand the scope of replication issues. We'll review these briefly here because we think a good understanding of the meta-science literature on replication can help us decide how worried we should be about the state of the psychology literature. 

One set of follow-up studies to the OSC2015 study has focused on evaluating replication success within subsamples. Many of these have focused on particular subfields or journals, with the goal of informing particular field-specific practices or questions. For example, @camerer2016 largely adopted the methodological choices of OSC2015, but applied the procedure to all of the between-subject laboratory articles published in two top economics journals in the period 2011--2014. They found a top-line replication rate of 61%, higher than in OSC2015 but lower than the naive expectation based on their level of statistical power. Another notable followup to OSC2015 replicated all the 


Another set ManyLabs



::: {.ethics-box}
üåø Ethics box: Consequences for the study, consequences for the person

"Power posing" is the idea that changing your physical posture to make it more open and expansive might also change your confidence. @carney2010 reported a striking study of this phenomenon, in which 42 participants were told they were taking part in a study of physiological recording. They then held two poses, each for a minute. In one condition, the poses were expansive (e.g., legs out, hands on head) or contractive (e.g., arms and legs crossed). Participants in the expansive pose condition showed increases in testosterone and decreases in salivary cortisol (a stress marker), they took a greater number of risk in a gambling task, and they reported that they were more "in charge" in a survey. This result suggested that a two-minute induction could lead to striking physiological and psychological changes -- in turn leading to power posing becoming firmly enshrined as part of the set of recommended strategies in business and elsewhere. The original publication contributed to the rise of the researchers' careers, including becoming a principal piece of evidence in a hugely-popular TED talk by Amy Cuddy, one of the authors.

This result is likely not definitive, however. A replication study with a larger number of participants (N=200) failed to find evidence for physiological effects, even as it did find some effects on participants' own beliefs [@ranehill2015]. And a review of the published literature suggested that many findings appeared to be the result of some sort of publication bias, as far too many of them had $p$-values very close to the .05 threshold [@simmons2017]. In light of this evidence, the first author made a public statement that she [does not believe that "power pose" effects are real](https://faculty.haas.berkeley.edu/dana_carney/pdf_my%20position%20on%20power%20poses.pdf).


```{r replication-powerpose, fig.cap="Google trends time series for \"power pose\" from 2004-2021.", fig.margin=TRUE}
knitr::include_graphics("images/replication/powerpose.png")
```

From the scientific perspective, it's very tempting to take this example as a case in which the scientific ecosystem corrects itself. Although many people continue to cite the original power posing work, we suspect the issues are well-known throughout the social psychology community, and overall interest has gone down (see Figure \@ref(fig:replication-powerpose). But this narrative masks the very real human impacts of the self-correction process, which can raise ethical questions about the best way to address issues in the scientific record. 

The process of debate and discussion around individual findings can be bruising and complicated. In the case of power posing, Cuddy herself was tighly associated with the findings and many critiques of the findings became critiques of the individual. Several commentators used Cuddy's name as a synecdoche for low-quality psychological results, likely because of her prominence and perhaps because of her gender and age as well. These comments were likely harmful to Cuddy personally and her career more generally.^[For further reading, see ["When the Revolution Came for Amy Cuddy"](https://www.nytimes.com/2017/10/18/magazine/when-the-revolution-came-for-amy-cuddy.html).]

Scientists should critique, reproduce, and replicate -- these are all critical parts of the progress of normal science. But it's important to do this in a way that's sensitive to the people involved. Here are a few guidelines for courteous and ethical conduct: 

* Communicate personally before communicating publicly. As one critic said about the power posing debate, "‚ÄúI wish I‚Äôd had the presence of mind to pick up the phone and call [before publishing my critique].‚Äù
* Always communicate about the work, never the person. Try to use language that is specific to the analysis or design being critiqued, rather than the person who did the analysis or thought up the design.
* Avoid using language that assumes negative intentions, e.g. "the authors misleadingly state that ..."
* Ask for someone to read your paper, email, blogpost, or tweet before you hit send. It can be very difficult to figure out the tone of your writing without an external perspective.

As we will argue in the next chapter, we have an ethical duty as scientists to promote good science and critique low quality science. But we also have duties to our colleauges and communities to be good to one another.
:::

## Causes of the "replication crisis"









::: {.accident-report}
‚ö†Ô∏è Accident report: When I'm 64?

Simmons, Simonsohn, Nelson introduce p-hacking into our lexicon. 

:::



## Replication and theory building

Crisis is a term in Kuhn for the period preceding a paradigm shift.

How do reproducibility and replicability contribute to theory building? We draw on Hardwicke et al. (2018), considering these factors in the informativeness of an experiment.


Introduce p-hacking and publication biases as major sources of irreproducibility. These are biases - and we highlight bias reduction as a key goal.



<!-- ::: {.case-study} -->
<!-- üî¨ Case study: Hardwicke et al. (2018; 2021) meta-studies of the analytic reproducibility of specific analytic findings in the empirical literature. -->
<!-- ::: -->
